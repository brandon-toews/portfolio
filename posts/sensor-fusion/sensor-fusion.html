<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.3.450" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />

<meta name="author" content="Brandon Toews" />
<meta name="dcterms.date" content="2025-01-13" />
<meta name="description" content="Implementation of sensor fusion using Extended Kalman Filter for autonomous robot navigation and path planning in warehouse environments using Isaac Sim." />

<title>portfolio – Sensor Fusion Using EKF for Navigation and Path Planning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css" />
</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="/index.html">
    <span class="navbar-title">portfolio</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse"
  aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation"
  onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="/about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/brandon-toews" rel="" target=""><i 
  class="bi bi-github" 
  role="img" 
>
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
  <div class="quarto-title-banner">
    <div class="quarto-title column-body">
      <h1 class="title">Sensor Fusion Using EKF for Navigation and Path Planning</h1>
                  <div>
        <div class="description">
          Implementation of sensor fusion using Extended Kalman Filter for autonomous robot navigation and path planning in warehouse environments using Isaac Sim.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Sensor Fusion</div>
                <div class="quarto-category">Extended Kalman Filter</div>
                <div class="quarto-category">Robotics Navigation</div>
                <div class="quarto-category">Path Planning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Brandon Toews </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 13, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header>
<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#simulation-setup-and-kinematics-implementation" id="toc-simulation-setup-and-kinematics-implementation"><span class="header-section-number">1</span> Simulation Setup and Kinematics Implementation</a>
  <ul>
  <li><a href="#robot-kinematics" id="toc-robot-kinematics"><span class="header-section-number">1.1</span> Robot Kinematics</a></li>
  <li><a href="#sec-environment-setup" id="toc-sec-environment-setup"><span class="header-section-number">1.2</span> Environment Setup</a></li>
  </ul></li>
  <li><a href="#sensor-integration-and-data-acquisition" id="toc-sensor-integration-and-data-acquisition"><span class="header-section-number">2</span> Sensor Integration and Data Acquisition</a>
  <ul>
  <li><a href="#sensor-selection" id="toc-sensor-selection"><span class="header-section-number">2.1</span> Sensor Selection</a></li>
  <li><a href="#data-acquisition" id="toc-data-acquisition"><span class="header-section-number">2.2</span> Data Acquisition</a></li>
  </ul></li>
  <li><a href="#sensor-fusion-using-extended-kalman-filter-ekf" id="toc-sensor-fusion-using-extended-kalman-filter-ekf"><span class="header-section-number">3</span> Sensor Fusion Using Extended Kalman Filter (EKF)</a>
  <ul>
  <li><a href="#fusion-techniques" id="toc-fusion-techniques"><span class="header-section-number">3.1</span> Fusion Techniques</a></li>
  <li><a href="#application-in-navigation" id="toc-application-in-navigation"><span class="header-section-number">3.2</span> Application in Navigation</a></li>
  </ul></li>
  <li><a href="#simulation-results-and-comparison" id="toc-simulation-results-and-comparison"><span class="header-section-number">4</span> Simulation Results and Comparison</a>
  <ul>
  <li><a href="#performance-metrics" id="toc-performance-metrics"><span class="header-section-number">4.1</span> Performance Metrics</a></li>
  <li><a href="#analysis-of-results" id="toc-analysis-of-results"><span class="header-section-number">4.2</span> Analysis of Results</a></li>
  </ul></li>
  <li><a href="#evaluation-and-improvement-suggestions" id="toc-evaluation-and-improvement-suggestions"><span class="header-section-number">5</span> Evaluation and Improvement Suggestions</a>
  <ul>
  <li><a href="#critical-evaluation" id="toc-critical-evaluation"><span class="header-section-number">5.1</span> Critical Evaluation</a></li>
  <li><a href="#improvements" id="toc-improvements"><span class="header-section-number">5.2</span> Improvements</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references">References</a></li>
  </ul>
</nav>
<p><img src="3rd-person-view.png" class="img-fluid" /> ::: {.callout-tip collapse=“true”} ## Video Walkthrough</p>
<p><video src="SensorFusion_Video_Walkthrough.mp4" id="video-walkthrough" class="img-fluid" controls=""><a href="SensorFusion_Video_Walkthrough.mp4">Video Walkthrough</a></video> :::</p>
<section id="simulation-setup-and-kinematics-implementation" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Simulation Setup and Kinematics Implementation</h1>
<section id="robot-kinematics" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Robot Kinematics</h2>
<p><strong>Chosen Robot:</strong> Nova Carter</p>
<p><strong>Kinematic Model:</strong> Differential Drive</p>
<p><strong>Justification:</strong></p>
<ul>
<li>Simplicity and ease of implementation</li>
<li>Suitable for indoor environments and small-scale robotic platforms</li>
<li>Precise control over direction and speed with minimal mechanical complexity</li>
<li>Widely applicable for wheeled robots performing navigation tasks in structured environments</li>
</ul>
<p>The kinematic model for differential drive robots, like the Nova Carter robot in this implementation, is expressed in Equation 1 and is responsible for explaining the movement of the robot. It describes how the robot’s position (x, y) and orientation θ evolve with respect to linear and angular velocities.</p>
<p><span id="eq-kinematic-model"><span class="math display">\[
\begin{aligned}
\dot{x} &amp;= v \cdot \cos(\theta) \\
\dot{y} &amp;= v \cdot \sin(\theta) \\
\dot{\theta} &amp;= \omega
\end{aligned}
\tag{1}\]</span></span></p>
<p>Where:</p>
<ul>
<li>v is the linear velocity (from the Twist message)</li>
<li>ω is the angular velocity</li>
<li>θ is the robot’s orientation</li>
</ul>
<p>The wheel velocities <span class="math inline">\(v_r\)</span> and <span class="math inline">\(v_l\)</span> are derived using:</p>
<p><span id="eq-wheel-velocity"><span class="math display">\[
\begin{aligned}
v_r &amp;= \frac{2v + \omega L}{2R} \\
v_l &amp;= \frac{2v - \omega L}{2R}
\end{aligned}
\tag{2}\]</span></span></p>
<p>Where:</p>
<ul>
<li>L = 0.413m (wheel distance)</li>
<li>R = 0.14m (wheel radius)</li>
</ul>
<p>The simulated environment, discussed in greater detail in the next section (<a href="#sec-environment-setup">Section 1.2</a>), is run in Isaac Sim. Isaac Sim’s differential drive model uses OmniGraph nodes, specifically the Differential Controller Node, to compute wheel velocities from linear and angular velocity inputs using the wheel velocity equations (<a href="#eq-wheel-velocity">Equation 2</a>) and kinematic parameters specified in the node (<a href="#fig-differential-controller">Figure 1</a>). The differential drive path controller MATLAB script calculates linear and angular velocities from target positions in the planned path and sends these to the controller in Isaac Sim through a ROS2 bridge.</p>
<div id="fig-differential-controller" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="differential-controller.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 1<strong>.</strong> Nova Carter Differential Drive Parameters in Isaac Sim</figcaption>
</figure>
</div>
<p><strong>Kinematic Parameters:</strong></p>
<ul>
<li><strong>wheelDistance:</strong> 0.413 meters (distance between the wheels)</li>
<li><strong>wheelRadius:</strong> 0.14 meters (radius of the wheels)</li>
<li><strong>maxLinearSpeed:</strong> 2.0 m/s</li>
<li><strong>maxAngularSpeed:</strong> 3.0 rad/s</li>
<li><strong>maxAcceleration / maxDeceleration:</strong> 2.0 m/s² (limits for smooth motion)</li>
<li><strong>dt:</strong> 0.01667 (time step, approximately 60 Hz simulation rate)</li>
</ul>
<div id="fig-robot-dimensions" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="robot-dimensions.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 2<strong>.</strong> Robot Dimensions</figcaption>
</figure>
</div>
</section>
<section id="sec-environment-setup" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> Environment Setup</h2>
<p>The environment is simulated within Isaac Sim as it provides a realistic scene which is highly useful for practically applying theoretical robotics concepts. Isaac Sim is a high-fidelity physics engine that is capable of simulating commercial and industrial robots, robotic movement, and sensor data making it a prime candidate for this simulation (Nvidia Omniverse IsaacSim, 2025). The scene is a warehouse with forklifts, pallets, and shelving for the robot to navigate through. These static obstacles have been placed to create a maze-like environment that the robot can plan paths and navigate through. On running the main MATLAB script, a robot object is created and given a goal position to navigate to. The user can observe the robot moving towards the specified goal through the viewport in Isaac Sim (<a href="#fig-3rd-person-view">Figure 3</a>), front camera viewport in RViz2 (<a href="#fig-1st-person-view">Figure 4</a>), and the robot’s occupancy map displaying the planned path and the robot’s pose along the way (<a href="#fig-occupancy-map">Figure 5</a>).</p>
<div id="fig-3rd-person-view" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="3rd-person-view.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 3<strong>.</strong> 3rd Person View in Isaac Sim Viewport</figcaption>
</figure>
</div>
<div id="fig-1st-person-view" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="1st-person-view.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 4<strong>.</strong> 1st Person View in RViz2 Viewport</figcaption>
</figure>
</div>
<div id="fig-occupancy-map" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="occupancy-map.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 5<strong>.</strong> Ground Truth Occupancy Map (Left), Robot’s LiDAR updated Occupancy Map (Right)</figcaption>
</figure>
</div>
</section>
</section>
<section id="sensor-integration-and-data-acquisition" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Sensor Integration and Data Acquisition</h1>
<section id="sensor-selection" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Sensor Selection</h2>
<p><strong>Chassis Odometry Sensor:</strong> Collects position (x, y) and quaternion orientation (x, y, z, w) based on wheel encoder data, used in combination with IMU for more accurate localization. GPS signals are significantly weakened or blocked by warehouse roofs, walls, and metal racking (Ghasemieh, A. and Kashef, R., 2024, p. 2, para. 3). On the other hand, odometry works independently of external signals, making it ideal for indoor environments. GPS typically has 5-10 meter accuracy, while odometry typically achieves centimeter-level precision for short distances (Ghasemieh, A. and Kashef, R., 2024, p. 3, para. 1). This precision is crucial for warehouse operations like navigating narrow aisles. Odometry provides immediate feedback while GPS can have significant delay in position updates. This allows for quick response time, which is essential for obstacle avoidance and precise positioning. The system compensates for odometry’s main weakness, drift, by fusing it with IMU data through the Extended Kalman Filter and using LiDAR for correction against discovered landmarks.</p>
<p><strong>Chassis IMU Sensor:</strong> Is a combined sensor package that usually includes accelerometers (measuring linear acceleration) and gyroscopes (measuring angular velocity/rotation). These measurements are particularly valuable for detecting rapid changes in the robot’s motion and helping to correct for wheel slippage that might occur on the warehouse floor. The acceleration data is integrated over time to estimate velocity, while the angular velocity provides direct measurement of the robot’s rotation rate. The advantage of using an IMU in a warehouse setting is that it combines these sensors into a single calibrated package. Because of this, accelerometer and gyroscope readings are perfectly synchronized with a single timestamp for all measurements making it easier for sensor fusion. Sensors are pre-calibrated relative to each other ensuring alignment between acceleration and rotation axes. Finally, there is considerable space and cost efficiency in using a single component for mounting and wiring.</p>
<p><strong>Front 2D LiDAR Sensor:</strong> Measures distances in a single horizontal plane with accompanying angle of measurement. 2D LiDAR generates significantly less data to process than a 3D LiDAR sensor which allows for more responsive occupancy grid updates. Real-time performance is crucial for warehouse navigations that include dynamic obstacles. This data undergoes several processing steps, including range validation and conversion to Cartesian coordinates, before being used to update the robot’s internal map and assist in obstacle avoidance.</p>
</section>
<section id="data-acquisition" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Data Acquisition</h2>
<p>The heart of our data collection system lies in its ROS2-based architecture. ROS2 provides a robust framework for handling real-time sensor data through its publisher-subscriber model (Carreira, R. et al., 2024, pp. 11-12). Each sensor communicates through dedicated ROS2 topics controlled with a precise timing mechanism that ensures consistent data sampling across all sensors. Operating at 60Hz (with a time step of 0.01667 seconds), this timer-based approach synchronizes data collection and processing, crucial for maintaining accurate state estimation. This high update rate allows the robot to respond quickly to changes in its environment while maintaining smooth motion control. To facilitate system analysis and improvement, we’ve implemented a comprehensive data logging system. This system records not only the raw sensor readings but also the processed state estimates and ground truth data for comparison. The logged data proves invaluable for post-mission analysis, allowing us to evaluate system performance and identify areas for improvement.</p>
</section>
</section>
<section id="sensor-fusion-using-extended-kalman-filter-ekf" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Sensor Fusion Using Extended Kalman Filter (EKF)</h1>
<section id="fusion-techniques" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Fusion Techniques</h2>
<p>The Standard Kalman Filter assumes the state transition and observation models are linear. The kinematic model employed for the differential drive is inherently non-linear due to the trigonometric functions cos(θ) and sin(θ) (<a href="#eq-kinematic-model">Equation 1</a>). The state transition model (<a href="#eq-state-transition">Equation 4</a>) describes transitions from states by combining the state definition (<a href="#eq-state-definition">Equation 3</a>) with the non-linear kinematic model (<a href="#eq-kinematic-model">Equation 1</a>). EKF solves this problem by linearizing the non-linear system at each time step by computing Jacobian matrices (Jiang, L. and Wu, L., 2024, p. 2, para. 1). The state transition Jacobian matrix (<a href="#eq-state-jacobian">Equation 5</a>) is calculated by considering the state transition model (<a href="#eq-state-transition">Equation 4</a>) as f(x) and finding the partial derivatives for each function of f(x) with respect to each term in the state definition model (<a href="#eq-state-definition">Equation 3</a>).</p>
<p><span id="eq-state-definition"><span class="math display">\[
\mathbf{x} = \begin{bmatrix} x \\ y \\ \theta \\ v \\ \omega \end{bmatrix}
\tag{3}\]</span></span></p>
<p><span id="eq-state-transition"><span class="math display">\[
\begin{aligned}
x_{k+1} &amp;= x_k + v \cdot \Delta t \cdot \cos(\theta) \\
y_{k+1} &amp;= y_k + v \cdot \Delta t \cdot \sin(\theta) \\
\theta_{k+1} &amp;= \theta_k + \omega \cdot \Delta t \\
v_{k+1} &amp;= v_k \\
\omega_{k+1} &amp;= \omega_k
\end{aligned}
\tag{4}\]</span></span></p>
<p><span id="eq-state-jacobian"><span class="math display">\[
\mathbf{F} = \begin{bmatrix}
1 &amp; 0 &amp; -v \cdot \Delta t \cdot \sin(\theta) &amp; \Delta t \cdot \cos(\theta) &amp; 0 \\
0 &amp; 1 &amp; v \cdot \Delta t \cdot \cos(\theta) &amp; \Delta t \cdot \sin(\theta) &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; \Delta t \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\tag{5}\]</span></span></p>
<p>The observation models are linear, and we add measurement noise to explain the uncertainty in the readings from the sensors for things like drift or wheel slippage (<a href="#eq-odom-observation">Equation 6</a> &amp; <a href="#eq-imu-observation">Equation 7</a>). Since we need to perform matrix calculations because of having to linearize the state transition model with a Jacobian matrix, we also need to use Jacobians of the observation models to apply to the state transition Jacobian (<a href="#eq-state-jacobian">Equation 5</a>). The observation model Jacobians (<a href="#eq-odom-jacobian">Equation 8</a> &amp; <a href="#eq-imu-jacobian">Equation 9</a>) are calculated by considering the observation models (<a href="#eq-odom-observation">Equation 6</a> &amp; <a href="#eq-imu-observation">Equation 7</a>) as h(x) and finding the partial derivatives for each function of h(x) with respect to each term in the state definition model (<a href="#eq-state-definition">Equation 3</a>).</p>
<p><span id="eq-odom-observation"><span class="math display">\[
\mathbf{z}_{odom} = \mathbf{h}_{odom}(\mathbf{x}) = \begin{bmatrix} x \\ y \\ \theta \\ v \\ \omega \end{bmatrix}
\tag{6}\]</span></span></p>
<p><span id="eq-imu-observation"><span class="math display">\[
\mathbf{z}_{imu} = \mathbf{h}_{imu}(\mathbf{x}) = \begin{bmatrix} v \\ \omega \end{bmatrix}
\tag{7}\]</span></span></p>
<p><span id="eq-odom-jacobian"><span class="math display">\[
\mathbf{H}_{odom} = \frac{\partial \mathbf{h}_{odom}}{\partial \mathbf{x}} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\tag{8}\]</span></span></p>
<p><span id="eq-imu-jacobian"><span class="math display">\[
\mathbf{H}_{imu} = \frac{\partial \mathbf{h}_{imu}}{\partial \mathbf{x}} = \begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\tag{9}\]</span></span></p>
<p>Finally, we perform the prediction step of the state transition where we apply the process noise to the state transition Jacobian to model the uncertainty of the prediction and update the predicted state covariance (<a href="#eq-prediction">Equation 10</a>). Then we incorporate the measurement. Then we update the observed measurements using the observation Jacobians, measurement noise, and predicted state covariance to create the Kalman gain filter. This filter is then used to weigh the contribution of the predicted state transition against the measurement observations in the final outcome of the state prediction. The filter bases these contributions of the predicted state transition and measurement observations on the respective covariances, or the uncertainty, of each (<a href="#eq-measurement-update">Equation 11</a>).*</p>
<p><span id="eq-prediction"><span class="math display">\[
\begin{aligned}
\mathbf{x}_{k+1|k} &amp;= \mathbf{F} \cdot \mathbf{x}_k \\
\mathbf{P}_{k+1|k} &amp;= \mathbf{F} \cdot \mathbf{P}_k \cdot \mathbf{F}^T + \mathbf{Q}
\end{aligned}
\tag{10}\]</span></span></p>
<p><span id="eq-measurement-update"><span class="math display">\[
\begin{aligned}
\mathbf{y} &amp;= \mathbf{z} - \mathbf{H} \cdot \mathbf{x}_{k+1|k} \\
\mathbf{S} &amp;= \mathbf{H} \cdot \mathbf{P}_{k+1|k} \cdot \mathbf{H}^T + \mathbf{R} \\
\mathbf{K} &amp;= \mathbf{P}_{k+1|k} \cdot \mathbf{H}^T \cdot \mathbf{S}^{-1} \\
\mathbf{x}_{k+1} &amp;= \mathbf{x}_{k+1|k} + \mathbf{K} \cdot \mathbf{y} \\
\mathbf{P}_{k+1} &amp;= (\mathbf{I} - \mathbf{K} \cdot \mathbf{H}) \cdot \mathbf{P}_{k+1|k}
\end{aligned}
\tag{11}\]</span></span></p>
<p>*This measurement update step is done twice in the code implementation, once for the odometry measurements and again for IMU measurements.</p>
</section>
<section id="application-in-navigation" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Application in Navigation</h2>
<p>Ultimately, EKF is an appropriate method for sensor fusion in this case because it can effectively combine IMU and Odometry readings to improve state estimation and subsequent navigation accuracy. Not only this, EKF can simultaneously handle the non-linearity introduced by the state transition model to produce a reasonable level of accuracy for the task of navigating in a warehouse environment. Artificial Neural Networks (ANNs) are another viable option for sensor fusion as they too can combine sensor readings and manage non-linear associations quite well. However, ANNs are typically more computationally expensive than EKF and require an undetermined amount of training and fine tuning to produce sufficient accuracy resulting in longer development durations. As a result, EKF has been used in this simulation to produce a lightweight and accurate solution suitable for the hardware resource and real-time computational constraints of the project.</p>
<p>As seen depicted in <a href="#fig-fusion-accuracy">Figure 6</a>, EKF sensor fusion provides the robot with accurate enough pose estimations, in relation to ground truth, to allow for the robot to make properly informed decisions about velocities and headings. Whereas, when the robot does not have a consistent and accurate idea of where it is and how it is oriented in space the velocity adjusts become more erratic and susceptible to bias over time. The redundancy provided by data collected from multiple sensors measuring similar types of orientation information provides a navigation framework that is resilient against sensor noise and/or failure. Individual sensor noise fluctuations are not correlated with one another, permitting a more well-rounded estimation of state at any given time. We will delve into how well this EKF implementation performs by examining in comparison with alternate methods of pose estimation.</p>
<div id="fig-fusion-accuracy" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="fusion-accuracy.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 6<strong>.</strong> Sensor Fusion Accuracy Compared to A* Planned Path &amp; Ground Truth</figcaption>
</figure>
</div>
</section>
</section>
<section id="simulation-results-and-comparison" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Simulation Results and Comparison</h1>
<section id="performance-metrics" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span> Performance Metrics</h2>
<p>We provide three (3) different modes of state estimation to compare the effects of measurement noise on navigation and to illustrate just how effective EKF is at addressing this. First, a dead reckoning implementation that uses only control inputs and the state transition function to track orientation and navigate. Second, odometry-only state estimation that doesn’t compensate for the simulated sensor noise of the odometer. Lastly, the EKF solution that fuses odometry and IMU readings, models sensor and process uncertainty, and that elegantly combines state transition predictions with the observed measurements. We will look at:</p>
<ol type="1">
<li><p>How accurate the modes are at estimating their state in comparison with the ground truth state.</p></li>
<li><p>How accurate the modes are at following the planned path, calculated from an A* algorithm taken from the MATLAB robotics toolbox. This will be a comparison of the ground truth and the planned path as the ground truth is a representation of what the robot actually did.</p></li>
<li><p>Navigation and path error probability distributions, bar charts depicting mean and maximum path deviation, path length, completion time.</p></li>
</ol>
</section>
<section id="analysis-of-results" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span> Analysis of Results</h2>
<p>As can be seen in <a href="#fig-dead-reckoning-trajectory">Figure 7</a> and <a href="#fig-dead-reckoning-error">Figure 8</a>, dead reckoning is highly inaccurate in pose estimation and is subsequently unable to follow the planned path in any practical capacity. It seems that without sensor data to interoceptively inform the robot of its orientation it is overcome by sensor noise, bias, and process noise.</p>
<div id="fig-dead-reckoning-trajectory" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="dead-reckoning-trajectory.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 7<strong>.</strong> Dead Reckoning - Accuracy Compared to A* Planned Path, Ground Truth, &amp; State Estimation</figcaption>
</figure>
</div>
<div id="fig-dead-reckoning-error" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="dead-reckoning-error.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 8<strong>.</strong> Dead Reckoning - Pose Estimate Error Probability Distributions Compared to Ground Truth</figcaption>
</figure>
</div>
<p>Odometry-only is a marked improvement over dead reckoning on all accounts and performs reasonably well considering we are only using one (1) sensor (<a href="#fig-odom-only-trajectory">Figure 9</a> &amp; <a href="#fig-odom-only-error">Figure 10</a>). The robot’s state estimation is erratic but not so much so that it cannot follow the planned path to reach the goal pose. This mode of estimation may be sufficient in certain environments and with sensors that do not produce high levels of noise. However, as sensor noise increases the performance of this mode of estimation decreases, causing many overcompensations to velocities and heading (<a href="#fig-ekf-trajectory">Figure 11</a>). These frequent and powerful corrections would likely cause more wear and tear on the vehicle.</p>
<div id="fig-odom-only-trajectory" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="odom-only-trajectory.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 9<strong>.</strong> Odometry Only - Accuracy Compared to A* Planned Path, Ground Truth, &amp; State Estimation</figcaption>
</figure>
</div>
<div id="fig-odom-only-error" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="odom-only-error.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 10<strong>.</strong> Odometry Only - Pose Estimate Error Probability Distributions Compared to Ground Truth</figcaption>
</figure>
</div>
<p>The EKF solution performs the best of all three (3), showcasing a robust ability to handle measurement noise in real-time. This mode of state estimation boasts the least amount of error in all metrics across the board with the exception of completion time. EKF produces tighter error probability distributions with equal or lower magnitudes than the other modes of state estimation. The EKF ground truth has a maximum deviation from the planned path almost half that of, and even a few points away from being equal to the mean deviation of the odometry-only solution (<a href="#tbl-path-statistics">Table 2</a>). As we manually increase measurement noise, we can see how the EKF still maintains stability and accuracy while odometry-only suffers from even greater fluctuations in state estimation (<a href="#fig-odom-high-noise">Figure 16</a> &amp; <a href="#fig-ekf-high-noise">Figure 17</a>). Even though EKFs tend to be computationally expensive they do produce remarkable results in contexts like these (Vitali, R.V., McGinnis, R.S. and Perkins, N.C., 2021, pp. 1-2).</p>
<div id="fig-ekf-trajectory" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="ekf-trajectory.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 11<strong>.</strong> EKF - Accuracy Compared to A* Planned Path, Ground Truth, &amp; State Estimation</figcaption>
</figure>
</div>
<div id="fig-ekf-error" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="ekf-error.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 12<strong>.</strong> EKF - Pose Estimate Error Probability Distributions Compared to Ground Truth</figcaption>
</figure>
</div>
<div id="fig-all-ground-truths" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="all-ground-truths.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 13<strong>.</strong> All Ground Truths Compared to A* Planned Path</figcaption>
</figure>
</div>
<div id="fig-all-estimations" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="all-estimations.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 14<strong>.</strong> All Estimation Methods’ Estimations, Ground Truths, A* Planned Path</figcaption>
</figure>
</div>
<div id="fig-path-comparison" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="path-comparison.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 15<strong>.</strong> All Estimation Methods’ Ground Truths Against A* Planned Path</figcaption>
</figure>
</div>
<div id="tbl-path-error">
<table>
<caption>Table 1<strong>.</strong> All Estimation Methods’ Ground Truths Against A* Planned Path Error</caption>
<colgroup>
<col style="width: 7%" />
<col style="width: 23%" />
<col style="width: 22%" />
<col style="width: 24%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Mean Position Error (m)</th>
<th>RMS Position Error (m)</th>
<th>Mean Heading Error (rad)</th>
<th>RMS Heading Error (rad)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Fusion</strong></td>
<td>0.043518</td>
<td>0.049445</td>
<td>0.014593</td>
<td>0.017925</td>
</tr>
<tr class="even">
<td><strong>Odometry Only</strong></td>
<td>0.066875</td>
<td>0.076514</td>
<td>0.015236</td>
<td>0.019748</td>
</tr>
<tr class="odd">
<td><strong>Dead Reckoning</strong></td>
<td>0.72064</td>
<td>0.79609</td>
<td>0.29554</td>
<td>0.35184</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-path-statistics">
<table style="width:100%;">
<caption>Table 2<strong>.</strong> All Estimation Methods’ Ground Truths Against A* Planned Path Statistics</caption>
<colgroup>
<col style="width: 4%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 9%" />
<col style="width: 12%" />
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Mean Deviation (m)</th>
<th>Max Deviation (m)</th>
<th>Path Length (m)</th>
<th>Completion Time (s)</th>
<th>Path Smoothness (avg heading change)</th>
<th>Time within 0.1m</th>
<th>Time within 0.2m</th>
<th>Time within 0.5m</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Fusion</strong></td>
<td>0.12613</td>
<td>0.18047</td>
<td>2.1128</td>
<td>0.96</td>
<td>0.030647 rad</td>
<td>23.5294%</td>
<td>100%</td>
<td>100%</td>
</tr>
<tr class="even">
<td><strong>Odometry Only</strong></td>
<td>0.1577</td>
<td>0.35182</td>
<td>2.172</td>
<td>0.84</td>
<td>0.021918 rad</td>
<td>42.8571%</td>
<td>66.6667%</td>
<td>100%</td>
</tr>
<tr class="odd">
<td><strong>Dead Reckoning</strong></td>
<td>0.25327</td>
<td>0.39551</td>
<td>1.0653</td>
<td>0.68</td>
<td>0.042377 rad</td>
<td>6.0606%</td>
<td>45.4545%</td>
<td>100%</td>
</tr>
</tbody>
</table>
</div>
<div id="fig-odom-high-noise" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="odom-high-noise.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 16<strong>.</strong> Odometry Only - Higher Noise Accuracy Compared to A* Planned Path &amp; Ground Truth</figcaption>
</figure>
</div>
<div id="fig-ekf-high-noise" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="ekf-high-noise.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 17<strong>.</strong> EKF - Higher Noise Accuracy Compared to A* Planned Path &amp; Ground Truth</figcaption>
</figure>
</div>
</section>
</section>
<section id="evaluation-and-improvement-suggestions" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Evaluation and Improvement Suggestions</h1>
<section id="critical-evaluation" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span> Critical Evaluation</h2>
<p>While EKF clearly outperforms the other methods of state estimation explored in this paper, it is not without its challenges. Tuning the process noise manually is a time-consuming trial-and-error endeavor even with data analytics to inform the adjustments. Meanwhile, deploying and developing Artificial Neural Networks (ANN) are becoming easier with support from libraries like PyTorch and are more than capable of handling the non-linear nature of state estimations and transitions (Ghorbani, S. and Janabi-Sharifi, F., 2022, p.1, para. 5). The current EKF implementation could take more full advantage of the IMU and odometry orientation data by processing quaternion information directly using quaternion specific equations rather than converting the quaternion to θ at the time of collection (Vitali, R.V., McGinnis, R.S. and Perkins, N.C., 2021, pp. 2-6).</p>
<p>As stated previously, the robot uses a 2D LiDAR sensor to update its internal occupancy map which is crucial for navigating a dynamic environment like a warehouse. The horizontal orientation of the obstacle detection provided by this sensor presents issues in this context. Even though the sensor is effectively mapping most of the obstacles in the environment it is missing a key feature that results in collisions. If forklift forks are raised off the ground, then the LiDAR doesn’t detect these long protrusions which routinely causes collisions with the robot. Compounding the situation further, the robot’s occupancy map cannot be inflated as each time the LiDAR updates the map it would have to call the inflate method causing already inflated objects to continue to grow until enveloping the entire map. The current A* algorithm being used doesn’t have a minimum turning radius property, therefore, the planner charts paths that bring the robot too close to objects causing collisions, even with obstacles it registers in its occupancy map. Using a high-fidelity simulation engine like Isaac Sim pushes these kinds of real-world challenges to the fore providing the opportunity to address these in a simulated environment before deployment. In conjunction with this, usage of the ROS2 bridge for the environment setup facilitates easy portability to real-world applications (Carreira, R. et al., 2024, pp. 5-8).</p>
<div id="fig-lidar-limitation" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="lidar-limitation.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 18<strong>.</strong> 2D LiDAR Doesn’t Update Forklift Forks onto Robot’s Occupancy Map</figcaption>
</figure>
</div>
<div id="fig-robot-collision" class="quarto-figure quarto-figure-center">
<figure>
<p><img src="robot-collision.png" class="img-fluid" /></p>
<figcaption><strong>Figure</strong> 19<strong>.</strong> Robot Collides with Forklift Forks</figcaption>
</figure>
</div>
</section>
<section id="improvements" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span> Improvements</h2>
<p>A more comprehensive obstacle detection system is required before this implementation could be used for real world applications. The Nova Carter robot does have a 3D LiDAR sensor which would be more than capable of addressing the forklift forks issue. Or a YOLOv8n vision model, custom trained to detect forklifts, could be used in combination with the front camera to identify forklifts and mark the area around the forks on the robot’s occupancy map (Jiang, L. and Wu, L., 2024, pp. 10-13). Also, the <code>plannerHybridAStar</code> tool in the MATLAB robotics toolbox would effectively address the obstacle avoidance issues that are currently experienced.</p>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<p>Carreira, R. et al. (2024) “A ROS2-Based Gateway for Modular Hardware Usage in Heterogeneous Environments,” Sensors (Basel, Switzerland), 24(19). Available at: https://doi.org/10.3390/s24196341.</p>
<p>Ghasemieh, A. and Kashef, R. (2024) “Towards explainable artificial intelligence in deep vision-based odometry,” Computers and Electrical Engineering, 115. Available at: https://doi.org/10.1016/j.compeleceng.2024.109127.</p>
<p>Ghorbani, S. and Janabi-Sharifi, F. (2022) “Extended Kalman Filter State Estimation for Aerial Continuum Manipulation Systems,” IEEE Sensors Letters, 6(8). Available at: https://doi.org/10.1109/LSENS.2022.3190890.</p>
<p>Jiang, L. and Wu, L. (2024) “Enhanced Yolov8 network with Extended Kalman Filter for wildlife detection and tracking in complex environments,” Ecological Informatics, 84. Available at: https://doi.org/10.1016/j.ecoinf.2024.102856.</p>
<p>Nvidia Omniverse IsaacSim (2025) Isaac Sim Reference Architecture. Available at: https://docs.omniverse.nvidia.com/isaacsim/latest/isaac_sim_reference_architecture.html#isaac-sim-reference-architecture (Accessed: 11 January 2025).</p>
<p>Vitali, R.V., McGinnis, R.S. and Perkins, N.C. (2021) “Robust Error-State Kalman Filter for Estimating IMU Orientation,” IEEE Sensors Journal, 21(3). Available at: https://doi.org/10.1109/JSEN.2020.3026895.</p>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden" data-render-id="quarto-int-sidebar-title">portfolio</span> <span class="hidden" data-render-id="quarto-int-navbar-title">portfolio</span> <span class="hidden" data-render-id="quarto-int-navbar:About">About</span> <span class="hidden" data-render-id="quarto-int-navbar:/about.html">/about.html</span> <span class="hidden" data-render-id="quarto-int-navbar:https://github.com/brandon-toews">https://github.com/brandon-toews</span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden" data-render-id="quarto-metatitle">portfolio - Sensor Fusion Using EKF for Navigation and Path Planning</span> <span class="hidden" data-render-id="quarto-twittercardtitle">portfolio - Sensor Fusion Using EKF for Navigation and Path Planning</span> <span class="hidden" data-render-id="quarto-ogcardtitle">portfolio - Sensor Fusion Using EKF for Navigation and Path Planning</span> <span class="hidden" data-render-id="quarto-metasitename">portfolio</span> <span class="hidden" data-render-id="quarto-twittercarddesc">Implementation of sensor fusion using Extended Kalman Filter for autonomous robot navigation and path planning in warehouse environments using Isaac Sim.</span> <span class="hidden" data-render-id="quarto-ogcardddesc">Implementation of sensor fusion using Extended Kalman Filter for autonomous robot navigation and path planning in warehouse environments using Isaac Sim.</span></p>
</div>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->

</body>

</html>