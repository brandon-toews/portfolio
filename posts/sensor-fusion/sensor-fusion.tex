% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Sensor Fusion Using EKF for Navigation and Path Planning},
  pdfauthor={Brandon Toews},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Sensor Fusion Using EKF for Navigation and Path Planning}
\author{Brandon Toews}
\date{2025-01-13}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, borderline west={3pt}{0pt}{shadecolor}, frame hidden, breakable, boxrule=0pt, interior hidden, enhanced]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\includegraphics{3rd-person-view.png} ::: \{.callout-tip
collapse=``true''\} \#\# Video Walkthrough

\includegraphics{SensorFusion_Video_Walkthrough.mp4} :::

\hypertarget{simulation-setup-and-kinematics-implementation}{%
\section{Simulation Setup and Kinematics
Implementation}\label{simulation-setup-and-kinematics-implementation}}

\hypertarget{robot-kinematics}{%
\subsection{Robot Kinematics}\label{robot-kinematics}}

\textbf{Chosen Robot:} Nova Carter

\textbf{Kinematic Model:} Differential Drive

\textbf{Justification:}

\begin{itemize}
\tightlist
\item
  Simplicity and ease of implementation
\item
  Suitable for indoor environments and small-scale robotic platforms
\item
  Precise control over direction and speed with minimal mechanical
  complexity
\item
  Widely applicable for wheeled robots performing navigation tasks in
  structured environments
\end{itemize}

The kinematic model for differential drive robots, like the Nova Carter
robot in this implementation, is expressed in Equation 1 and is
responsible for explaining the movement of the robot. It describes how
the robot's position (x, y) and orientation θ evolve with respect to
linear and angular velocities.

\begin{equation}\protect\hypertarget{eq-kinematic-model}{}{
\begin{aligned}
\dot{x} &= v \cdot \cos(\theta) \\
\dot{y} &= v \cdot \sin(\theta) \\
\dot{\theta} &= \omega
\end{aligned}
}\label{eq-kinematic-model}\end{equation}

Where:

\begin{itemize}
\tightlist
\item
  v is the linear velocity (from the Twist message)
\item
  ω is the angular velocity
\item
  θ is the robot's orientation
\end{itemize}

The wheel velocities \(v_r\) and \(v_l\) are derived using:

\begin{equation}\protect\hypertarget{eq-wheel-velocity}{}{
\begin{aligned}
v_r &= \frac{2v + \omega L}{2R} \\
v_l &= \frac{2v - \omega L}{2R}
\end{aligned}
}\label{eq-wheel-velocity}\end{equation}

Where:

\begin{itemize}
\tightlist
\item
  L = 0.413m (wheel distance)
\item
  R = 0.14m (wheel radius)
\end{itemize}

The simulated environment, discussed in greater detail in the next
section (Section~\ref{sec-environment-setup}), is run in Isaac Sim.
Isaac Sim's differential drive model uses OmniGraph nodes, specifically
the Differential Controller Node, to compute wheel velocities from
linear and angular velocity inputs using the wheel velocity equations
(Equation~\ref{eq-wheel-velocity}) and kinematic parameters specified in
the node (Figure~\ref{fig-differential-controller}). The differential
drive path controller MATLAB script calculates linear and angular
velocities from target positions in the planned path and sends these to
the controller in Isaac Sim through a ROS2 bridge.

\begin{figure}

{\centering \includegraphics{differential-controller.png}

}

\caption{\label{fig-differential-controller}Nova Carter Differential
Drive Parameters in Isaac Sim}

\end{figure}

\textbf{Kinematic Parameters:}

\begin{itemize}
\tightlist
\item
  \textbf{wheelDistance:} 0.413 meters (distance between the wheels)
\item
  \textbf{wheelRadius:} 0.14 meters (radius of the wheels)
\item
  \textbf{maxLinearSpeed:} 2.0 m/s
\item
  \textbf{maxAngularSpeed:} 3.0 rad/s
\item
  \textbf{maxAcceleration / maxDeceleration:} 2.0 m/s² (limits for
  smooth motion)
\item
  \textbf{dt:} 0.01667 (time step, approximately 60 Hz simulation rate)
\end{itemize}

\begin{figure}

{\centering \includegraphics{robot-dimensions.png}

}

\caption{\label{fig-robot-dimensions}Robot Dimensions}

\end{figure}

\hypertarget{sec-environment-setup}{%
\subsection{Environment Setup}\label{sec-environment-setup}}

The environment is simulated within Isaac Sim as it provides a realistic
scene which is highly useful for practically applying theoretical
robotics concepts. Isaac Sim is a high-fidelity physics engine that is
capable of simulating commercial and industrial robots, robotic
movement, and sensor data making it a prime candidate for this
simulation (Nvidia Omniverse IsaacSim, 2025). The scene is a warehouse
with forklifts, pallets, and shelving for the robot to navigate through.
These static obstacles have been placed to create a maze-like
environment that the robot can plan paths and navigate through. On
running the main MATLAB script, a robot object is created and given a
goal position to navigate to. The user can observe the robot moving
towards the specified goal through the viewport in Isaac Sim
(Figure~\ref{fig-3rd-person-view}), front camera viewport in RViz2
(Figure~\ref{fig-1st-person-view}), and the robot's occupancy map
displaying the planned path and the robot's pose along the way
(Figure~\ref{fig-occupancy-map}).

\begin{figure}

{\centering \includegraphics{3rd-person-view.png}

}

\caption{\label{fig-3rd-person-view}3rd Person View in Isaac Sim
Viewport}

\end{figure}

\begin{figure}

{\centering \includegraphics{1st-person-view.png}

}

\caption{\label{fig-1st-person-view}1st Person View in RViz2 Viewport}

\end{figure}

\begin{figure}

{\centering \includegraphics{occupancy-map.png}

}

\caption{\label{fig-occupancy-map}Ground Truth Occupancy Map (Left),
Robot's LiDAR updated Occupancy Map (Right)}

\end{figure}

\hypertarget{sensor-integration-and-data-acquisition}{%
\section{Sensor Integration and Data
Acquisition}\label{sensor-integration-and-data-acquisition}}

\hypertarget{sensor-selection}{%
\subsection{Sensor Selection}\label{sensor-selection}}

\textbf{Chassis Odometry Sensor:} Collects position (x, y) and
quaternion orientation (x, y, z, w) based on wheel encoder data, used in
combination with IMU for more accurate localization. GPS signals are
significantly weakened or blocked by warehouse roofs, walls, and metal
racking (Ghasemieh, A. and Kashef, R., 2024, p.~2, para. 3). On the
other hand, odometry works independently of external signals, making it
ideal for indoor environments. GPS typically has 5-10 meter accuracy,
while odometry typically achieves centimeter-level precision for short
distances (Ghasemieh, A. and Kashef, R., 2024, p.~3, para. 1). This
precision is crucial for warehouse operations like navigating narrow
aisles. Odometry provides immediate feedback while GPS can have
significant delay in position updates. This allows for quick response
time, which is essential for obstacle avoidance and precise positioning.
The system compensates for odometry's main weakness, drift, by fusing it
with IMU data through the Extended Kalman Filter and using LiDAR for
correction against discovered landmarks.

\textbf{Chassis IMU Sensor:} Is a combined sensor package that usually
includes accelerometers (measuring linear acceleration) and gyroscopes
(measuring angular velocity/rotation). These measurements are
particularly valuable for detecting rapid changes in the robot's motion
and helping to correct for wheel slippage that might occur on the
warehouse floor. The acceleration data is integrated over time to
estimate velocity, while the angular velocity provides direct
measurement of the robot's rotation rate. The advantage of using an IMU
in a warehouse setting is that it combines these sensors into a single
calibrated package. Because of this, accelerometer and gyroscope
readings are perfectly synchronized with a single timestamp for all
measurements making it easier for sensor fusion. Sensors are
pre-calibrated relative to each other ensuring alignment between
acceleration and rotation axes. Finally, there is considerable space and
cost efficiency in using a single component for mounting and wiring.

\textbf{Front 2D LiDAR Sensor:} Measures distances in a single
horizontal plane with accompanying angle of measurement. 2D LiDAR
generates significantly less data to process than a 3D LiDAR sensor
which allows for more responsive occupancy grid updates. Real-time
performance is crucial for warehouse navigations that include dynamic
obstacles. This data undergoes several processing steps, including range
validation and conversion to Cartesian coordinates, before being used to
update the robot's internal map and assist in obstacle avoidance.

\hypertarget{data-acquisition}{%
\subsection{Data Acquisition}\label{data-acquisition}}

The heart of our data collection system lies in its ROS2-based
architecture. ROS2 provides a robust framework for handling real-time
sensor data through its publisher-subscriber model (Carreira, R. et al.,
2024, pp.~11-12). Each sensor communicates through dedicated ROS2 topics
controlled with a precise timing mechanism that ensures consistent data
sampling across all sensors. Operating at 60Hz (with a time step of
0.01667 seconds), this timer-based approach synchronizes data collection
and processing, crucial for maintaining accurate state estimation. This
high update rate allows the robot to respond quickly to changes in its
environment while maintaining smooth motion control. To facilitate
system analysis and improvement, we've implemented a comprehensive data
logging system. This system records not only the raw sensor readings but
also the processed state estimates and ground truth data for comparison.
The logged data proves invaluable for post-mission analysis, allowing us
to evaluate system performance and identify areas for improvement.

\hypertarget{sensor-fusion-using-extended-kalman-filter-ekf}{%
\section{Sensor Fusion Using Extended Kalman Filter
(EKF)}\label{sensor-fusion-using-extended-kalman-filter-ekf}}

\hypertarget{fusion-techniques}{%
\subsection{Fusion Techniques}\label{fusion-techniques}}

The Standard Kalman Filter assumes the state transition and observation
models are linear. The kinematic model employed for the differential
drive is inherently non-linear due to the trigonometric functions cos(θ)
and sin(θ) (Equation~\ref{eq-kinematic-model}). The state transition
model (Equation~\ref{eq-state-transition}) describes transitions from
states by combining the state definition
(Equation~\ref{eq-state-definition}) with the non-linear kinematic model
(Equation~\ref{eq-kinematic-model}). EKF solves this problem by
linearizing the non-linear system at each time step by computing
Jacobian matrices (Jiang, L. and Wu, L., 2024, p.~2, para. 1). The state
transition Jacobian matrix (Equation~\ref{eq-state-jacobian}) is
calculated by considering the state transition model
(Equation~\ref{eq-state-transition}) as f(x) and finding the partial
derivatives for each function of f(x) with respect to each term in the
state definition model (Equation~\ref{eq-state-definition}).

\begin{equation}\protect\hypertarget{eq-state-definition}{}{
\mathbf{x} = \begin{bmatrix} x \\ y \\ \theta \\ v \\ \omega \end{bmatrix}
}\label{eq-state-definition}\end{equation}

\begin{equation}\protect\hypertarget{eq-state-transition}{}{
\begin{aligned}
x_{k+1} &= x_k + v \cdot \Delta t \cdot \cos(\theta) \\
y_{k+1} &= y_k + v \cdot \Delta t \cdot \sin(\theta) \\
\theta_{k+1} &= \theta_k + \omega \cdot \Delta t \\
v_{k+1} &= v_k \\
\omega_{k+1} &= \omega_k
\end{aligned}
}\label{eq-state-transition}\end{equation}

\begin{equation}\protect\hypertarget{eq-state-jacobian}{}{
\mathbf{F} = \begin{bmatrix}
1 & 0 & -v \cdot \Delta t \cdot \sin(\theta) & \Delta t \cdot \cos(\theta) & 0 \\
0 & 1 & v \cdot \Delta t \cdot \cos(\theta) & \Delta t \cdot \sin(\theta) & 0 \\
0 & 0 & 1 & 0 & \Delta t \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}
}\label{eq-state-jacobian}\end{equation}

The observation models are linear, and we add measurement noise to
explain the uncertainty in the readings from the sensors for things like
drift or wheel slippage (Equation~\ref{eq-odom-observation} \&
Equation~\ref{eq-imu-observation}). Since we need to perform matrix
calculations because of having to linearize the state transition model
with a Jacobian matrix, we also need to use Jacobians of the observation
models to apply to the state transition Jacobian
(Equation~\ref{eq-state-jacobian}). The observation model Jacobians
(Equation~\ref{eq-odom-jacobian} \& Equation~\ref{eq-imu-jacobian}) are
calculated by considering the observation models
(Equation~\ref{eq-odom-observation} \&
Equation~\ref{eq-imu-observation}) as h(x) and finding the partial
derivatives for each function of h(x) with respect to each term in the
state definition model (Equation~\ref{eq-state-definition}).

\begin{equation}\protect\hypertarget{eq-odom-observation}{}{
\mathbf{z}_{odom} = \mathbf{h}_{odom}(\mathbf{x}) = \begin{bmatrix} x \\ y \\ \theta \\ v \\ \omega \end{bmatrix}
}\label{eq-odom-observation}\end{equation}

\begin{equation}\protect\hypertarget{eq-imu-observation}{}{
\mathbf{z}_{imu} = \mathbf{h}_{imu}(\mathbf{x}) = \begin{bmatrix} v \\ \omega \end{bmatrix}
}\label{eq-imu-observation}\end{equation}

\begin{equation}\protect\hypertarget{eq-odom-jacobian}{}{
\mathbf{H}_{odom} = \frac{\partial \mathbf{h}_{odom}}{\partial \mathbf{x}} = \begin{bmatrix}
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}
}\label{eq-odom-jacobian}\end{equation}

\begin{equation}\protect\hypertarget{eq-imu-jacobian}{}{
\mathbf{H}_{imu} = \frac{\partial \mathbf{h}_{imu}}{\partial \mathbf{x}} = \begin{bmatrix}
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}
}\label{eq-imu-jacobian}\end{equation}

Finally, we perform the prediction step of the state transition where we
apply the process noise to the state transition Jacobian to model the
uncertainty of the prediction and update the predicted state covariance
(Equation~\ref{eq-prediction}). Then we incorporate the measurement.
Then we update the observed measurements using the observation
Jacobians, measurement noise, and predicted state covariance to create
the Kalman gain filter. This filter is then used to weigh the
contribution of the predicted state transition against the measurement
observations in the final outcome of the state prediction. The filter
bases these contributions of the predicted state transition and
measurement observations on the respective covariances, or the
uncertainty, of each (Equation~\ref{eq-measurement-update}).*

\begin{equation}\protect\hypertarget{eq-prediction}{}{
\begin{aligned}
\mathbf{x}_{k+1|k} &= \mathbf{F} \cdot \mathbf{x}_k \\
\mathbf{P}_{k+1|k} &= \mathbf{F} \cdot \mathbf{P}_k \cdot \mathbf{F}^T + \mathbf{Q}
\end{aligned}
}\label{eq-prediction}\end{equation}

\begin{equation}\protect\hypertarget{eq-measurement-update}{}{
\begin{aligned}
\mathbf{y} &= \mathbf{z} - \mathbf{H} \cdot \mathbf{x}_{k+1|k} \\
\mathbf{S} &= \mathbf{H} \cdot \mathbf{P}_{k+1|k} \cdot \mathbf{H}^T + \mathbf{R} \\
\mathbf{K} &= \mathbf{P}_{k+1|k} \cdot \mathbf{H}^T \cdot \mathbf{S}^{-1} \\
\mathbf{x}_{k+1} &= \mathbf{x}_{k+1|k} + \mathbf{K} \cdot \mathbf{y} \\
\mathbf{P}_{k+1} &= (\mathbf{I} - \mathbf{K} \cdot \mathbf{H}) \cdot \mathbf{P}_{k+1|k}
\end{aligned}
}\label{eq-measurement-update}\end{equation}

*This measurement update step is done twice in the code implementation,
once for the odometry measurements and again for IMU measurements.

\hypertarget{application-in-navigation}{%
\subsection{Application in Navigation}\label{application-in-navigation}}

Ultimately, EKF is an appropriate method for sensor fusion in this case
because it can effectively combine IMU and Odometry readings to improve
state estimation and subsequent navigation accuracy. Not only this, EKF
can simultaneously handle the non-linearity introduced by the state
transition model to produce a reasonable level of accuracy for the task
of navigating in a warehouse environment. Artificial Neural Networks
(ANNs) are another viable option for sensor fusion as they too can
combine sensor readings and manage non-linear associations quite well.
However, ANNs are typically more computationally expensive than EKF and
require an undetermined amount of training and fine tuning to produce
sufficient accuracy resulting in longer development durations. As a
result, EKF has been used in this simulation to produce a lightweight
and accurate solution suitable for the hardware resource and real-time
computational constraints of the project.

As seen depicted in Figure~\ref{fig-fusion-accuracy}, EKF sensor fusion
provides the robot with accurate enough pose estimations, in relation to
ground truth, to allow for the robot to make properly informed decisions
about velocities and headings. Whereas, when the robot does not have a
consistent and accurate idea of where it is and how it is oriented in
space the velocity adjusts become more erratic and susceptible to bias
over time. The redundancy provided by data collected from multiple
sensors measuring similar types of orientation information provides a
navigation framework that is resilient against sensor noise and/or
failure. Individual sensor noise fluctuations are not correlated with
one another, permitting a more well-rounded estimation of state at any
given time. We will delve into how well this EKF implementation performs
by examining in comparison with alternate methods of pose estimation.

\begin{figure}

{\centering \includegraphics{fusion-accuracy.png}

}

\caption{\label{fig-fusion-accuracy}Sensor Fusion Accuracy Compared to
A* Planned Path \& Ground Truth}

\end{figure}

\hypertarget{simulation-results-and-comparison}{%
\section{Simulation Results and
Comparison}\label{simulation-results-and-comparison}}

\hypertarget{performance-metrics}{%
\subsection{Performance Metrics}\label{performance-metrics}}

We provide three (3) different modes of state estimation to compare the
effects of measurement noise on navigation and to illustrate just how
effective EKF is at addressing this. First, a dead reckoning
implementation that uses only control inputs and the state transition
function to track orientation and navigate. Second, odometry-only state
estimation that doesn't compensate for the simulated sensor noise of the
odometer. Lastly, the EKF solution that fuses odometry and IMU readings,
models sensor and process uncertainty, and that elegantly combines state
transition predictions with the observed measurements. We will look at:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How accurate the modes are at estimating their state in comparison
  with the ground truth state.
\item
  How accurate the modes are at following the planned path, calculated
  from an A* algorithm taken from the MATLAB robotics toolbox. This will
  be a comparison of the ground truth and the planned path as the ground
  truth is a representation of what the robot actually did.
\item
  Navigation and path error probability distributions, bar charts
  depicting mean and maximum path deviation, path length, completion
  time.
\end{enumerate}

\hypertarget{analysis-of-results}{%
\subsection{Analysis of Results}\label{analysis-of-results}}

As can be seen in Figure~\ref{fig-dead-reckoning-trajectory} and
Figure~\ref{fig-dead-reckoning-error}, dead reckoning is highly
inaccurate in pose estimation and is subsequently unable to follow the
planned path in any practical capacity. It seems that without sensor
data to interoceptively inform the robot of its orientation it is
overcome by sensor noise, bias, and process noise.

\begin{figure}

{\centering \includegraphics{dead-reckoning-trajectory.png}

}

\caption{\label{fig-dead-reckoning-trajectory}Dead Reckoning - Accuracy
Compared to A* Planned Path, Ground Truth, \& State Estimation}

\end{figure}

\begin{figure}

{\centering \includegraphics{dead-reckoning-error.png}

}

\caption{\label{fig-dead-reckoning-error}Dead Reckoning - Pose Estimate
Error Probability Distributions Compared to Ground Truth}

\end{figure}

Odometry-only is a marked improvement over dead reckoning on all
accounts and performs reasonably well considering we are only using one
(1) sensor (Figure~\ref{fig-odom-only-trajectory} \&
Figure~\ref{fig-odom-only-error}). The robot's state estimation is
erratic but not so much so that it cannot follow the planned path to
reach the goal pose. This mode of estimation may be sufficient in
certain environments and with sensors that do not produce high levels of
noise. However, as sensor noise increases the performance of this mode
of estimation decreases, causing many overcompensations to velocities
and heading (Figure~\ref{fig-ekf-trajectory}). These frequent and
powerful corrections would likely cause more wear and tear on the
vehicle.

\begin{figure}

{\centering \includegraphics{odom-only-trajectory.png}

}

\caption{\label{fig-odom-only-trajectory}Odometry Only - Accuracy
Compared to A* Planned Path, Ground Truth, \& State Estimation}

\end{figure}

\begin{figure}

{\centering \includegraphics{odom-only-error.png}

}

\caption{\label{fig-odom-only-error}Odometry Only - Pose Estimate Error
Probability Distributions Compared to Ground Truth}

\end{figure}

The EKF solution performs the best of all three (3), showcasing a robust
ability to handle measurement noise in real-time. This mode of state
estimation boasts the least amount of error in all metrics across the
board with the exception of completion time. EKF produces tighter error
probability distributions with equal or lower magnitudes than the other
modes of state estimation. The EKF ground truth has a maximum deviation
from the planned path almost half that of, and even a few points away
from being equal to the mean deviation of the odometry-only solution
(Table~\ref{tbl-path-statistics}). As we manually increase measurement
noise, we can see how the EKF still maintains stability and accuracy
while odometry-only suffers from even greater fluctuations in state
estimation (Figure~\ref{fig-odom-high-noise} \&
Figure~\ref{fig-ekf-high-noise}). Even though EKFs tend to be
computationally expensive they do produce remarkable results in contexts
like these (Vitali, R.V., McGinnis, R.S. and Perkins, N.C., 2021,
pp.~1-2).

\begin{figure}

{\centering \includegraphics{ekf-trajectory.png}

}

\caption{\label{fig-ekf-trajectory}EKF - Accuracy Compared to A* Planned
Path, Ground Truth, \& State Estimation}

\end{figure}

\begin{figure}

{\centering \includegraphics{ekf-error.png}

}

\caption{\label{fig-ekf-error}EKF - Pose Estimate Error Probability
Distributions Compared to Ground Truth}

\end{figure}

\begin{figure}

{\centering \includegraphics{all-ground-truths.png}

}

\caption{\label{fig-all-ground-truths}All Ground Truths Compared to A*
Planned Path}

\end{figure}

\begin{figure}

{\centering \includegraphics{all-estimations.png}

}

\caption{\label{fig-all-estimations}All Estimation Methods' Estimations,
Ground Truths, A* Planned Path}

\end{figure}

\begin{figure}

{\centering \includegraphics{path-comparison.png}

}

\caption{\label{fig-path-comparison}All Estimation Methods' Ground
Truths Against A* Planned Path}

\end{figure}

\hypertarget{tbl-path-error}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0741}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2315}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2407}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2315}}@{}}
\caption{\label{tbl-path-error}All Estimation Methods' Ground Truths
Against A* Planned Path Error}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Position Error (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RMS Position Error (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Heading Error (rad)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RMS Heading Error (rad)
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Position Error (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RMS Position Error (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Heading Error (rad)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RMS Heading Error (rad)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fusion} & 0.043518 & 0.049445 & 0.014593 & 0.017925 \\
\textbf{Odometry Only} & 0.066875 & 0.076514 & 0.015236 & 0.019748 \\
\textbf{Dead Reckoning} & 0.72064 & 0.79609 & 0.29554 & 0.35184 \\
\end{longtable}

\hypertarget{tbl-path-statistics}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0462}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1098}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1098}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.0983}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1214}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.2023}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1040}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1040}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1040}}@{}}
\caption{\label{tbl-path-statistics}All Estimation Methods' Ground
Truths Against A* Planned Path Statistics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Deviation (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Max Deviation (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Path Length (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Completion Time (s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Path Smoothness (avg heading change)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time within 0.1m
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time within 0.2m
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time within 0.5m
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean Deviation (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Max Deviation (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Path Length (m)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Completion Time (s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Path Smoothness (avg heading change)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time within 0.1m
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time within 0.2m
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Time within 0.5m
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fusion} & 0.12613 & 0.18047 & 2.1128 & 0.96 & 0.030647 rad &
23.5294\% & 100\% & 100\% \\
\textbf{Odometry Only} & 0.1577 & 0.35182 & 2.172 & 0.84 & 0.021918 rad
& 42.8571\% & 66.6667\% & 100\% \\
\textbf{Dead Reckoning} & 0.25327 & 0.39551 & 1.0653 & 0.68 & 0.042377
rad & 6.0606\% & 45.4545\% & 100\% \\
\end{longtable}

\begin{figure}

{\centering \includegraphics{odom-high-noise.png}

}

\caption{\label{fig-odom-high-noise}Odometry Only - Higher Noise
Accuracy Compared to A* Planned Path \& Ground Truth}

\end{figure}

\begin{figure}

{\centering \includegraphics{ekf-high-noise.png}

}

\caption{\label{fig-ekf-high-noise}EKF - Higher Noise Accuracy Compared
to A* Planned Path \& Ground Truth}

\end{figure}

\hypertarget{evaluation-and-improvement-suggestions}{%
\section{Evaluation and Improvement
Suggestions}\label{evaluation-and-improvement-suggestions}}

\hypertarget{critical-evaluation}{%
\subsection{Critical Evaluation}\label{critical-evaluation}}

While EKF clearly outperforms the other methods of state estimation
explored in this paper, it is not without its challenges. Tuning the
process noise manually is a time-consuming trial-and-error endeavor even
with data analytics to inform the adjustments. Meanwhile, deploying and
developing Artificial Neural Networks (ANN) are becoming easier with
support from libraries like PyTorch and are more than capable of
handling the non-linear nature of state estimations and transitions
(Ghorbani, S. and Janabi-Sharifi, F., 2022, p.1, para. 5). The current
EKF implementation could take more full advantage of the IMU and
odometry orientation data by processing quaternion information directly
using quaternion specific equations rather than converting the
quaternion to θ at the time of collection (Vitali, R.V., McGinnis, R.S.
and Perkins, N.C., 2021, pp.~2-6).

As stated previously, the robot uses a 2D LiDAR sensor to update its
internal occupancy map which is crucial for navigating a dynamic
environment like a warehouse. The horizontal orientation of the obstacle
detection provided by this sensor presents issues in this context. Even
though the sensor is effectively mapping most of the obstacles in the
environment it is missing a key feature that results in collisions. If
forklift forks are raised off the ground, then the LiDAR doesn't detect
these long protrusions which routinely causes collisions with the robot.
Compounding the situation further, the robot's occupancy map cannot be
inflated as each time the LiDAR updates the map it would have to call
the inflate method causing already inflated objects to continue to grow
until enveloping the entire map. The current A* algorithm being used
doesn't have a minimum turning radius property, therefore, the planner
charts paths that bring the robot too close to objects causing
collisions, even with obstacles it registers in its occupancy map. Using
a high-fidelity simulation engine like Isaac Sim pushes these kinds of
real-world challenges to the fore providing the opportunity to address
these in a simulated environment before deployment. In conjunction with
this, usage of the ROS2 bridge for the environment setup facilitates
easy portability to real-world applications (Carreira, R. et al., 2024,
pp.~5-8).

\begin{figure}

{\centering \includegraphics{lidar-limitation.png}

}

\caption{\label{fig-lidar-limitation}2D LiDAR Doesn't Update Forklift
Forks onto Robot's Occupancy Map}

\end{figure}

\begin{figure}

{\centering \includegraphics{robot-collision.png}

}

\caption{\label{fig-robot-collision}Robot Collides with Forklift Forks}

\end{figure}

\hypertarget{improvements}{%
\subsection{Improvements}\label{improvements}}

A more comprehensive obstacle detection system is required before this
implementation could be used for real world applications. The Nova
Carter robot does have a 3D LiDAR sensor which would be more than
capable of addressing the forklift forks issue. Or a YOLOv8n vision
model, custom trained to detect forklifts, could be used in combination
with the front camera to identify forklifts and mark the area around the
forks on the robot's occupancy map (Jiang, L. and Wu, L., 2024,
pp.~10-13). Also, the \texttt{plannerHybridAStar} tool in the MATLAB
robotics toolbox would effectively address the obstacle avoidance issues
that are currently experienced.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

Carreira, R. et al.~(2024) ``A ROS2-Based Gateway for Modular Hardware
Usage in Heterogeneous Environments,'' Sensors (Basel, Switzerland),
24(19). Available at: https://doi.org/10.3390/s24196341.

Ghasemieh, A. and Kashef, R. (2024) ``Towards explainable artificial
intelligence in deep vision-based odometry,'' Computers and Electrical
Engineering, 115. Available at:
https://doi.org/10.1016/j.compeleceng.2024.109127.

Ghorbani, S. and Janabi-Sharifi, F. (2022) ``Extended Kalman Filter
State Estimation for Aerial Continuum Manipulation Systems,'' IEEE
Sensors Letters, 6(8). Available at:
https://doi.org/10.1109/LSENS.2022.3190890.

Jiang, L. and Wu, L. (2024) ``Enhanced Yolov8 network with Extended
Kalman Filter for wildlife detection and tracking in complex
environments,'' Ecological Informatics, 84. Available at:
https://doi.org/10.1016/j.ecoinf.2024.102856.

Nvidia Omniverse IsaacSim (2025) Isaac Sim Reference Architecture.
Available at:
https://docs.omniverse.nvidia.com/isaacsim/latest/isaac\_sim\_reference\_architecture.html\#isaac-sim-reference-architecture
(Accessed: 11 January 2025).

Vitali, R.V., McGinnis, R.S. and Perkins, N.C. (2021) ``Robust
Error-State Kalman Filter for Estimating IMU Orientation,'' IEEE Sensors
Journal, 21(3). Available at: https://doi.org/10.1109/JSEN.2020.3026895.



\end{document}
