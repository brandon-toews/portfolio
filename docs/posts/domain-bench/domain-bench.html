<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brandon Toews">
<meta name="dcterms.date" content="2025-06-16">
<meta name="description" content="A framework for evaluating Large Language Models in domain-specific contexts, addressing benchmark contamination, evaluation misalignment, and dynamic knowledge requirements.">

<title>portfolio - Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>


<link rel="stylesheet" href="../../styles.css">
<meta name="twitter:title" content="portfolio - Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation">
<meta name="twitter:description" content="A framework for evaluating Large Language Models in domain-specific contexts, addressing benchmark contamination, evaluation misalignment, and dynamic knowledge requirements.">
<meta name="twitter:image" content="https://brandon-toews.github.io/portfolio/posts/domain-bench/system-architecture.png">
<meta name="twitter:image-height" content="1326">
<meta name="twitter:image-width" content="1025">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">portfolio</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/brandon-toews" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source" data-quarto-source-url="https://github.com/brandon-toews/Domain_Bench"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          A framework for evaluating Large Language Models in domain-specific contexts, addressing benchmark contamination, evaluation misalignment, and dynamic knowledge requirements.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Large Language Models</div>
                <div class="quarto-category">Benchmarking</div>
                <div class="quarto-category">AI / Machine Learning</div>
                <div class="quarto-category">Cybersecurity</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Brandon Toews </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 16, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#motivations" id="toc-motivations" class="nav-link" data-scroll-target="#motivations"><span class="header-section-number">1.1</span> Motivations</a></li>
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement"><span class="header-section-number">1.2</span> Problem Statement</a></li>
  <li><a href="#research-questions" id="toc-research-questions" class="nav-link" data-scroll-target="#research-questions"><span class="header-section-number">1.3</span> Research Questions</a></li>
  <li><a href="#research-objectives" id="toc-research-objectives" class="nav-link" data-scroll-target="#research-objectives"><span class="header-section-number">1.4</span> Research Objectives</a></li>
  <li><a href="#scope" id="toc-scope" class="nav-link" data-scroll-target="#scope"><span class="header-section-number">1.5</span> Scope</a></li>
  <li><a href="#ethical-considerations" id="toc-ethical-considerations" class="nav-link" data-scroll-target="#ethical-considerations"><span class="header-section-number">1.6</span> Ethical Considerations</a></li>
  <li><a href="#layout" id="toc-layout" class="nav-link" data-scroll-target="#layout"><span class="header-section-number">1.7</span> Layout</a></li>
  </ul></li>
  <li><a href="#literature-review" id="toc-literature-review" class="nav-link" data-scroll-target="#literature-review"><span class="header-section-number">2</span> Literature Review</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">2.1</span> Background</a></li>
  <li><a href="#related-work" id="toc-related-work" class="nav-link" data-scroll-target="#related-work"><span class="header-section-number">2.2</span> Related Work</a>
  <ul class="collapse">
  <li><a href="#benchmark-contamination-issues" id="toc-benchmark-contamination-issues" class="nav-link" data-scroll-target="#benchmark-contamination-issues"><span class="header-section-number">2.2.1</span> Benchmark Contamination Issues</a></li>
  <li><a href="#misalignment-in-llm-evaluation-practices" id="toc-misalignment-in-llm-evaluation-practices" class="nav-link" data-scroll-target="#misalignment-in-llm-evaluation-practices"><span class="header-section-number">2.2.2</span> Misalignment in LLM Evaluation Practices</a></li>
  <li><a href="#newer-benchmarking-frameworks" id="toc-newer-benchmarking-frameworks" class="nav-link" data-scroll-target="#newer-benchmarking-frameworks"><span class="header-section-number">2.2.3</span> Newer Benchmarking Frameworks</a></li>
  <li><a href="#rag-multi-task-evaluation" id="toc-rag-multi-task-evaluation" class="nav-link" data-scroll-target="#rag-multi-task-evaluation"><span class="header-section-number">2.2.4</span> RAG &amp; Multi-Task Evaluation</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">2.3</span> Conclusions</a>
  <ul class="collapse">
  <li><a href="#maintaining-benchmark-integrity" id="toc-maintaining-benchmark-integrity" class="nav-link" data-scroll-target="#maintaining-benchmark-integrity"><span class="header-section-number">2.3.1</span> Maintaining Benchmark Integrity</a></li>
  <li><a href="#evolving-evaluation-to-reflect-real-usage" id="toc-evolving-evaluation-to-reflect-real-usage" class="nav-link" data-scroll-target="#evolving-evaluation-to-reflect-real-usage"><span class="header-section-number">2.3.2</span> Evolving Evaluation to Reflect Real Usage</a></li>
  <li><a href="#leveraging-tools-and-hybrid-approaches" id="toc-leveraging-tools-and-hybrid-approaches" class="nav-link" data-scroll-target="#leveraging-tools-and-hybrid-approaches"><span class="header-section-number">2.3.3</span> Leveraging Tools and Hybrid Approaches</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#output-design" id="toc-output-design" class="nav-link" data-scroll-target="#output-design"><span class="header-section-number">3</span> Output Design</a>
  <ul class="collapse">
  <li><a href="#methodologies" id="toc-methodologies" class="nav-link" data-scroll-target="#methodologies"><span class="header-section-number">3.1</span> Methodologies</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software"><span class="header-section-number">3.2</span> Software</a></li>
  <li><a href="#system" id="toc-system" class="nav-link" data-scroll-target="#system"><span class="header-section-number">3.3</span> System</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources"><span class="header-section-number">3.4</span> Resources</a></li>
  <li><a href="#evaluation-parameters" id="toc-evaluation-parameters" class="nav-link" data-scroll-target="#evaluation-parameters"><span class="header-section-number">3.5</span> Evaluation Parameters</a></li>
  <li><a href="#implementation-workflow" id="toc-implementation-workflow" class="nav-link" data-scroll-target="#implementation-workflow"><span class="header-section-number">3.6</span> Implementation Workflow</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="domain-bench.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><img src="system-architecture.png" class="img-fluid"></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video Walkthrough
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="video-walkthrough" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><video src="output.mp4" class="img-fluid" controls=""><a href="output.mp4">Video</a></video></p>
<figcaption class="figure-caption">Video Walkthrough</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Source Code
</div>
</div>
<div class="callout-body-container callout-body">
<p>View source code on Github <a href="https://github.com/brandon-toews/Domain_Bench">repository</a>.</p>
</div>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<section id="motivations" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="motivations"><span class="header-section-number">1.1</span> Motivations</h2>
<p>Large Language Models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities across diverse domains <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span>. Their increasing integration into critical sectors such as healthcare, law, and cybersecurity necessitates rigorous evaluation of their performance <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span>. While general benchmarks have contributed to progress, these benchmarks often fail to capture specific requirements crucial to specialized applications, indicating the need for domain-specific evaluation frameworks.</p>
</section>
<section id="problem-statement" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="problem-statement"><span class="header-section-number">1.2</span> Problem Statement</h2>
<p>Current benchmarking practices face several critical challenges. Benchmark data contamination (BDC), resulting from overlaps between training datasets and test benchmarks, significantly inflates performance metrics and misleads stakeholders about the true capabilities of a model <span class="citation" data-cites="golchin2024time">(<a href="#ref-golchin2024time" role="doc-biblioref">Golchin and Surdeanu 2024</a>)</span>. Many evaluation methods also use probability-based scoring in multiple-choice tests, which inadequately represents the true reasoning and generation capabilities of a model <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024</a>)</span>. Traditional static benchmarks do not adequately evaluate LLM performance in evolving knowledge domains, as demonstrated by substantial performance declines observed when models are tested against dynamically evolving benchmarks <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span>.</p>
</section>
<section id="research-questions" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="research-questions"><span class="header-section-number">1.3</span> Research Questions</h2>
<p>This research seeks to answer the following questions:</p>
<ul>
<li><p>How can a domain-adaptable LLM benchmarking framework mitigate common evaluation challenges such as data contamination and evaluation method misalignment?</p></li>
<li><p>To what extent does retrieval-based augmentation improve LLM performance in specialized tasks?</p></li>
<li><p>What multidimensional metrics best capture nuanced performance aspects of LLMs?</p></li>
<li><p>How can benchmarks adapt to evolving domain knowledge while maintaining consistent performance evaluation?</p></li>
</ul>
</section>
<section id="research-objectives" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="research-objectives"><span class="header-section-number">1.4</span> Research Objectives</h2>
<p>Aligned with these research questions, the objectives of this study are:</p>
<ul>
<li><p>Develop a modular and dynamic benchmarking framework that is adaptable to specialized domains.</p></li>
<li><p>Implement and evaluate multiple LLM architectures and augmentation strategies, including retrieval augmentation and knowledge graph integration.</p></li>
<li><p>Establish a comprehensive multimetric evaluation approach.</p></li>
<li><p>Create mechanisms for continual benchmark evolution to reflect updates in domain knowledge.</p></li>
</ul>
</section>
<section id="scope" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="scope"><span class="header-section-number">1.5</span> Scope</h2>
<p>This research focuses specifically on domain-specific benchmarking frameworks for LLMs demonstrated through cybersecurity compliance advisory tasks, addressing a rapidly evolving and complex knowledge domain. The applicability of this framework to similar specialized domains is also examined.</p>
</section>
<section id="ethical-considerations" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="ethical-considerations"><span class="header-section-number">1.6</span> Ethical Considerations</h2>
<p>Key ethical considerations include the following:</p>
<ul>
<li>Potential misinformation risks due to incorrect or misleading model output.</li>
<li>Transparency regarding known model limitations.</li>
<li>Fairness of evaluation across diverse domain requirements.</li>
<li>Privacy concerns addressed through the use of publicly available and synthetic data.</li>
<li>Mitigating automation bias by emphasizing justification and evidence-supported responses.</li>
</ul>
</section>
<section id="layout" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="layout"><span class="header-section-number">1.7</span> Layout</h2>
<p>The dissertation proceeds with a Literature Review examining current practices, emerging frameworks, and some distilled conclusions based on the review; followed by Output Design detailing the methodology, architecture, and evaluation parameters of the proposed framework; and concludes with comprehensive references.</p>
</section>
</section>
<section id="literature-review" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Literature Review</h1>
<section id="background" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="background"><span class="header-section-number">2.1</span> Background</h2>
<p>Benchmarking LLMs serves several critical functions. Benchmarks provide a solid basis for performance evaluations of how well LLMs perform across different capabilities such as reasoning, knowledge retrieval, language understanding, etc. They in turn allow researchers to quantify improvements over time and compare different model architectures and training techniques. Organizations can then use benchmark results to inform the appropriate selection of models for specific applications based on their strengths and weaknesses. Benchmarks highlight areas where models struggle, guiding future research efforts. Many benchmarks now include evaluations of harmful outputs, biases, and other safety concerns. Despite their importance and utility, LLM benchmarks face several significant challenges.</p>
<p>Benchmarking LLMs has historically been crucial for measuring progress and comparing models. Standard benchmarks such as General Language Understanding Evaluation (GLUE), SuperGLUE and Massive Multitask Language Understanding (MMLU) provided fixed datasets and tasks to evaluate core NLP capabilities, enabling consistent comparisons between models <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span>. These benchmarks focus on end-to-end metrics for tasks such as answering questions, translating, or common-sense reasoning, treating the model as a black box that produces an answer per query. This approach helped quantify improvements and highlight strengths and weaknesses of the model over time. Organizations could use benchmark results to guide model selection for applications and researchers could identify where models struggle to spur future advances. Over time, benchmarks also began to assess ethical and safety dimensions, such as bias and harmful output, to ensure responsible artificial intelligence (AI) development. In short, LLM benchmarks serve as standardized yardsticks for performance, driving the evolution of the field.</p>
<p>However, foundational benchmark practices come with assumptions and limitations. One such assumption is that improvements in model performance on high-profile benchmarks like MMLU, HumanEval, etc., are indicative of the model developing a deeper understanding or acquiring certain abilities. In addition, a common practice in LLM evaluation is using the predicted probabilities of a model to choose answers in tasks, especially multiple-choice questions, instead of letting the model generate an answer in natural language. Many benchmark evaluations, for efficiency, will have the model score each possible answer (option A, B, C, etc.) and pick the highest-probability option as its answer. This method is label-based or probability-based evaluation, as opposed to generation-based evaluation, where the model actually produces an answer, sometimes with an explanation, and that output is checked. The main reason why many current evaluation frameworks default to probability-based scoring is due to computational constraints, namely, it is faster and cheaper to get the probability of a model on a fixed set of answers than to generate text. The assumption is that this method of evaluation serves as a suitable proxy for the behavior of a model in generation-based tasks. Finally, long-standing statistical metrics such as accuracy for classification, F1, precision/recall, BLEU/ROUGE for generation, etc., have provided a way to objectively quantify performance. However, these statistical measures are limited in their ability to properly stratify and score nuanced outputs that often result from real-world application.</p>
<p>A growing trend in LLM research and application is the incorporation of tools, such as Retrieval-Augmented Generation (RAG), to enhance LLM capabilities and accuracy. A RAG system has (1) a retrieval component that fetches documents relevant to the query, and (2) a generation component that produces a final answer using both its internal knowledge and the retrieved context (see <a href="#fig-rag-process">Figure&nbsp;1</a>), with the aim to reducing hallucinations and keeping knowledge up-to-date <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024, 1</a>)</span>. Traditional LLM benchmarks assume that the model’s knowledge and reasoning are self-contained, that the model knows the answer from training or must infer it from a given prompt context. They do not explicitly test the model’s ability to retrieve and use external information, nor do they decompose performance into subtasks. For example, MMLU assesses knowledge across domains with multiple-choice questions, but a model’s score conflates knowledge recall and reasoning, without isolating whether an error was due to lack of information or incorrect reasoning. Similarly, holistic evaluation efforts like Holistic Evaluation of Language Models (HELM) provide diverse metrics (accuracy, calibration, bias, etc.), but still treat the model as a single black-box system producing an answer per query. In short, conventional benchmarks excel at measuring what answer the model gives, but not how it got that answer.</p>
<div id="fig-rag-process" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="rag-process.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer. Source: <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024, 3</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="related-work" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="related-work"><span class="header-section-number">2.2</span> Related Work</h2>
<section id="benchmark-contamination-issues" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="benchmark-contamination-issues"><span class="header-section-number">2.2.1</span> Benchmark Contamination Issues</h3>
<p>One critical threat to benchmark integrity is BDC, which is the leakage of test examples into the training data of a model. Xu et al. <span class="citation" data-cites="xu2024benchmark">(<a href="#ref-xu2024benchmark" role="doc-biblioref">Xu et al. 2024</a>)</span> underscore that BDC is widespread and often hard to detect, yet undermines the credibility of benchmark results. Essentially, many high-profile benchmarks have had some of their questions or answers seen by large models during training, for instance, models memorized solutions to portions of MMLU or HumanEval. The reasoning is that it leads to inflated evaluation scores as a model may appear to excel at a task not because it truly mastered the underlying skill, but because it recalls the answers from memory. The survey also finds that while some researchers originally argued that memorizing answers is not necessarily bad or unavoidable, the prevalent view is that it “poses significant challenges to the reliability and validity of LLM evaluations” <span class="citation" data-cites="xu2024benchmark">(<a href="#ref-xu2024benchmark" role="doc-biblioref">Xu et al. 2024, 1</a>)</span>. Although the survey is limited to only compiling known techniques, one gap they identify is the lack of a unified, systematic approach to defining and tackling BDC. They highlight that no single mitigation will solve the issue completely given the scale of LLM training data, which often sweeps up entire Internet archives.</p>
<p>An analysis by Golchin &amp; Surdeanu <span class="citation" data-cites="golchin2024time">(<a href="#ref-golchin2024time" role="doc-biblioref">Golchin and Surdeanu 2024</a>)</span> revealed specific contaminated benchmarks, for example, finding that datasets like AG News and XSum had leaked content when tested with GPT-4. These findings reinforce Xu et al. <span class="citation" data-cites="xu2024benchmark">(<a href="#ref-xu2024benchmark" role="doc-biblioref">Xu et al. 2024</a>)</span>’s conclusions that training data contamination is common and widespread across many tasks.</p>
</section>
<section id="misalignment-in-llm-evaluation-practices" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="misalignment-in-llm-evaluation-practices"><span class="header-section-number">2.2.2</span> Misalignment in LLM Evaluation Practices</h3>
<p>Another issue is the misalignment between how we evaluate LLMs in benchmarks and how LLMs are actually used in the real world. McIntosh et al. <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span> argue that many benchmarks fail to measure what we really care about. In their audit of 23 state-of-the-art benchmarks, they found that evaluations often rely on simplistic proxies or narrow setups that do not reflect genuine performance in deployment. For example, several benchmarks credit a model for getting the correct answer but do not check whether the model’s reasoning process made sense or if it just guessed patterns. Many benchmarks lack diversity in inputs or values, meaning a model can score well by overfitting to stereotyped prompts. McIntosh et al. <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span> observed issues such as cultural bias in test content and high sensitivity to prompt formatting, which indicates that benchmark scores might not translate to real-world reliability. This focus on final answers rather than the process means that models can sometimes ‘game’ benchmarks, thus achieving good scores through shortcuts or pattern matching rather than true understanding. In short, models could appear state-of-the-art on a leaderboard yet disappoint in practical usage because the evaluation was misaligned with real usage conditions. A limitation of the study by McIntosh et al. <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span> itself is that their critique is qualitative and does not provide a quantitative fix. However, the study unveils a diversity of issues present within benchmarking practices that highlight the need for intentional improvement.</p>
<p>A concrete example of evaluation misalignment is given by Lyu, Wu, &amp; Aji <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024</a>)</span>, which brings to light some compelling insights into the current limitations of predicted probability-based evaluations. The primary metrics considered in the study were (1) accuracy under each evaluation mode, probabilities-based evaluations versus generation-based, and (2) consistency between the two. They show that the probability-based method “inadequately aligns with generation-based prediction” <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024, 1</a>)</span> creating a misrepresentation of the performance and behavior of a model (see <a href="#fig-evaluation-methods">Figure&nbsp;2</a>). For example, the option that the model assigns highest probability is not always the one it would output when asked to explain or answer directly. The misalignment could manifest as the model having a hidden preference it does not act on when forced to choose via probabilities. Essentially, the authors treat the generation-based outcome as the ‘ground truth’ of what the model really believes or would do in practice, and they check how often the probability proxy matches that. This underscores that the convenience and cost efficiency of probability-based evaluations come at the cost of not fully understanding the real-world behavior of a model. A limitation of Lyu, Wu, &amp; Aji <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024</a>)</span>‘s study is that they assume generation-based evaluation is the ’ground truth’ measure of performance, but generation brings its own uncertainties. For example, a model might generate a correct answer phrased differently from the expected answer, and automatic evaluation could count that wrong unless carefully handled. They do note that evaluating generation often requires careful parsing or human judgment, which is why probabilistic methods gained popularity in the first place, but they do not propose solutions to ameliorate the extra costs of doing so.</p>
<div id="fig-evaluation-methods" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="evaluation-methods.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;2<strong>.</strong> An illustration of label-based, sequence-based and generation-based predictions for evaluating LLMs on NLP benchmarks. Source: <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024, 2</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="newer-benchmarking-frameworks" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="newer-benchmarking-frameworks"><span class="header-section-number">2.2.3</span> Newer Benchmarking Frameworks</h3>
<p>Xia, Deng, &amp; L. Zhang <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span> tackle the problem of benchmarks becoming overfitted by introducing EVOEVAL, an approach to dynamically evolve coding challenges (see <a href="#fig-evoeval-pipeline">Figure&nbsp;3</a>). They start from popular coding benchmarks like HumanEval and MBPP and use LLMs themselves to generate new variations of these programming problems. The idea is to create challenges that are similar in spirit but sufficiently different in surface details or domain, so that an LLM which memorized the original solutions will be caught off guard. The study uses standard coding task metrics, such as the pass rate of generated code against unit tests (e.g.&nbsp;pass@k metrics), to evaluate model proficiency. However, they compared metrics on the original benchmarks versus the evolved ones. A key finding in their analysis is the drop in performance (%) when switching to evolved tasks. Models that previously topped the coding leaderboards saw absolute drops in accuracy of 20%–47% in the evolved problems, and many fell dramatically in ranking. This reveals that existing coding benchmarks probably overestimated true model competency, as models had effectively overfit on the narrow distribution or even leaked solutions. Xia, Deng, &amp; L. Zhang <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span> highlight phenomena like brittleness to slight rewording, for instance, a prompt asking for a solution “in two sentences” might confuse a model that learned to expect a certain format. Some EVOEVAL tasks require combining two simpler tasks, which many models struggled with, showing weakness in multistep reasoning or code synthesis. Some notable limitations are: (1) using an LLM to generate benchmarks might introduce its own biases or errors. (2) The authors had to ensure that the new problems were neither trivial variations nor unsolvable; this probably required manual curation, which could be time consuming. (3) EVOEVAL is specifically focused on the domain of coding. Although their findings pertain to code benchmarks, it is entirely possible that these trends are prevalent in other domains but would require their own “evolution” to prove definitively. (4) Finally, as models improve or new training data appear, possibly including EVOEVAL itself in the future, this approach would have to continually generate further evolved tasks.</p>
<div id="fig-evoeval-pipeline" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="evoeval-pipeline.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;3<strong>.</strong> Overview of EVOEVAL evolving problem generation pipeline. Source: <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024, 4</a>)</span></figcaption>
</figure>
</div>
<p>Dalvi et al. <span class="citation" data-cites="dalvi2024llmebench">(<a href="#ref-dalvi2024llmebench" role="doc-biblioref">Dalvi et al. 2024</a>)</span> take a very practical angle by introducing LLMeBench, a benchmarking framework aimed at making LLM evaluation flexible, extensible, and efficient. LLMeBench comes with generic dataset loaders, supports multiple model providers, and has many pre-implemented standard evaluation metrics (see <a href="#fig-llmebench-architecture">Figure&nbsp;4</a>). Importantly, it supports in-context learning setups like zero-shot and few-shot, meaning it can automate prompts for models with given examples if needed. The framework does not invent new metrics, but streamlines the use of existing ones such as accuracy for classification, F1, precision/recall, BLEU/ROUGE, etc. Another aspect is that by supporting multiple tasks, LLMeBench encourages the use of holistic evaluation. LLMeBench can easily run a model through a battery of tasks such as translation, question-answering, and reasoning puzzles using a single framework. Therefore, this framework fosters comprehensive evaluation rather than a single-metric focus. One limitation is that LLMeBench, while flexible, is only as good as the benchmarks one feeds into it, but it does not solve what to evaluate, it helps with how. If one were to use poor-quality or biased benchmark data, the framework would faithfully report metrics, but the insights would still depend on the input. In addition, LLMeBench will not automatically flag contamination issues or suggest new tasks and is therefore susceptible to the issues already raised. In terms of real-world applicability, LLMeBench addresses the barrier to entry, which could allow domain experts to focus more on writing good questions and let the tool handle the rest. McIntosh et al. <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span> noted that many benchmarks suffer from “implementation inconsistencies” and slow iteration, which LLMeBench directly tackles by providing a consistent implementation and enabling quick reruns of evaluations.</p>
<div id="fig-llmebench-architecture" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="llmebench-architecture.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;4<strong>.</strong> The architecture of the LLMeBench framework. The dotted boxes represent the core implemented modules of the architecture. Customization for new tasks, datasets, and models can be done on Dataset, Model Provider, Evaluation, and Asset modules. Source: <span class="citation" data-cites="dalvi2024llmebench">(<a href="#ref-dalvi2024llmebench" role="doc-biblioref">Dalvi et al. 2024, 1</a>)</span></figcaption>
</figure>
</div>
<p>To summarize key challenges in standard LLM benchmarking, <a href="#tbl-general-benchmarking">Table&nbsp;1</a> presents a comparative analysis of major studies on benchmark contamination, overfitting, and misalignment.</p>
<div id="tbl-general-benchmarking" class="anchored">
<table class="table">
<caption>Table&nbsp;1<strong>.</strong> General LLM Benchmarking Issues</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Paper &amp; Authors</th>
<th>Benchmarking Issue</th>
<th>Proposed Approach</th>
<th>Key Findings</th>
<th>Future Directions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>McIntosh et al.&nbsp;2024</strong></td>
<td>Overfitting in static benchmarks; misalignment with real-world use.</td>
<td>Analyzed 23 benchmarks, advocating for evolving test sets.</td>
<td>Benchmarks overestimate model reliability due to gaming.</td>
<td>Implementing truly dynamic benchmarks remains unsolved.</td>
</tr>
<tr class="even">
<td><strong>Xu et al.&nbsp;2024</strong></td>
<td>Test contamination leads to inflated scores.</td>
<td>Advocates systematic dataset filtering and leak detection.</td>
<td>Training data contamination is widespread.</td>
<td>Large-scale detection remains a challenge.</td>
</tr>
<tr class="odd">
<td><strong>Lyu, Wu, &amp; Aji 2024</strong></td>
<td>Misalignment between probability-based and free-form evaluation.</td>
<td>Compares probability-based vs.&nbsp;generated response accuracy.</td>
<td>Probability selection misrepresents true model ability.</td>
<td>Efficient generation-based scoring remains an open problem.</td>
</tr>
<tr class="even">
<td><strong>Xia, Deng, &amp; L. Zhang 2024</strong></td>
<td>Overfitting in static coding benchmarks.</td>
<td>EVOEVAL: Dynamically mutates coding tasks.</td>
<td>Models show 20-47% drop on evolved problems.</td>
<td>Needs broader application beyond coding.</td>
</tr>
<tr class="odd">
<td><strong>Dalvi et al.&nbsp;2024</strong></td>
<td>Fragmented evaluation tools.</td>
<td>LLMeBench: Unified multi-metric benchmarking.</td>
<td>Standardizes model comparisons.</td>
<td>Still depends on pre-existing datasets.</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="rag-multi-task-evaluation" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="rag-multi-task-evaluation"><span class="header-section-number">2.2.4</span> RAG &amp; Multi-Task Evaluation</h3>
<p>The survey by Gao et al. <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024</a>)</span> provides a comprehensive overview of RAG systems and specifically introduces metrics and benchmarks to assess RAG models alongside an up-to-date evaluation framework. RAG evaluation frameworks go one step further by explicitly measuring the different stages of a retrieve-and-generate pipeline (see <a href="#fig-rag-evaluation">Figure&nbsp;5</a>) <span class="citation" data-cites="es2023ragas">(<a href="#ref-es2023ragas" role="doc-biblioref">Es et al. 2023</a>)</span>. Evaluating such systems involves multidimensional metrics:</p>
<ul>
<li><strong>Retrieval quality</strong> – Is the retrieved context relevant and sufficient for the query?</li>
<li><strong>Generation quality</strong> – Is the final answer correct and is it faithful to the retrieved evidence?</li>
<li><strong>Integration performance</strong> – How well does the LLM incorporate the retrieved information? Does it avoid hallucination or ignore evidence?</li>
</ul>
<div id="fig-rag-evaluation" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="rag-evaluation.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;5<strong>.</strong> Overview of RAG Evaluations. Adapted from: <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024, 16</a>)</span></figcaption>
</figure>
</div>
<p>Recent RAG-specific evaluators such as Retrieval Augmented Generation Assessment (RAGAS) <span class="citation" data-cites="es2023ragas">(<a href="#ref-es2023ragas" role="doc-biblioref">Es et al. 2023</a>)</span> and the Automated RAG Evaluation System (ARES) <span class="citation" data-cites="saad2024ares">(<a href="#ref-saad2024ares" role="doc-biblioref">Saad-Falcon et al. 2024</a>)</span> exemplify this approach. RAGAS introduces a reference-free multimetric framework to automatically assess RAG pipelines. It proposes a suite of zero-shot LLM-based evaluation metrics that target each aspect of the pipeline, (1) the relevance of the retrieved passages, (2) the faithfulness of the LLM’s response to those passages, and (3) the overall quality of the response. In practice, RAGAS uses prompt-based evaluation with an LLM, such as GPT-4, to score output on several dimensions, eliminating the need for ground truth answers for every query. Concretely, the RAGAS score combines two metrics for the generation stage, faithfulness and answer relevancy, and two metrics for retrieval, context precision, and context recall. Faithfulness measures whether the answer accurately reflects the information found in the retrieved documents (i.e., no unsupported claims), and answer relevancy checks if the answer addresses the query directly without unnecessary content. Context precision/recall evaluate whether retrieved passages are relevant to the query and cover the needed information.</p>
<p>In contrast, existing LLM benchmarks rarely assess these facets separately. Traditional QA evaluations might reward an answer that is correct, but they will not detect whether the model had to hallucinate missing facts or if it ignored provided context. RAG evaluation frameworks fill this gap by explicitly rewarding answers that are correct and grounded in evidence. For example, Gao et al. <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024</a>)</span> note that RAG models are particularly aimed at mitigating hallucinations and providing traceable sources, so the evaluation criteria emphasize factuality and source attribution. This is aligned with emerging benchmarks such as Knowledge Intensive Language Tasks (KILT), a set of knowledge-intensive tasks that require models to retrieve supporting Wikipedia passages, where evaluations similarly combine a correctness measure with evidence retrieval accuracy. RAGAS and ARES add automation and finer granularity to this more comprehensive style of evaluation. ARES, for example, evaluates RAG systems along three dimensions, context relevance, answer faithfulness, and answer relevance. It creates synthetic QA pairs to fine-tune lightweight ‘judge’ models that can score each aspect, and calibrates them with a small number of human-annotated examples. This allowed ARES to reliably evaluate RAG performance on eight knowledge-intensive tasks, from benchmarks like KILT and SuperGLUE, with only a few hundred human labels, by having the learned judges predict ratings for thousands of cases.</p>
<p><a href="#tbl-rag-benchmarking">Table&nbsp;2</a> summarizes recent advancements in RAG benchmarking, highlighting how different studies address retrieval accuracy, generation quality, and multistage evaluation.</p>
<div id="tbl-rag-benchmarking" class="anchored">
<table class="table">
<caption>Table&nbsp;2<strong>.</strong> RAG / Multi-Task Benchmarking Issues</caption>
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Paper &amp; Authors</th>
<th>Benchmarking Issue</th>
<th>Proposed Approach</th>
<th>Key Findings</th>
<th>Future Directions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Gao et al.&nbsp;2024</strong></td>
<td>Conventional benchmarks ignore retrieval models.</td>
<td>Multi-stage RAG evaluation: retrieval, generation, integration.</td>
<td>Distinguishes retrieval quality from generated response.</td>
<td>Widespread adoption of multi-stage metrics is needed.</td>
</tr>
<tr class="even">
<td><strong>Es et al.&nbsp;2023</strong></td>
<td>Lack of automated RAG evaluation.</td>
<td>Uses LLM-based scoring for retrieval and accuracy.</td>
<td>Enables scalable evaluation without ground truth labels.</td>
<td>LLM-generated scores may introduce biases.</td>
</tr>
<tr class="odd">
<td><strong>Saad-Falcon et al.&nbsp;2024</strong></td>
<td>Scaling RAG evaluation with minimal human labeling.</td>
<td>ARES: Uses few human labels to train evaluators.</td>
<td>Reduces manual scoring dependency.</td>
<td>Updating evaluators for evolving data remains an issue.</td>
</tr>
<tr class="even">
<td><strong>Rasiah et al.&nbsp;2024</strong></td>
<td>Benchmarks too simplistic for real-world applications.</td>
<td>SCALE: Legal domain benchmark with long-text, multilingual tasks.</td>
<td>Models struggle with long, domain-specific inputs.</td>
<td>Similar domain-specific benchmarks are needed.</td>
</tr>
<tr class="odd">
<td><strong>Friel, Belyi, &amp; Sanyal 2025</strong></td>
<td>No standardized RAG benchmark or explainability.</td>
<td>RAGBench: 100k examples + TRACe, an explainability framework.</td>
<td>Enables large-scale, explainable RAG evaluation.</td>
<td>Ensuring alignment between automated and human scoring remains an issue.</td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="conclusions" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">2.3</span> Conclusions</h2>
<p>Examining these works collectively, several common themes emerge with regard to general principles and best practices for domain-specific LLM benchmarks.</p>
<section id="maintaining-benchmark-integrity" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="maintaining-benchmark-integrity"><span class="header-section-number">2.3.1</span> Maintaining Benchmark Integrity</h3>
<p>A consistent concern is to ensure that the evaluation truly measures generalization, not memory. Xu et al. <span class="citation" data-cites="xu2024benchmark">(<a href="#ref-xu2024benchmark" role="doc-biblioref">Xu et al. 2024</a>)</span>‘s survey highlighted how pervasive this issue is, warning that models often get inflated scores by ’knowing’ test answers in advance. It is important to distinguish that the memorization of the answers is not the problem in question, but can be an indicator of the real issue, which is brittleness and overfitting. Although the mere presence of model memorization does not definitively prove overfitting has occurred benchmarking solutions should be designed to elucidate this issue. Works such as Xia, Deng, &amp; L. Zhang <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span>’s EVOEVAL, which regenerates and mutates test questions, show promise in this regard. The general principle made clear in this review is to keep the benchmarks novel and unpredictable. Echoing this theme, McIntosh et al. <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span> advocates for dynamic benchmarks that evolve so that models cannot simply overfit. For domain-specific benchmarks, this might mean using proprietary or freshly collected data that was not in common pre-training corpora, or continuously adding new test cases over time. In doing so, we maintain the integrity of the benchmark, ensuring that the scores remain a trustworthy signal of the capability of a model.</p>
</section>
<section id="evolving-evaluation-to-reflect-real-usage" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="evolving-evaluation-to-reflect-real-usage"><span class="header-section-number">2.3.2</span> Evolving Evaluation to Reflect Real Usage</h3>
<p>There is a clear trend towards making evaluations more holistic, realistic and aligned with how LLMs are actually used. Lyu, Wu, &amp; Aji <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024</a>)</span> explicitly show that evaluation methods can be misaligned, if we optimize for convenience like in the case of multiple-choice probability evaluation, we might miss the true behavior of the model. Similarly, McIntosh et al. <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span> and Rasiah et al. <span class="citation" data-cites="rasiah2024scale">(<a href="#ref-rasiah2024scale" role="doc-biblioref">Rasiah et al. 2024</a>)</span> push for benchmarks that test models in more complex scenarios (multi-turn interactions, long documents, diverse languages) because real-world tasks are complex. A general principle is that benchmarks should simulate the conditions under which we expect the model to perform. For instance, if an LLM will be used by non-English speakers, the benchmark should have multilingual components, as SCALE does. If the model will function as a dialogue agent, the benchmark should include interactive prompts or multistep reasoning tasks, not just single-turn queries. We see this in RAGBench and Li, Yuan, &amp; Z. Zhang <span class="citation" data-cites="li2024enhancing">(<a href="#ref-li2024enhancing" role="doc-biblioref">Li, Yuan, and Zhang 2024</a>)</span>’s work by incorporating retrieval into the evaluation system. Since many real deployments use tools to assist LLMs, the benchmarks must evaluate that combined system. Another aspect of evolving evaluation is the use of multimetric assessment. Instead of a one-number accuracy or BLEU, there is a move to break down the performance into submetrics such as RAGBench’s TRACe to get a more complete picture. This is especially important in domain-specific contexts like legal or medical, where an answer might need to be not only correct but also justified and safe. By having granular metrics such as correctness, justification adequacy, harmful content check, etc., benchmark results become more actionable, developers can see why a model fails and improve it. In summary, best practices involve designing benchmarks that are high-fidelity proxies for deployment scenarios: dynamic, diverse, and evaluated on multiple axes of quality.</p>
</section>
<section id="leveraging-tools-and-hybrid-approaches" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="leveraging-tools-and-hybrid-approaches"><span class="header-section-number">2.3.3</span> Leveraging Tools and Hybrid Approaches</h3>
<p>Another emerging principle is that benchmarks can and should test a model’s ability to use tools or external knowledge, rather than confining the evaluation to end-to-end prompting. Li, Yuan, &amp; Z. Zhang <span class="citation" data-cites="li2024enhancing">(<a href="#ref-li2024enhancing" role="doc-biblioref">Li, Yuan, and Zhang 2024</a>)</span> and Friel, Belyi, &amp; Sanyal <span class="citation" data-cites="friel2025ragbench">(<a href="#ref-friel2025ragbench" role="doc-biblioref">Friel, Belyi, and Sanyal 2025</a>)</span> both illustrate this by focusing on retrieval-augmented settings. This intersection of tool-use with benchmarking is increasingly relevant as advanced models often come with an ecosystem of plugins or support systems. A domain-specific example: A cybersecurity LLM might have access to a database of known vulnerabilities; a good benchmark would measure how well the LLM queries that database and integrates the results into its advice, not just what it remembers. By designing benchmarks that allow tool use, for example, providing an API or knowledge base as part of the test environment, we measure a more practical skill, the ability of an AI to know what it does not know and find out. This also helps combat hallucinations and data staleness, as seen in RAG approaches. In intersections, this addresses some contamination issues by relying on an external source rather than training memory. This in turn aligns with the goal of realistic evaluation, since human AI users often expect AI to cite sources or use web search. It is a shift from the old paradigm of closed-book QA towards an open-book evaluation model.</p>
<p>In particular, RAG evaluation highlights the importance of ground truth reference signals for factual tasks. In base LLM evaluation, this insight suggests incorporating open-book testing: instead of only closed-book QA, have benchmarks where the model can consult a knowledge source, as a form of RAG, and see if that boosts performance. If an LLM under closed-book conditions fails a question but succeeds when allowed to retrieve relevant text, that indicates the base model’s limitation was missing knowledge, not reasoning ability. Conversely, if it fails even with the reference provided, the issue lies in understanding or reasoning. This differentiated evaluation, closed-book versus open-book, was historically done in QA research and can be informed by RAG frameworks. Gao et al. <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024</a>)</span> mention that RAG enables continuous knowledge updates and domain-specific info integration. Evaluating a base model in scenarios with and without such updates can quantify how much retrieval augments it. In summary, by borrowing RAG’s metrics such as faithfulness, relevance, etc., and techniques such as LLM-based judging, multicomponent analysis, we can design more nuanced and robust evaluations for base models. This ensures that enhanced capabilities such as factual grounding are explicitly tested and that a model’s score reflects not just whether it is right, but why and how it arrives at the answers.</p>
</section>
</section>
</section>
<section id="output-design" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Output Design</h1>
<p>This framework provides a systematic, repeatable, and automated approach to benchmarking advisory LLMs across various domains. By integrating domain-specific knowledge, retrieval augmentation, and knowledge graphs, it ensures robust evaluations that align with real-world application needs. The modular design allows organizations to continuously update their benchmarking pipeline as domain requirements evolve, ensuring that advisory models remain accurate, trustworthy, and effective in real-world deployment. While demonstrated here with a cybersecurity compliance use case, the framework’s architecture is intentionally domain-agnostic. The same methodology can be applied to financial advisory, legal consultation, healthcare guidance, or any other domain requiring specialized knowledge. This flexibility allows organizations to adapt the benchmarking process to their specific needs while maintaining rigorous evaluation standards (see <a href="#fig-framework-overview">Figure&nbsp;6</a>).</p>
<div id="fig-framework-overview" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="framework-overview.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;6<strong>.</strong> Depicts the cyclical nature of benchmarking with five phases: Benchmark Creation, Model Evaluation, Results Analysis, Framework Refinement, and Benchmark Evolution.</figcaption>
</figure>
</div>
<section id="methodologies" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="methodologies"><span class="header-section-number">3.1</span> Methodologies</h2>
<p>To develop an effective cybersecurity compliance advisory benchmarking framework, we employ a structured methodology that ensures real-world relevance and automation. The benchmark is designed around publicly available cybersecurity standards (e.g., <strong>NIST 800-53, NIST Cybersecurity Framework (CSF) 2.0, CIS Controls, CSA Cloud Controls Matrix (CCM), GDPR, and MITRE ATT&amp;CK</strong>) and focuses on evaluating an LLM’s ability to advise on compliance-related queries. This includes:</p>
<ol type="1">
<li><p><strong>Identifying Real-World Advisory Tasks</strong> – Defining key use cases such as answering compliance-related questions, identifying gaps, providing policy recommendations, and cross-mapping standards <span class="citation" data-cites="nist2020security">(<a href="#ref-nist2020security" role="doc-biblioref">NIST 2020</a>)</span>.</p></li>
<li><p><strong>Automating Benchmark Dataset Creation</strong> – Extracting and structuring compliance questions from regulatory texts, case law, certification exams, and expert Q&amp;A forums <span class="citation" data-cites="mcintosh2024inadequacies">(<a href="#ref-mcintosh2024inadequacies" role="doc-biblioref">McIntosh et al. 2024</a>)</span>.</p></li>
<li><p><strong>Evaluating Multiple Architectures</strong> – Comparing base models, fine-tuned models, RAG-enhanced models, and GraphRAG architectures <span class="citation" data-cites="xu2024benchmark">(<a href="#ref-xu2024benchmark" role="doc-biblioref">Xu et al. 2024</a>)</span>.</p></li>
<li><p><strong>Continuous Refinement and Benchmark Evolution</strong> – Preventing benchmark overfitting by generating test variants and monitoring for artificial performance inflation <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span>. The framework addresses benchmark evolution through three practical mechanisms:</p>
<ol type="a">
<li><p><strong>Scheduled Resource Updates</strong>: Periodic manual replacement of resource documents with the latest regulatory versions, recognizing that automated detection of domain knowledge evolution represents a complex research problem requiring continuous monitoring of regulatory bodies.</p></li>
<li><p><strong>Schema-Preserved Regeneration</strong>: Maintaining consistent output schemas while regenerating questions with updated resource documents, producing differently worded questions with the same evaluative content to test genuine understanding versus memorization.</p></li>
<li><p><strong>Multi-Model Generation</strong>: Using different LLMs (e.g., Gemma2:9b, Llama3.1, GPT-4) with identical schemas and resource documents to create diverse question formulations while ensuring benchmarks are not biased toward specific model families.</p></li>
</ol></li>
</ol>
</section>
<section id="software" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="software"><span class="header-section-number">3.2</span> Software</h2>
<p>The benchmarking framework is implemented with Docker and Ollama for seamless local model deployment and evaluation <span class="citation" data-cites="ollama2024documentation">(<a href="#ref-ollama2024documentation" role="doc-biblioref">Ollama 2024</a>)</span>. Ollama enables running 7B-13B parameter open-source models on consumer GPUs, ensuring cost-effective testing. The TrustGraph framework is used for GraphRAG, leveraging knowledge graphs to enhance retrieval accuracy <span class="citation" data-cites="trustgraph2024graph">(<a href="#ref-trustgraph2024graph" role="doc-biblioref">TrustGraph 2024</a>)</span>. Hugging Face’s “evaluate” library provides standardized performance metrics, while Python-based scripts automate dataset curation, evaluation pipeline execution, and metric tracking. Additional tools include:</p>
<ul>
<li><strong>LangGraph</strong> – For agent workflow orchestration and state management <span class="citation" data-cites="langgraph2025framework">(<a href="#ref-langgraph2025framework" role="doc-biblioref">LangGraph 2025</a>)</span>.</li>
<li><strong>LangChain</strong> – For LLM-based retrieval, response generation, and logging interactions <span class="citation" data-cites="langchain2024framework">(<a href="#ref-langchain2024framework" role="doc-biblioref">LangChain 2024</a>)</span>.</li>
<li><strong>FAISS</strong> – For vector-based document retrieval in standard RAG configurations <span class="citation" data-cites="douze2025faiss">(<a href="#ref-douze2025faiss" role="doc-biblioref">Douze et al. 2025</a>)</span>.</li>
<li><strong>Qdrant</strong> – For knowledge graph storage and querying in GraphRAG experiments <span class="citation" data-cites="qdrant2025open">(<a href="#ref-qdrant2025open" role="doc-biblioref">Qdrant 2025</a>)</span>.</li>
<li><strong>Pydantic</strong> – For dynamic schema generation and structured output validations <span class="citation" data-cites="pydantic2025data">(<a href="#ref-pydantic2025data" role="doc-biblioref">Pydantic 2025</a>)</span>.</li>
</ul>
</section>
<section id="system" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="system"><span class="header-section-number">3.3</span> System</h2>
<p>The benchmarking system follows a modular three-phase pipeline (see <a href="#fig-system-architecture">Figure&nbsp;7</a>), comparing four LLM configurations (see <a href="#fig-model-architectures">Figure&nbsp;8</a>):</p>
<ol type="1">
<li><strong>Base LLM</strong> - Direct model inference using Ollama (7B-13B open models).</li>
<li><strong>Standard RAG</strong> - Vector-based retrieval augmentation using FAISS indexing.</li>
<li><strong>GraphRAG</strong> - Knowledge graph-based retrieval using TrustGraph framework.</li>
<li><strong>Agent</strong> - Multi-tool ReAct agent combining vector RAG, graph RAG, and web search capabilities.</li>
</ol>
<div id="fig-system-architecture" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="system-architecture.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;7<strong>.</strong> Domain-Bench framework architecture showing the complete three-phase evaluation process: (1) EvalAgent-driven benchmark generation using LangGraph workflows and multi-modal RAG, (2) systematic evaluation across four distinct model architectures, and (3) multidimensional assessment using five evaluation criteria.</figcaption>
</figure>
</div>
<div id="fig-model-architectures" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="model-architectures.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;8<strong>.</strong> Four model architectures evaluated systematically: BaseLLM (direct inference), StandardRAG (vector-based retrieval), GraphRAG (knowledge graph retrieval using TrustGraph), and Agent (multi-tool ReAct agent combining multiple retrieval strategies).</figcaption>
</figure>
</div>
<p>Each architecture is deployed through a unified interface and evaluated on curated benchmarks of domain-specific queries. The pipeline executes queries across all architectures, retrieves relevant context (for RAG/GraphRAG/Agent models), logs outputs, and calculates performance metrics using the EvalAgent framework.</p>
</section>
<section id="resources" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="resources"><span class="header-section-number">3.4</span> Resources</h2>
<p>The implementation relies on publicly available compliance datasets and documentation:</p>
<ul>
<li><strong>NIST Special Publications (SP 800 series)</strong> <span class="citation" data-cites="nist2020security">(<a href="#ref-nist2020security" role="doc-biblioref">NIST 2020</a>)</span></li>
<li><strong>NIST Cybersecurity Framework (CSF) 2.0</strong> <span class="citation" data-cites="nist2024cybersecurity">(<a href="#ref-nist2024cybersecurity" role="doc-biblioref">NIST 2024</a>)</span></li>
<li><strong>CIS Critical Security Controls v8</strong> <span class="citation" data-cites="cis2023critical">(<a href="#ref-cis2023critical" role="doc-biblioref">CIS 2023</a>)</span></li>
<li><strong>FedRAMP Security Controls &amp; Compliance Guidelines</strong> <span class="citation" data-cites="fedramp2023security">(<a href="#ref-fedramp2023security" role="doc-biblioref">FedRAMP 2023</a>)</span></li>
<li><strong>GDPR regulatory text and enforcement case studies</strong> <span class="citation" data-cites="europeancommission2023gdpr">(<a href="#ref-europeancommission2023gdpr" role="doc-biblioref">EuropeanCommission 2023</a>)</span></li>
</ul>
<p>The system employs an automated EvalAgent framework that processes academic papers to extract evaluation principles, generates domain-specific system prompts, and creates benchmarks through multi-document RAG retrieval from compliance resources (see <a href="#fig-evalagent-workflow">Figure&nbsp;9</a>) <span class="citation" data-cites="krishna2024attackqa">(<a href="#ref-krishna2024attackqa" role="doc-biblioref">Krishna 2024</a>)</span>.</p>
<div id="fig-evalagent-workflow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="evalagent-workflow.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;9<strong>.</strong> EvalAgent benchmark generation workflow showing the automated extraction of evaluation principles from academic papers through a five-step process: principle extraction, schema generation, system prompt creation, and benchmark generation using multi-modal RAG.</figcaption>
</figure>
</div>
</section>
<section id="evaluation-parameters" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="evaluation-parameters"><span class="header-section-number">3.5</span> Evaluation Parameters</h2>
<p>To measure the success of each implementation, the benchmarking framework (see <a href="#fig-evaluation-pipeline">Figure&nbsp;10</a>) evaluates models across multiple dimensions:</p>
<ol type="1">
<li><p><strong>Agreement</strong> – Is there consistency between a model’s predicted probabilities and its ability to generate coherent, relevant, and accurate text? High agreement implies that the model’s probabilistic outputs accurately reflect its generative capabilities across diverse tasks, including those with both definitive solutions and open-ended inquiries <span class="citation" data-cites="lyu2024beyond">(<a href="#ref-lyu2024beyond" role="doc-biblioref">Lyu, Wu, and Aji 2024</a>)</span>.</p></li>
<li><p><strong>Function Correctness</strong> – Does the answer reliably and accurately fulfill the intended function specified in a task or prompt? Does it properly address its real-world intended usage? <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span>.</p></li>
<li><p><strong>Reasoning</strong> – Can the model logically justify its recommendations? <span class="citation" data-cites="xu2024benchmark">(<a href="#ref-xu2024benchmark" role="doc-biblioref">Xu et al. 2024</a>)</span>.</p></li>
<li><p><strong>Relevance</strong> – Does the response directly address the compliance question? <span class="citation" data-cites="es2023ragas">(<a href="#ref-es2023ragas" role="doc-biblioref">Es et al. 2023</a>)</span>.</p></li>
<li><p><strong>Retrieval Effectiveness (for RAG/GraphRAG)</strong> – Are retrieved documents relevant and properly used? <span class="citation" data-cites="gao2024retrieval">(<a href="#ref-gao2024retrieval" role="doc-biblioref">Gao et al. 2024</a>)</span>.</p></li>
</ol>
<div id="fig-evaluation-pipeline" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="evaluation-pipeline.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption"><strong>Figure</strong>&nbsp;10<strong>.</strong> Evaluation pipeline demonstrating the systematic assessment process from benchmark questions through multi-architecture execution to comprehensive scoring across five evaluation dimensions.</figcaption>
</figure>
</div>
<p>Each model is tested on a standardized benchmark and its performance is logged across these metrics. The results are analyzed to determine:</p>
<ul>
<li>Which model architecture performs best?</li>
<li>Does retrieval improve reasoning and accuracy?</li>
<li>Does GraphRAG reduce hallucinations and improve compliance adherence?</li>
<li>Are improvements genuine or due to artificial benchmark gaming? <span class="citation" data-cites="xia2024top">(<a href="#ref-xia2024top" role="doc-biblioref">Xia, Deng, and Zhang 2024</a>)</span></li>
</ul>
<p>If the best-performing model exhibits benchmark overfitting (e.g., memorization of test questions), the architecture is adjusted, re-tested, and iteratively refined <span class="citation" data-cites="rasiah2024scale">(<a href="#ref-rasiah2024scale" role="doc-biblioref">Rasiah et al. 2024</a>)</span>.</p>
</section>
<section id="implementation-workflow" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="implementation-workflow"><span class="header-section-number">3.6</span> Implementation Workflow</h2>
<p>The Domain-Bench framework operates through an integrated workflow that combines automated benchmark generation with systematic architecture evaluation (as shown in the complete framework, <a href="#fig-system-architecture">Figure&nbsp;7</a>):</p>
<ul>
<li><p><strong>Principle Extraction</strong>: Academic papers are processed to extract evaluation criteria using the EvalAgent’s Chain-of-Thought reasoning capabilities.</p></li>
<li><p><strong>Benchmark Generation</strong>: Domain-specific questions are created through multi-modal RAG, combining vector search and knowledge graph retrieval from compliance resources.</p></li>
<li><p><strong>Architecture Evaluation</strong>: Each benchmark is executed across all four model configurations, with responses logged and contextualized.</p></li>
<li><p><strong>Multidimensional Assessment</strong>: The same EvalAgent framework that generated benchmarks evaluates responses across five dimensions, ensuring consistency and fairness.</p></li>
</ul>
<p>This end-to-end automation enables scalable, reproducible evaluation while maintaining the flexibility to adapt to evolving domain requirements.</p>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-cis2023critical" class="csl-entry" role="listitem">
CIS. 2023. <em>Critical Security Controls V8</em>. <a href="https://www.cisecurity.org/controls/v8-1">https://www.cisecurity.org/controls/v8-1</a>.
</div>
<div id="ref-dalvi2024llmebench" class="csl-entry" role="listitem">
Dalvi, Fahim, Maram Hasanain, Sabri Boughorbel, Basel Mousi, Samir Abdaljalil, Nizi Nazar, Ahmed Abdelali, et al. 2024. <span>“LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking.”</span> <a href="https://arxiv.org/abs/2308.04945">https://arxiv.org/abs/2308.04945</a>.
</div>
<div id="ref-douze2025faiss" class="csl-entry" role="listitem">
Douze, Matthijs, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2025. <span>“The Faiss Library.”</span> <a href="https://arxiv.org/abs/2401.08281">https://arxiv.org/abs/2401.08281</a>.
</div>
<div id="ref-es2023ragas" class="csl-entry" role="listitem">
Es, Shahul, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. <span>“RAGAS: Automated Evaluation of Retrieval Augmented Generation.”</span> <a href="https://arxiv.org/abs/2309.15217">https://arxiv.org/abs/2309.15217</a>.
</div>
<div id="ref-europeancommission2023gdpr" class="csl-entry" role="listitem">
EuropeanCommission. 2023. <span>“GDPR Enforcement Cases: Official Public Reports.”</span> European Union. <a href="https://gdpr.eu">https://gdpr.eu</a>.
</div>
<div id="ref-fedramp2023security" class="csl-entry" role="listitem">
FedRAMP. 2023. <em>Security Controls and Compliance Guidelines</em>. <a href="https://www.fedramp.gov/documents-templates/">https://www.fedramp.gov/documents-templates/</a>.
</div>
<div id="ref-friel2025ragbench" class="csl-entry" role="listitem">
Friel, Robert, Masha Belyi, and Atindriyo Sanyal. 2025. <span>“RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems.”</span> <a href="https://arxiv.org/abs/2407.11005">https://arxiv.org/abs/2407.11005</a>.
</div>
<div id="ref-gao2024retrieval" class="csl-entry" role="listitem">
Gao, Yunfan, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. <span>“Retrieval-Augmented Generation for Large Language Models: A Survey.”</span> <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a>.
</div>
<div id="ref-golchin2024time" class="csl-entry" role="listitem">
Golchin, Shahriar, and Mihai Surdeanu. 2024. <span>“Time Travel in LLMs: Tracing Data Contamination in Large Language Models.”</span> <a href="https://arxiv.org/abs/2308.08493">https://arxiv.org/abs/2308.08493</a>.
</div>
<div id="ref-krishna2024attackqa" class="csl-entry" role="listitem">
Krishna, Varun Badrinath. 2024. <span>“AttackQA: Development and Adoption of a Dataset for Assisting Cybersecurity Operations Using Fine-Tuned and Open-Source LLMs.”</span> <a href="https://arxiv.org/abs/2411.01073">https://arxiv.org/abs/2411.01073</a>.
</div>
<div id="ref-langchain2024framework" class="csl-entry" role="listitem">
LangChain. 2024. <em>Framework for Developing Applications Powered by Large Language Models</em>. <a href="https://python.langchain.com/docs/introduction/">https://python.langchain.com/docs/introduction/</a>.
</div>
<div id="ref-langgraph2025framework" class="csl-entry" role="listitem">
LangGraph. 2025. <em>Framework for Building, Managing, and Deploying Long Running, Stateful Agents</em>. <a href="https://langchain-ai.github.io/langgraph/">https://langchain-ai.github.io/langgraph/</a>.
</div>
<div id="ref-li2024enhancing" class="csl-entry" role="listitem">
Li, Jiarui, Ye Yuan, and Zehua Zhang. 2024. <span>“Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases.”</span> <a href="https://arxiv.org/abs/2403.10446">https://arxiv.org/abs/2403.10446</a>.
</div>
<div id="ref-lyu2024beyond" class="csl-entry" role="listitem">
Lyu, Chenyang, Minghao Wu, and Alham Fikri Aji. 2024. <span>“Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models.”</span> <a href="https://arxiv.org/abs/2402.13887">https://arxiv.org/abs/2402.13887</a>.
</div>
<div id="ref-mcintosh2024inadequacies" class="csl-entry" role="listitem">
McIntosh, Timothy R., Teo Susnjak, Nalin Arachchilage, Tong Liu, Paul Watters, and Malka N. Halgamuge. 2024. <span>“Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence.”</span> <a href="https://arxiv.org/abs/2402.09880">https://arxiv.org/abs/2402.09880</a>.
</div>
<div id="ref-nist2020security" class="csl-entry" role="listitem">
NIST. 2020. <span>“Security and Privacy Controls for Information Systems and Organizations.”</span> NIST Special Publication 800-53 Rev. 5. Gaithersburg, MD: NIST. <a href="https://doi.org/10.6028/NIST.SP.800-53r5">https://doi.org/10.6028/NIST.SP.800-53r5</a>.
</div>
<div id="ref-nist2024cybersecurity" class="csl-entry" role="listitem">
———. 2024. <span>“Cybersecurity Framework 2.0.”</span> NIST Cybersecurity White Paper [CSWP] NIST CSWP 29. NIST. <a href="https://doi.org/10.6028/NIST.CSWP.29">https://doi.org/10.6028/NIST.CSWP.29</a>.
</div>
<div id="ref-ollama2024documentation" class="csl-entry" role="listitem">
Ollama. 2024. <em>Documentation for Local Model Deployment</em>. <a href="https://github.com/ollama/ollama/tree/main/docs">https://github.com/ollama/ollama/tree/main/docs</a>.
</div>
<div id="ref-pydantic2025data" class="csl-entry" role="listitem">
Pydantic. 2025. <em>Data Validation Library for Python.</em> <a href="https://docs.pydantic.dev/latest/">https://docs.pydantic.dev/latest/</a>.
</div>
<div id="ref-qdrant2025open" class="csl-entry" role="listitem">
Qdrant. 2025. <em>Open Source Vector Database and Similarity Search Engine Designed to Handle High-Dimensional Vectors for Performance and Massive-Scale AI Applications</em>. <a href="https://qdrant.tech/">https://qdrant.tech/</a>.
</div>
<div id="ref-rasiah2024scale" class="csl-entry" role="listitem">
Rasiah, Vishvaksenan, Ronja Stern, Veton Matoshi, Matthias Stürmer, Ilias Chalkidis, Daniel E. Ho, and Joel Niklaus. 2024. <span>“<span>SCALE</span>: Scaling up the Complexity for Advanced Language Model Evaluation.”</span> <a href="https://openreview.net/forum?id=aLXRYfIUUd">https://openreview.net/forum?id=aLXRYfIUUd</a>.
</div>
<div id="ref-saad2024ares" class="csl-entry" role="listitem">
Saad-Falcon, Jon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024. <span>“ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems.”</span> <a href="https://arxiv.org/abs/2311.09476">https://arxiv.org/abs/2311.09476</a>.
</div>
<div id="ref-trustgraph2024graph" class="csl-entry" role="listitem">
TrustGraph. 2024. <span>“Graph-Based Retrieval-Augmented Generation.”</span> <em>Technical Report</em>. <a href="https://trustgraph.ai/docs/TrustGraph">https://trustgraph.ai/docs/TrustGraph</a>.
</div>
<div id="ref-xia2024top" class="csl-entry" role="listitem">
Xia, Chunqiu Steven, Yinlin Deng, and Lingming Zhang. 2024. <span>“Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM.”</span> <a href="https://arxiv.org/abs/2403.19114">https://arxiv.org/abs/2403.19114</a>.
</div>
<div id="ref-xu2024benchmark" class="csl-entry" role="listitem">
Xu, Cheng, Shuhao Guan, Derek Greene, and M-Tahar Kechadi. 2024. <span>“Benchmark Data Contamination of Large Language Models: A Survey.”</span> <a href="https://arxiv.org/abs/2406.04244">https://arxiv.org/abs/2406.04244</a>.
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
</div> <!-- /content -->



<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>