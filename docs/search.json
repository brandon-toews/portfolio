[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/modes-of-motion/modes-of-motion.html",
    "href": "posts/modes-of-motion/modes-of-motion.html",
    "title": "Modes of Motion Programming Mathematics",
    "section": "",
    "text": "Play Demo\n\n\n\n\n\n\nCamera Keyboard Controls\n\n\n\n\n\n\n\n\nForward\nUp Arrow or W\nDown:\nQ\n\n\nBackward:\nDown Arrow or S\nLook Left:\nZ\n\n\nStrafe Left:\nLeft Arrow or A\nLook Right:\nX\n\n\nStrafe Right:\nRight Arrow or D\nLook Up\nR\n\n\nUp:\nE\nLook Down\nF\n\n\nEscape Program\nEsc\n\n\n\n\n\nCamera Canvas\nWhen user clicks anywhere in the scene camera canvas is enabled allowing the user to do various camera movements based on the options they choose. In this mode the user can create a limited amount of cubes and spheres using the ‚ÄúCreate Cube‚Äù & ‚ÄúCreate Sphere‚Äù buttons.\nObject Canvas\nWhen an object is clicked on it is selected then the object canvas is enabled allowing the user to move the object using the various options they choose.\n\nüöÄ Play Demo\n\n\n\n\n\nModes of Motion\nProject to develop a scene to demonstrate various methods of movement within the Unity Games Engine, making use of my own linear interpolation (lerp) libraries and scripts using Unity physics. This scene employs mathematical techniques like projectile formulas and equations that have been developed over the course of the module.\n\n\n\n\n\n\nSource Code\n\n\n\nView source code on Github repository.\n\n\n\n\nVideo Walkthrough\nVideo\n\n\nReferences\n\nUnity Documentation\nhttps://docs.unity3d.com/ScriptReference/index.html\n\n\nSpawning Objects\n\n\n\nCollision Detection\n\n\n\nSpecific Coding Problems Queried on:\nhttps://stackoverflow.com/questions/"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#purpose-of-document",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#purpose-of-document",
    "title": "Linear Regression, Classification, Clustering",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide a comparison between different AI models for a given dataset to determine which models are most accurate. This document also explores what measures can be taken to improve accuracy in various AI models."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#scope",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#scope",
    "title": "Linear Regression, Classification, Clustering",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an exploratory examination of a dataset to determine how best to sample and clean the data for AI training and testing purposes. Various data visualizations are needed to properly understand the dataset and how best to proceed with training models. Training of various models and algorithms are required to produce sufficient comparisons with the ultimate goal of improving accuracy."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#descriptive-analysis",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#descriptive-analysis",
    "title": "Linear Regression, Classification, Clustering",
    "section": "2.1 Descriptive Analysis",
    "text": "2.1 Descriptive Analysis\nThe dataset used in this project consists of available independent variables for a variety of cars to ascertain how they affect the price. The chosen dataset contains 26 columns and 205 rows of data with no null values. It is a sufficient dataset in terms of size and types of data for use in training univariate & multivariate linear regression, classification and clustering models.\nThe Columns\n\nCar_ID : Unique id of each observation (Integer)\nSymboling : Its assigned insurance risk rating, A value of +3 - Indicates that the auto is risky, -3 that it is probably pretty safe.\ncarCompany : Name of car company (Categorical)\nfueltype : Car fuel type i.e gas or diesel (Categorical)\naspiration : Aspiration used in a car (Categorical)\ndoornumber : Number of doors in a car (Categorical)\ncarbody : Body of car (Categorical)\ndrivewheel : Type of drive wheel (Categorical)\nenginelocation : Location of car engine (Categorical)\nwheelbase : Wheelbase of car (Numeric)\ncarlength : Length of car (Numeric)\ncarwidth : Width of car (Numeric)\ncarheight : Height of car (Numeric)\ncurbweight : The weight of a car without occupants or baggage. (Numeric)\nenginetype : Type of engine. (Categorical)\ncylindernumber : Cylinder placed in the car (Numeric)\nenginesize : Size of car (Numeric)\nfuelsystem : Fuel system of car (Categorical)\nboreratio : Boreratio of car (Numeric)\nstroke : Stroke or volume inside the engine (Numeric)\ncompressionratio : Compression ratio of car (Numeric)\nhorsepower : Horsepower (Numeric)\npeakrpm : Car peak rpm (Numeric)\ncitympg : Mileage in city (Numeric)\nhighwaympg : Mileage on highway (Numeric)\nprice(Dependent variable) : Price of car (Numeric)\n\n\n\nCode\n# Import libraries for analysis and plotting\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Save data in Pandas dataframe\ndataset = pd.read_csv(\"CarPrice_Assignment.csv\")\n\n# Print how many rows and columns are in dataset\nprint('Dataset Shape:',dataset.shape)\n\n# Turn of max columns so that head() displays all columns in dataset\npd.set_option('display.max_columns', None)\n\npd.set_option('display.max_rows', 5)\n\n# Display 1st five entries of dataset\ndataset.head()\n\n\nDataset Shape: (205, 26)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\nsix\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\nfour\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\nfive\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n\nCode\n# Print data types and how many null values are present\ndataset.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   car_ID            205 non-null    int64  \n 1   symboling         205 non-null    int64  \n 2   CarName           205 non-null    object \n 3   fueltype          205 non-null    object \n 4   aspiration        205 non-null    object \n 5   doornumber        205 non-null    object \n 6   carbody           205 non-null    object \n 7   drivewheel        205 non-null    object \n 8   enginelocation    205 non-null    object \n 9   wheelbase         205 non-null    float64\n 10  carlength         205 non-null    float64\n 11  carwidth          205 non-null    float64\n 12  carheight         205 non-null    float64\n 13  curbweight        205 non-null    int64  \n 14  enginetype        205 non-null    object \n 15  cylindernumber    205 non-null    object \n 16  enginesize        205 non-null    int64  \n 17  fuelsystem        205 non-null    object \n 18  boreratio         205 non-null    float64\n 19  stroke            205 non-null    float64\n 20  compressionratio  205 non-null    float64\n 21  horsepower        205 non-null    int64  \n 22  peakrpm           205 non-null    int64  \n 23  citympg           205 non-null    int64  \n 24  highwaympg        205 non-null    int64  \n 25  price             205 non-null    float64\ndtypes: float64(8), int64(8), object(10)\nmemory usage: 41.8+ KB\n\n\n\n\nCode\n# Display some descriptive statistics\ndataset.describe().round(2)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\ncount\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n\n\nmean\n103.00\n0.83\n98.76\n174.05\n65.91\n53.72\n2555.57\n126.91\n3.33\n3.26\n10.14\n104.12\n5125.12\n25.22\n30.75\n13276.71\n\n\nstd\n59.32\n1.25\n6.02\n12.34\n2.15\n2.44\n520.68\n41.64\n0.27\n0.31\n3.97\n39.54\n476.99\n6.54\n6.89\n7988.85\n\n\nmin\n1.00\n-2.00\n86.60\n141.10\n60.30\n47.80\n1488.00\n61.00\n2.54\n2.07\n7.00\n48.00\n4150.00\n13.00\n16.00\n5118.00\n\n\n25%\n52.00\n0.00\n94.50\n166.30\n64.10\n52.00\n2145.00\n97.00\n3.15\n3.11\n8.60\n70.00\n4800.00\n19.00\n25.00\n7788.00\n\n\n50%\n103.00\n1.00\n97.00\n173.20\n65.50\n54.10\n2414.00\n120.00\n3.31\n3.29\n9.00\n95.00\n5200.00\n24.00\n30.00\n10295.00\n\n\n75%\n154.00\n2.00\n102.40\n183.10\n66.90\n55.50\n2935.00\n141.00\n3.58\n3.41\n9.40\n116.00\n5500.00\n30.00\n34.00\n16503.00\n\n\nmax\n205.00\n3.00\n120.90\n208.10\n72.30\n59.80\n4066.00\n326.00\n3.94\n4.17\n23.00\n288.00\n6600.00\n49.00\n54.00\n45400.00"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#cleaning",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#cleaning",
    "title": "Linear Regression, Classification, Clustering",
    "section": "2.2 Cleaning",
    "text": "2.2 Cleaning\nMultiple columns are object data types but for classification and clustering purposes they were converted to category types. Column 16, ‚Äúcylindernumber‚Äù, values were changed from strings to integers to assist in training some of the linear regression models.\n\n\nCode\n# Convert object data types to category types\ndataset['CarName'] = dataset['CarName'].astype('category')\ndataset['fueltype'] = dataset['fueltype'].astype('category')\ndataset['aspiration'] = dataset['aspiration'].astype('category')\ndataset['doornumber'] = dataset['doornumber'].astype('category')\ndataset['carbody'] = dataset['carbody'].astype('category')\ndataset['drivewheel'] = dataset['drivewheel'].astype('category')\ndataset['enginelocation'] = dataset['enginelocation'].astype('category')\ndataset['enginetype'] = dataset['enginetype'].astype('category')\ndataset['fuelsystem'] = dataset['fuelsystem'].astype('category')\ndataset['curbweight'] = dataset['curbweight'].astype('int')\n\n# Convert strings to integers in cylindernumber column to potentially use in the regression models\ndataset['cylindernumber'] = dataset['cylindernumber'].replace(['two'], 2).replace(['three'], 3)\\\n.replace(['four'], 4).replace(['five'], 5).replace(['six'], 6).replace(['eight'], 8).replace(['twelve'], 12)\n\ndataset.head()\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\n6\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\n4\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\n5\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n\nCode\n# Print new data types and how many null values are present\ndataset.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   car_ID            205 non-null    int64   \n 1   symboling         205 non-null    int64   \n 2   CarName           205 non-null    category\n 3   fueltype          205 non-null    category\n 4   aspiration        205 non-null    category\n 5   doornumber        205 non-null    category\n 6   carbody           205 non-null    category\n 7   drivewheel        205 non-null    category\n 8   enginelocation    205 non-null    category\n 9   wheelbase         205 non-null    float64 \n 10  carlength         205 non-null    float64 \n 11  carwidth          205 non-null    float64 \n 12  carheight         205 non-null    float64 \n 13  curbweight        205 non-null    int64   \n 14  enginetype        205 non-null    category\n 15  cylindernumber    205 non-null    int64   \n 16  enginesize        205 non-null    int64   \n 17  fuelsystem        205 non-null    category\n 18  boreratio         205 non-null    float64 \n 19  stroke            205 non-null    float64 \n 20  compressionratio  205 non-null    float64 \n 21  horsepower        205 non-null    int64   \n 22  peakrpm           205 non-null    int64   \n 23  citympg           205 non-null    int64   \n 24  highwaympg        205 non-null    int64   \n 25  price             205 non-null    float64 \ndtypes: category(9), float64(8), int64(9)\nmemory usage: 36.1 KB"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#visualizations",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#visualizations",
    "title": "Linear Regression, Classification, Clustering",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nA pairplot ( See Figure 1 ) provides a quick overview of how the variables relate, showing some possibilities for training models. The ‚Äòcarbody‚Äô ( See Figure 2 ) and ‚Äòfueltype‚Äô ( See Figure 3 ) columns show promise for use with the classification and clustering models ( See Figure 4 (f) & Figure 4 (g) ). Clear linear relationships exist between ‚Äòcarlength‚Äô, ‚Äòcarwidth‚Äô, ‚Äòcurbweight‚Äô, ‚Äòenginesize‚Äô, ‚Äòcylindernumber‚Äô and ‚Äòhorsepower‚Äô independent variables and the dependent variable ‚Äòprice‚Äô ( See Figure 4 (a) - (f) ).\n\n\nCode\n# Display a pairplot to quickly see how varaiables relate to one another with 'fueltype' hue\nsns.pairplot(dataset, kind= 'scatter', hue= 'fueltype')\nplt.show()\n\n\n\n\n\nFigure¬†1. Pairplot with Fuel Type Hue\n\n\n\n\n\n\nCode\n# display pie chart data for carbody\ndataset['carbody'].value_counts().plot.pie(autopct='%1.3f%%');\n\n# Display relationship between body style and price\ndataset.groupby('carbody')['price'].mean().round(2)\n\n\n\n\n\nFigure¬†2. Carbody Pie Plot\n\n\n\n\n\n\ncarbody\nconvertible    21890.50\nhardtop        22208.50\nhatchback      10376.65\nsedan          14344.27\nwagon          12371.96\nName: price, dtype: float64\n\n\n\n\nCode\n# display pie chart data for fueltype\ndataset['fueltype'].value_counts().plot.pie(autopct='%1.3f%%');\n\n# Display ralationship between body style and price\ndataset.groupby('fueltype')['price'].mean().round(2)\n\n\n\n\n\nFigure¬†3. Fuel Type Pie Plot\n\n\n\n\n\n\nfueltype\ndiesel    15838.15\ngas       12999.80\nName: price, dtype: float64\n\n\n\n\nCode\n# Carlength has moderate relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"carlength\", y=\"price\");\n\n# Carwidth has moderate relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"carwidth\", y=\"price\");\n\n# Carweight has moderate/strong relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"curbweight\", y=\"price\");\n\n# Engine size has strong relationship to price\nsns.lmplot(data=dataset, x=\"enginesize\", y=\"price\", hue='fueltype', height=6)\n\n# Horsepower has strong relationship to price for both fuel types\nsns.lmplot(data=dataset, x=\"horsepower\", y=\"price\", hue='fueltype', height=6)\n\n# Cylinder number has moderate/strong relationship to price\nsns.jointplot(data=dataset, x=\"cylindernumber\", y=\"price\", kind=\"reg\");\n\n# Clear classification relationship between compressionratio and fueltype\ng = sns.jointplot(data=dataset, x=\"compressionratio\", y=\"price\", hue='fueltype');\ng.plot_joint(sns.kdeplot, hue='fueltype');\n\n\n\n\n\n\n\n\n(a) Car Length vs Price\n\n\n\n\n\n\n\n(b) Car Width vs Price\n\n\n\n\n\n\n\n\n\n(c) Curb Weight vs Price\n\n\n\n\n\n\n\n(d) Engine Size vs Price\n\n\n\n\n\n\n\n\n\n(e) Horsepower vs Price\n\n\n\n\n\n\n\n(f) Cylinder Number vs Price\n\n\n\n\n\n\n\n\n\n(g) Compression Ratio vs Price\n\n\n\n\nFigure¬†4. Plots for visualizing"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#univariate-models",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#univariate-models",
    "title": "Linear Regression, Classification, Clustering",
    "section": "3.1 Univariate Models",
    "text": "3.1 Univariate Models\nMultiple univariate models were trained for comparison using a custom Linear Regression training function. Models were trained with ‚Äòcarlength‚Äô, ‚Äòcarwidth‚Äô, ‚Äòcurbweight‚Äô, ‚Äòcylindernumber‚Äô, ‚Äòenginesize‚Äô and ‚Äòhorsepower‚Äô independent variables. In most cases the model accuracy was the best with a 70% training and 30% testing split ( See Items 3.1.1 - 3.1.4 ). However, with engine size and horsepower models more accuracy was achieved with an 80% training and 20% testing split ( See Items 3.1.5 - 3.1.6 ).\n\n\nCode\n# Import regression training libraries and packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\n# Linear Regression training function that takes in X and Y arguments and displays results\ndef myLinRegModel(x, y, testSize):\n    \n    # While loop to iterate every 10% from given test size\n    while testSize&gt;0:\n        \n        # Splitting data into training and testing variables using the values passed into function\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=(testSize/100), random_state=0)\n\n        # Training model with LinearRegression function and training data\n        regressor = LinearRegression()\n        regressor.fit(x_train, y_train)\n    \n        # Print test size of current iteration\n        print('Test Size:', testSize, '%\\n')\n\n        # Print intercept and CoEfficient values of model\n        print(\"a =\", regressor.intercept_)\n        print(\"b =\", regressor.coef_)\n\n        # Test the trained model with test data and store in variable\n        y_pred = regressor.predict(x_test)\n\n        # Display predicted values next to actual values for comparison\n        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n        print(df)\n    \n        # Display accuracy of model predictions in the form of Mean Absolute Error, Mean Squared Error,\n        # Root Mean Squared Error using the difference between actual and predicted values\n        print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n        print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n        print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n        print('R2 Score: ', metrics.r2_score(y_test,y_pred)*100, '%\\n', sep='')\n        \n        # Decrease test size by 10\n        testSize -= 10\n\n\n\n3.1.1 Car Length vs Price\n\n\nCode\n# Carlength Column\ncarLength = dataset.iloc[:, 10:-15].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carLength, price, 30)\n\n\nTest Size: 30 %\n\na = -63541.11342037626\nb = [440.33603117]\n     Actual     Predicted\n0    6795.0   6516.349139\n1   15750.0  19153.993234\n..      ...           ...\n60   6479.0    131.476687\n61  15510.0  18625.589997\n\n[62 rows x 2 columns]\nMean Absolute Error: 3981.584437549869\nMean Squared Error: 32715085.38508641\nRoot Mean Squared Error: 5719.71025359558\nR2 Score: 50.45973947401606%\n\nTest Size: 20 %\n\na = -63738.09854118214\nb = [441.42013341]\n     Actual     Predicted\n0    6795.0   6491.844685\n1   15750.0  19160.602514\n..      ...           ...\n39  45400.0  24192.792035\n40   8916.5   5079.300258\n\n[41 rows x 2 columns]\nMean Absolute Error: 4528.295484564718\nMean Squared Error: 43469954.12056989\nRoot Mean Squared Error: 6593.174813439266\nR2 Score: 43.84913586892223%\n\nTest Size: 10 %\n\na = -66742.8631493445\nb = [459.19742348]\n     Actual     Predicted\n0    6795.0   6315.446927\n1   15750.0  19494.412981\n..      ...           ...\n19   6488.0   6131.767957\n20   9959.0  12698.291113\n\n[21 rows x 2 columns]\nMean Absolute Error: 3945.9317457788898\nMean Squared Error: 29396652.85426334\nRoot Mean Squared Error: 5421.868022578873\nR2 Score: 26.410928631260454%\n\n\n\n\n\n3.1.2 Car Width vs Price\n\n\nCode\n# Carwidth Column\ncarWidth = dataset.iloc[:, 11:-14].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWidth, price, 30)\n\n\nTest Size: 30 %\n\na = -172630.60948546475\nb = [2822.14912394]\n     Actual     Predicted\n0    6795.0   8551.364271\n1   15750.0  15042.307256\n..      ...           ...\n60   6479.0   7704.719534\n61  15510.0  15042.307256\n\n[62 rows x 2 columns]\nMean Absolute Error: 3036.57768015824\nMean Squared Error: 22710512.087679498\nRoot Mean Squared Error: 4765.554751304354\nR2 Score: 65.60960571372881%\n\nTest Size: 20 %\n\na = -172526.22359994025\nb = [2819.03318321]\n     Actual     Predicted\n0    6795.0   8455.706762\n1   15750.0  14939.483084\n..      ...           ...\n39  45400.0  30444.165591\n40   8916.5   6764.286852\n\n[41 rows x 2 columns]\nMean Absolute Error: 3674.9155902799166\nMean Squared Error: 31370813.470780104\nRoot Mean Squared Error: 5600.965405247573\nR2 Score: 59.47779746918066%\n\nTest Size: 10 %\n\na = -181627.87173597398\nb = [2957.89666431]\n     Actual     Predicted\n0    6795.0   8269.094113\n1   15750.0  15072.256441\n..      ...           ...\n19   6488.0   6494.356114\n20   9959.0  11818.570110\n\n[21 rows x 2 columns]\nMean Absolute Error: 3197.214272696799\nMean Squared Error: 19783555.22362383\nRoot Mean Squared Error: 4447.87086409035\nR2 Score: 50.47553663690271%\n\n\n\n\n\n3.1.3 Curb Weight vs Price\n\n\nCode\n# Curbweight Column\ncarWeight = dataset.iloc[:, 13:-12].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWeight, price, 30)\n\n\nTest Size: 30 %\n\na = -18679.037713196016\nb = [12.40359272]\n     Actual     Predicted\n0    6795.0   4949.806413\n1   15750.0  20404.682939\n..      ...           ...\n60   6479.0   2568.316611\n61  15510.0  15530.071001\n\n[62 rows x 2 columns]\nMean Absolute Error: 2670.404540077829\nMean Squared Error: 18443910.151758883\nRoot Mean Squared Error: 4294.637371392244\nR2 Score: 72.0704958192619%\n\nTest Size: 20 %\n\na = -18833.605447325583\nb = [12.47623193]\n     Actual     Predicted\n0    6795.0   4933.616372\n1   15750.0  20479.001353\n..      ...           ...\n39  45400.0  27515.596159\n40   8916.5   4546.853183\n\n[41 rows x 2 columns]\nMean Absolute Error: 3256.3206631106873\nMean Squared Error: 25249391.034916148\nRoot Mean Squared Error: 5024.877215904499\nR2 Score: 67.3849408384091%\n\nTest Size: 10 %\n\na = -19880.405624111718\nb = [12.9537027]\n     Actual     Predicted\n0    6795.0   4796.398026\n1   15750.0  20936.711595\n..      ...           ...\n19   6488.0   6221.305324\n20   9959.0  10819.869783\n\n[21 rows x 2 columns]\nMean Absolute Error: 2695.197926817389\nMean Squared Error: 11737364.677960433\nRoot Mean Squared Error: 3425.9837533123873\nR2 Score: 70.61768320191304%\n\n\n\n\n\n3.1.4 Cylinder Number vs Price\n\n\nCode\n# Cylinder Number Column\ncylinderNumber = dataset.iloc[:, 15:-10].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(cylinderNumber, price, 30)\n\n\nTest Size: 30 %\n\na = -8750.74345729567\nb = [5045.13677503]\n     Actual     Predicted\n0    6795.0  11429.803643\n1   15750.0  21520.077193\n..      ...           ...\n60   6479.0  11429.803643\n61  15510.0  11429.803643\n\n[62 rows x 2 columns]\nMean Absolute Error: 3944.3868255082953\nMean Squared Error: 26684225.038138304\nRoot Mean Squared Error: 5165.677597192676\nR2 Score: 59.59223566856468%\n\nTest Size: 20 %\n\na = -9046.162097201766\nb = [5112.35112126]\n     Actual     Predicted\n0    6795.0  11403.242388\n1   15750.0  21627.944630\n..      ...           ...\n39  45400.0  31852.646873\n40   8916.5  11403.242388\n\n[41 rows x 2 columns]\nMean Absolute Error: 4280.5628888250285\nMean Squared Error: 32605207.611888204\nRoot Mean Squared Error: 5710.096987958103\nR2 Score: 57.88330998687709%\n\nTest Size: 10 %\n\na = -10564.254121382277\nb = [5479.84028365]\n     Actual     Predicted\n0    6795.0  11355.107013\n1   15750.0  22314.787580\n..      ...           ...\n19   6488.0  11355.107013\n20   9959.0  11355.107013\n\n[21 rows x 2 columns]\nMean Absolute Error: 3464.088015146232\nMean Squared Error: 18346836.560473613\nRoot Mean Squared Error: 4283.32073985519\nR2 Score: 54.072095495610604%\n\n\n\n\n\n3.1.5 Engine Size vs Price\n\n\nCode\n# Engine Size Column\nengineSize = dataset['enginesize'].values.reshape(-1, 1)\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(engineSize, price, 30)\n\n\nTest Size: 30 %\n\na = -7574.131488222356\nb = [163.29075344]\n     Actual     Predicted\n0    6795.0   7285.327074\n1   15750.0  18715.679815\n..      ...           ...\n60   6479.0   7448.617828\n61  15510.0  12184.049678\n\n[62 rows x 2 columns]\nMean Absolute Error: 2898.9726929694702\nMean Squared Error: 14541824.65222288\nRoot Mean Squared Error: 3813.374444271488\nR2 Score: 77.97940083865093%\n\nTest Size: 20 %\n\na = -7613.370926304753\nb = [164.31545176]\n     Actual     Predicted\n0    6795.0   7339.335184\n1   15750.0  18841.416808\n..      ...           ...\n39  45400.0  42338.526410\n40   8916.5   7175.019732\n\n[41 rows x 2 columns]\nMean Absolute Error: 3195.031241401546\nMean Squared Error: 16835544.028987687\nRoot Mean Squared Error: 4103.113942969131\nR2 Score: 78.25324722629195%\n\nTest Size: 10 %\n\na = -8207.420855494747\nb = [169.490971]\n     Actual     Predicted\n0    6795.0   7216.257505\n1   15750.0  19080.625475\n..      ...           ...\n19   6488.0   7385.748476\n20   9959.0  10436.585954\n\n[21 rows x 2 columns]\nMean Absolute Error: 2877.111549011615\nMean Squared Error: 12997474.409783443\nRoot Mean Squared Error: 3605.2010221045157\nR2 Score: 67.46323206602058%\n\n\n\n\n\n3.1.6 Horsepower vs Price\n\n\nCode\n# Horsepower Column\nhorsepower = dataset.iloc[:, 21:-4].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(horsepower, price, 30)\n\n\nTest Size: 30 %\n\na = -4438.686268723588\nb = [170.53827527]\n     Actual     Predicted\n0    6795.0   7157.916450\n1   15750.0  22165.284674\n..      ...           ...\n60   6479.0   5452.533697\n61  15510.0  14320.524011\n\n[62 rows x 2 columns]\nMean Absolute Error: 3518.2488303322393\nMean Squared Error: 25821021.51495541\nRoot Mean Squared Error: 5081.438921698795\nR2 Score: 60.89937966412712%\n\nTest Size: 20 %\n\na = -4053.153036276188\nb = [166.64923709]\n     Actual     Predicted\n0    6795.0   7278.995086\n1   15750.0  21944.127950\n..      ...           ...\n39  45400.0  26610.306588\n40   8916.5   7612.293560\n\n[41 rows x 2 columns]\nMean Absolute Error: 3733.6933754512147\nMean Squared Error: 29626244.692692798\nRoot Mean Squared Error: 5442.999604325983\nR2 Score: 61.73128603174041%\n\nTest Size: 10 %\n\na = -4796.241165629246\nb = [174.95075436]\n     Actual     Predicted\n0    6795.0   7100.410131\n1   15750.0  22496.076514\n..      ...           ...\n19   6488.0   6050.705605\n20   9959.0  15498.046340\n\n[21 rows x 2 columns]\nMean Absolute Error: 3839.1982159225827\nMean Squared Error: 26172943.363739382\nRoot Mean Squared Error: 5115.949898478227\nR2 Score: 34.48088778430891%"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#multivariate-models",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#multivariate-models",
    "title": "Linear Regression, Classification, Clustering",
    "section": "3.2 Multivariate Models",
    "text": "3.2 Multivariate Models\nMultiple univariate models were trained for comparison using a custom Linear Regression training function. Accuracy varies in each model with changes in training/test splits and would most likely would be benefitted with more rows of data ( See Items 3.2.1 - 3.2.3 ). The highest accurracy is seen with the model that takes in the most columns for the independent variables ( See Items 3.2.3 ).\n\n3.2.1 Carlength, Carwidth, Curbweight vs Price\n\n\nCode\n# Create copy of dataset and drop all columns not used for multivariate regression models\ndatasetCopy = dataset\ndatasetCopy.drop(['carheight', 'enginetype', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio'],\\\ninplace=True, axis=1)\n\n# Store carlength, carwidth & curbweight columns in X\nX1 = datasetCopy.iloc[:, 10:-7].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X1, price, 30)\n\n\nTest Size: 30 %\n\na = -36739.21951780665\nb = [-208.48890057  764.98885537   13.97180015]\n     Actual     Predicted\n0    6795.0   5818.760203\n1   15750.0  19003.466111\n..      ...           ...\n60   6479.0   5929.766976\n61  15510.0  13762.735333\n\n[62 rows x 2 columns]\nMean Absolute Error: 2458.5442776902337\nMean Squared Error: 16492573.815910544\nRoot Mean Squared Error: 4061.1049993703123\nR2 Score: 75.02539290462342%\n\nTest Size: 20 %\n\na = -44634.67127094674\nb = [-188.42001434  856.204069     13.34802623]\n     Actual     Predicted\n0    6795.0   5783.995638\n1   15750.0  18977.251262\n..      ...           ...\n39  45400.0  29066.672270\n40   8916.5   5459.428429\n\n[41 rows x 2 columns]\nMean Absolute Error: 2943.0381053387778\nMean Squared Error: 22423198.502769\nRoot Mean Squared Error: 4735.313981434494\nR2 Score: 71.03558082852051%\n\nTest Size: 10 %\n\na = -51021.53652492876\nb = [-194.19115484  961.28023332   13.59661598]\n     Actual     Predicted\n0    6795.0   5698.395155\n1   15750.0  19277.437054\n..      ...           ...\n19   6488.0   6694.931234\n20   9959.0  10475.100811\n\n[21 rows x 2 columns]\nMean Absolute Error: 2357.566870487648\nMean Squared Error: 9101964.422986511\nRoot Mean Squared Error: 3016.9462081691995\nR2 Score: 77.21491923452972%\n\n\n\n\n\n3.2.2 Cylinder Number, Engine Size, Horsepower vs Price\n\n\nCode\n# Store cylindernumber, enginesize & horsepower columns in X\nX2 = datasetCopy.iloc[:, 13:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X2, price, 30)\n\n\nTest Size: 30 %\n\na = -6717.131795698624\nb = [-875.82889691  133.38667558   65.31455209]\n     Actual     Predicted\n0    6795.0   6359.129636\n1   15750.0  19692.219717\n..      ...           ...\n60   6479.0   5839.370791\n61  15510.0  13103.941091\n\n[62 rows x 2 columns]\nMean Absolute Error: 2681.2430726638395\nMean Squared Error: 13246002.119140355\nRoot Mean Squared Error: 3639.505752041114\nR2 Score: 79.941657245098%\n\nTest Size: 20 %\n\na = -7307.824968281864\nb = [-522.31275964  128.16628088   63.17240963]\n     Actual     Predicted\n0    6795.0   6561.779408\n1   15750.0  20047.965597\n..      ...           ...\n39  45400.0  39099.945713\n40   8916.5   6559.957946\n\n[41 rows x 2 columns]\nMean Absolute Error: 3028.44528450474\nMean Squared Error: 15255724.671464592\nRoot Mean Squared Error: 3905.8577382522003\nR2 Score: 80.29392621688581%\n\nTest Size: 10 %\n\na = -7824.384102535303\nb = [-613.42877141  135.45474355   63.82356299]\n     Actual     Predicted\n0    6795.0   6388.284759\n1   15750.0  20259.732808\n..      ...           ...\n19   6488.0   6140.798124\n20   9959.0  12025.455910\n\n[21 rows x 2 columns]\nMean Absolute Error: 2944.3040595022885\nMean Squared Error: 12712894.325743863\nRoot Mean Squared Error: 3565.5145948016907\nR2 Score: 68.17562555579416%\n\n\n\n\n\n3.2.3 Carlength, Carwidth, Curbweight, Cylinder Number, Engine Size, Horsepower vs Price\n\n\nCode\n# Store carlength, carwidth, curbweight, cylindernumber,\n# enginesize & horsepower columns in X\nX3 = datasetCopy.iloc[:, 10:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X3, price, 30)\n\n\nTest Size: 30 %\n\na = -50133.27454870472\nb = [-62.20404054 772.47835707   3.18527494  18.99449375  65.77507898\n  64.33402142]\n     Actual     Predicted\n0    6795.0   6067.345502\n1   15750.0  20331.280734\n..      ...           ...\n60   6479.0   5548.422659\n61  15510.0  13525.755400\n\n[62 rows x 2 columns]\nMean Absolute Error: 2536.7293535638096\nMean Squared Error: 12555624.008739235\nRoot Mean Squared Error: 3543.39159686581\nR2 Score: 80.98709273909488%\n\nTest Size: 20 %\n\na = -54793.72590677004\nb = [-38.92156396 789.71920542   2.95208156 366.09875078  59.3369646\n  58.3182745 ]\n     Actual     Predicted\n0    6795.0   6167.243070\n1   15750.0  20562.635157\n..      ...           ...\n39  45400.0  36977.654082\n40   8916.5   5783.745608\n\n[41 rows x 2 columns]\nMean Absolute Error: 2873.7239149228203\nMean Squared Error: 16025434.859390952\nRoot Mean Squared Error: 4003.178094887979\nR2 Score: 79.29967874050975%\n\nTest Size: 10 %\n\na = -55531.82635570387\nb = [-30.01857827 784.53201154   2.29960226 213.59515415  74.83936968\n  58.2469381 ]\n     Actual     Predicted\n0    6795.0   6065.470340\n1   15750.0  20665.341927\n..      ...           ...\n19   6488.0   5585.072554\n20   9959.0  11876.766620\n\n[21 rows x 2 columns]\nMean Absolute Error: 3020.6881171033187\nMean Squared Error: 13605511.765351668\nRoot Mean Squared Error: 3688.5650008304947\nR2 Score: 65.94112325398689%"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#no-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#no-scaling",
    "title": "Linear Regression, Classification, Clustering",
    "section": "4.1 No Scaling",
    "text": "4.1 No Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'None')\n\n\n\n\n\nKNeigbors Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       0.00      0.00      0.00         6\n         gas       0.85      0.94      0.89        35\n\n    accuracy                           0.80        41\n   macro avg       0.42      0.47      0.45        41\nweighted avg       0.72      0.80      0.76        41\n\nDecision Tree Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         6\n         gas       1.00      1.00      1.00        35\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure¬†5. Classifier Models Confusion Matrices - No Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#standardized-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#standardized-scaling",
    "title": "Linear Regression, Classification, Clustering",
    "section": "4.2 Standardized Scaling",
    "text": "4.2 Standardized Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Standardize')\n\n\n\n\n\nKNeigbors Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure¬†6. Classifier Models Confusion Matrices - Standard Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#normalized-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#normalized-scaling",
    "title": "Linear Regression, Classification, Clustering",
    "section": "4.3 Normalized Scaling",
    "text": "4.3 Normalized Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Normalize')\n\n\n\n\n\nKNeigbors Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure¬†7. Classifier Models Confusion Matrices - Normalized Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#kmeans-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#kmeans-model",
    "title": "Linear Regression, Classification, Clustering",
    "section": "5.1 KMeans Model",
    "text": "5.1 KMeans Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# KMeans Cluster Model to be used\nmodel = 'KM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Scatter Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - No Scaling\n\n\n\n\n\n\n\n\n\n(c) Joint Plot - Normalized Scaling\n\n\n\n\nFigure¬†8. KMeans Cluster Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#gaussian-mixture-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#gaussian-mixture-model",
    "title": "Linear Regression, Classification, Clustering",
    "section": "5.2 Gaussian Mixture Model",
    "text": "5.2 Gaussian Mixture Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Gaussian Mixture Model to be used\nmodel = 'GMM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Joint Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - Normalized Scaling\n\n\n\n\nFigure¬†9. Gaussian Mixture Cluster Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#spectral-clustering-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#spectral-clustering-model",
    "title": "Linear Regression, Classification, Clustering",
    "section": "5.3 Spectral Clustering Model",
    "text": "5.3 Spectral Clustering Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Spectral Clustering Model to be used\nmodel = 'SC'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Joint Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - Normalized Scaling\n\n\n\n\nFigure¬†10. Spectral Clustering Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#references",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#references",
    "title": "Linear Regression, Classification, Clustering",
    "section": "References",
    "text": "References\nDataset\nhttps://www.kaggle.com/datasets/hellbuoy/car-price-prediction?select=CarPrice_Assignment.csv\nData Cleaning\nhttps://datatofish.com/category/python/\nData Scaling\nhttps://dataakkadian.medium.com/standardization-vs-normalization-da7a3a308c64\nhttps://medium.datadriveninvestor.com/data-pre-processing-with-scikit-learn-9896c561ef2f\nMeasuring Accuracy\nhttps://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/\nVisualizations\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\nhttps://seaborn.pydata.org/api.html\nhttps://matplotlib.org/stable/api/pyplot_summary.html\nClassification & Clustering Models\nhttps://www.activestate.com/resources/quick-reads/how-to-classify-data-in-python/\nhttps://builtin.com/data-science/data-clustering-python\nhttps://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\nGeneral Python\nhttps://stackoverflow.com"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#purpose-of-document",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#purpose-of-document",
    "title": "Deep Learning FastAI Model",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to detail the building of deep learning models using a convolutional neural network architecture. The different techniques, models and methods used to improve performance will be discussed."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#bee-vs-wasp",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#bee-vs-wasp",
    "title": "Deep Learning FastAI Model",
    "section": "2.1 Bee vs Wasp",
    "text": "2.1 Bee vs Wasp\nFor this project I chose a Bee vs Wasp dataset found on Kaggle. I imported the dataset and created a new folder called images that I then put subfolders bee1, bee2, wasp1, wasp2, other_insect and other_noinsect into. The data loader in my custom train_models function then creates classes based on the folder structure and feeds that to the model. The data set itself isn‚Äôt the cleanest as it seems that some images have not been placed in the correct folder which will sometimes give the model wrong information. No doubt this will affect the accuracy that can be attained with this dataset.\n\n\n\n\n\n\nDataset\n\n\n\nView Bee vs Wasp dataset on Kaggle."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#trial-and-error",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#trial-and-error",
    "title": "Deep Learning FastAI Model",
    "section": "3.1 Trial and Error",
    "text": "3.1 Trial and Error\nTo begin with I created a custom function named train_models that I could use to conduct my tests a little faster. With a trial and error approach, I began manually trying different learning rates, model types and image sizes, along with training models with unfrozen weights ( See Figure¬†1, Table¬†1, Figure¬†2 & Table¬†2 ). Eventually I thought I should start trying to automate some of these tuning methods and, by doing so, hopefully optimize the outcomes.\n\n\n\n\n\n\nNote\n\n\n\nView full details of the trial and error testing in the Experimenting section on the Google Colab Notebook.\n\n\n\n\n#Function to train models more easily\ndef train_models(image_size, batch_size, images_path, test_size, model_type):\n    #instructions for preparing data batches, size of images\n    #and normalize data\n    batch_tfms = [*aug_transforms(size=image_size),Normalize.from_stats(*imagenet_stats)]\n\n    #function for creating batches with specified parameters\n    data = ImageDataLoaders.from_folder(images_path,\n                                        valid_pct=test_size,\n                                        ds_tfms=batch_tfms,\n                                        item_tfms=Resize(460),\n                                        bs=batch_size)\n    \n    # test whether batch function is working with parameters\n    data.show_batch(max_n=9, figsize=(20,10))\n\n    #return the trained model\n    return vision_learner(data, model_type, metrics=error_rate).to_fp16()\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#image size\nimage_size = 224\n\n#batch size, number of images to transfer to GPU to train at one time\nbatch_size = 64\n\n#Image path\nimages_path = \"kaggle_bee_vs_wasp/images\"\n\n#test size\ntest_size = 0.2\n\n#CNN model\nmodel_type = resnet34\n\n#Create model with dataset and parameters\nlearn_resnet34 = train_models(image_size, batch_size, images_path, test_size, model_type)\n\n\n\n\n\nFigure¬†1. First resnet34 model test\n\n\n\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#train with discovered learning\n#rates and train two more epochs... may improve accuracy\nlearn_resnet34.fit_one_cycle(2, lr_max=slice(1e-6,1e-3))\n\n\n\n\n\n\n\n\nTable¬†1. First resnet34 best training results\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.179072\n0.169389\n0.055166\n02:12\n\n\n1\n0.105617\n0.161333\n0.046848\n02:08\n\n\n\n\n\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#image size\nimage_size = 224\n\n#batch size, number of images to transfer to GPU to train at one time\nbatch_size = 64\n\n#Image path\nimages_path = \"kaggle_bee_vs_wasp/images\"\n\n#test size\ntest_size = 0.2\n\n#CNN model\nmodel_type = resnet50\n\n#Try resnet50 with same image size as first resnet34 tests\nlearn_resnet50 = train_models(image_size, batch_size, images_path, test_size, model_type)\n\n\n\n\n\nFigure¬†2. First resnet50 model test\n\n\n\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#save where the model is currently at\nlearn_resnet50.save('stage_2')\n# freeze most of the weights again and train two more epochs\nlearn_resnet50.freeze()\nlearn_resnet50.fit_one_cycle(2)\n\n\n\n\n\n\n\n\nTable¬†2. First resnet50 best training results\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.152754\n0.199921\n0.056918\n04:58\n\n\n1\n0.082319\n0.159553\n0.042907\n04:59\n\n\n\n\n\n\nSource: Google Colab Notebook"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automating-hyperparameter-tuning",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automating-hyperparameter-tuning",
    "title": "Deep Learning FastAI Model",
    "section": "3.2 Automating Hyperparameter Tuning",
    "text": "3.2 Automating Hyperparameter Tuning\nIn research I found a Python library called Optuna that could be used to automate hyperparameter tuning. Optuna does this by creating a ‚Äústudy‚Äù that runs a user specified amount of trials and uses an objective function to suggest user specified parameters to optimize for a certain metric. So in this case, I created a custom objective function named tune_hyperparameters that takes in learning rate, batch size, and weight decay parameters and returns the error rate of the model trained with those parameters. The Optuna optimize function then suggests hyperparameters that should start lowering the error rate of successive trials. I then wrote another custom function called optimization_study that ran the Optuna study using the tune_hyperparameters function. The optimization_study function also selects the trial that did the best and proceeds to unfreeze all of the weights and train the model again with the best found hyperparameters. Some of my initial tests with this automated hyperparameter tuning proved promising as I was able to get the error rate lower than I had previously gotten it.\n\n\n\n\n\n\nNote\n\n\n\nView full details of the automation testing in the Automate Hyperparameter Tuning section on the Google Colab Notebook.\n\n\n\n\n\n\n# Custom function to choose the best trial and unfreeze weights to train further\ndef optimization_study(selected_model):\n\n    # Create an Optuna study and optimize the objective function\n    study = optuna.create_study(direction=\"minimize\") # Minimize the error rate\n    study.optimize(lambda trial: tune_hyperparameters(trial, image_size, images_path, test_size, selected_model), n_trials=3)\n\n    # Print the best hyperparameters and the corresponding accuracy\n    best_params = study.best_params\n    best_error_rate = study.best_value\n    print(\"Best Hyperparameters:\", best_params)\n    print(\"Best Accuracy:\", best_error_rate)\n\n    #retrieve best model's state dict file\n    best_state_dict_file = f\"{selected_model}_state_dict_trial_{study.best_trial.number}.pth\"\n\n    # Get the best trial from the study\n    best_trial = study.trials[study.best_trial.number]\n\n    # Retrieve the hyperparameters of the best trial\n    hyperparameters = best_trial.params\n\n    # Access individual hyperparameters\n    learning_rate = hyperparameters[\"learning_rate\"]\n    batch_size = hyperparameters[\"batch_size\"]\n    weight_decay = hyperparameters[\"weight_decay\"]\n\n    #Create model with the same architecture as the best trial and load dict file into it\n    best_trial_learn = train_models(image_size, batch_size, images_path, test_size, selected_model)\n    best_trial_learn.model.load_state_dict(torch.load(best_state_dict_file))\n\n    #unfreeze all weights to train with optimal hyperparameters\n    best_trial_learn.unfreeze()\n    # Fit the model with the best trial's hyperparameters\n    best_trial_learn.fine_tune(4, base_lr=learning_rate, wd=weight_decay)\n\n    #close it back up\n    best_trial_learn.freeze()\n\n    #return the model with all of the best results\n    return best_trial_learn\n\nSource: Google Colab Notebook"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automate-testing-different-models",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automate-testing-different-models",
    "title": "Deep Learning FastAI Model",
    "section": "3.3 Automate Testing Different Models",
    "text": "3.3 Automate Testing Different Models\nAs I started to achieve some good results with my automations I decided to go even further. I wrote another custom function called try_models that loops through a list of different models, runs an Optuna study on it and saves the model state from the best trial from that particular study on that particular model. Once the try_models function has finished looping through the list of models it selects the model that achieved the lowest error rate, creates a learner from that model and loads the model state of the best trial from that model. It then proceeds to unfreeze all of the weights and train the model again with hyperparameters from that particular model‚Äôs best trail. After training is complete the function freezes the weights again, displays the results, and returns the model. I found some success using this new function as long as I kept the trial size relatively low as when I increased the trial size it exponentially increases compute time and quickly reaches the limits of free tier kernels.\n\n\n\n\n\n\nNote\n\n\n\nView full details of the try_models function automation testing in the Automate testing different models section on the Kaggle Notebook.\n\n\n\n\n# Custom function to try different models with the other automation functions\ndef try_models(image_size, images_path, test_size, models, trial_size):\n    best_trials = {}\n    for model in models:\n        best_trials[model.__name__] = optimization_study(image_size, images_path, test_size, model, trial_size)\n        \n    best_overall = min(best_trials,  key=lambda x: best_trials[x].value)\n    lowest_model = best_trials[best_overall]\n    print(\"\\n\\nBest Overall Model:\", lowest_model.user_attrs['model'].__name__)\n    print(\"Error Rate:\", lowest_model.value)\n    print(\"Load Model's state and retrain with best hyperparameters\")\n    \n    \n    #retrieve best model's state dict file\n    best_state_dict_file = f\"/kaggle/working/{lowest_model.user_attrs['model'].__name__}_state_dict_trial_{lowest_model.number}.pth\"\n\n    \n    # Retrieve the hyperparameters of the best trial\n    hyperparameters = lowest_model.params\n\n    # Access individual hyperparameters\n    learning_rate = hyperparameters[\"learning_rate\"]\n    batch_size = hyperparameters[\"batch_size\"]\n    weight_decay = hyperparameters[\"weight_decay\"]\n\n    #Create model with the same architecture as the best trial and load dict file into it\n    best_model_learn = train_models(image_size, batch_size, images_path, test_size, lowest_model.user_attrs[\"model\"])\n    best_model_learn.model.load_state_dict(torch.load(best_state_dict_file))\n\n    #unfreeze all weights to train with optimal hyperparameters\n    best_model_learn.unfreeze()\n    # Fit the model with the best trial's hyperparameters\n    best_model_learn.fine_tune(1, base_lr=learning_rate, wd=weight_decay)\n\n    #close it back up\n    best_model_learn.freeze()\n    \n    # Evaluate the model on the validation set\n    best_error_rate = best_model_learn.validate()[1]\n    \n    print(f\"\\n\\nFinal result after training model \"+lowest_model.user_attrs['model'].__name__+\" with:\")\n    print(\"Hyperparameters:\", hyperparameters)\n    print(\"Error Rate:\", best_error_rate)\n\n    \n    #return the model with all of the best results\n    return best_model_learn\n\nSource: Kaggle Notebook"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#data-augmentation",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#data-augmentation",
    "title": "Deep Learning FastAI Model",
    "section": "3.4 Data Augmentation",
    "text": "3.4 Data Augmentation\nI also briefly experimented with some data augmentation, namely randomly cropping to a 224x224 image size and introducing a random horizontal flip to the images. Tests with this didn‚Äôt seem to yield any improved results, in fact it seems it may have adversely affected model performance in training. I theorize that this didn‚Äôt have much effect because the dataset already possesses a great deal of randomness so injecting more isn‚Äôt advantageous."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#usage-limits",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#usage-limits",
    "title": "Deep Learning FastAI Model",
    "section": "4.1 Usage Limits",
    "text": "4.1 Usage Limits\nVery early on it was clear that usage limits of free tier kernels would significantly limit the ability to experiment, test and iterate. For this reason, the approach was taken to use more than one kernel so that when one reached its limit the other could be used to continue with the project. Google Colab and Kaggle were both used to complete this project and in the following two items ( 4.2 Google Colab & 4.3 Kaggle ) in this section I detail what each kernel was primarily used for. A notebook from each kernel is provided in this project submission, with Part 1 and Part 3 being included in the Google Colab notebook and Part 2 being included in the Kaggle notebook."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#google-colab",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#google-colab",
    "title": "Deep Learning FastAI Model",
    "section": "4.2 Google Colab",
    "text": "4.2 Google Colab\nI started my initial experimentation in Google Colab and that is why it starts with the heading Part 1. Part way through the refinement of my custom automation functions I reached my limit with Google Colab so Part 2 of my code is found in the Kaggle notebook. The final part of my testing and code can be found under Part 3 of the Google Colab notebook. In Part 3 I decided to purchase some Pay-As-You_Go compute so that I could continue the rest of my project without further delays."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#kaggle",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#kaggle",
    "title": "Deep Learning FastAI Model",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\nThe Kaggle notebook starts with the heading of Part 2 as it is the point where I switched from Google Colab. The Kaggle notebook only includes one part and it is where most of the refinements on my custom functions can be found. I was able to make some fairly large tests at the end of the Kaggle notebook but then reached my limit. At this point I switched back to finish things off in my Google Colab notebook under Part 3."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#worst-performance",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#worst-performance",
    "title": "Deep Learning FastAI Model",
    "section": "5.1 Worst Performance",
    "text": "5.1 Worst Performance\nI wasn‚Äôt able to test VGG16 to long before I ran into limit restrictions on the kernel but it wasn‚Äôt performing all that well from what was seen. Further investigation would be required to confirm that VGG16 is not a good model for this dataset. SqueezeNet models did not perform as well as the other models which is not surprising giving the size and architecture of SqueezeNet models ( See Figure¬†3 ). The 896x896 image size did not seem to yield better results and neither did batch sizes 16 and 64. Learning rate range 1e-5 - 1e-1 did not yield good results as well as weight decay range 1e-5 ‚Äì 1e-3.\n\n\n\nFigure¬†3. Best SqueezeNet model results after optimization automations were the worst results."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#best-performance",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#best-performance",
    "title": "Deep Learning FastAI Model",
    "section": "5.2 Best Performance",
    "text": "5.2 Best Performance\nAfter studying some of the tests I started to isolate that a batch size of 32 did consistently well. Along with training only with a 32 batch size I narrowed the learning rate range to 1e-3 ‚Äì 1e-2 and the weight decay range to 1e-5 -1e-4 as these ranges seems to provide the best results. In the end of all my testing the best performance I achieved was from a Resnet32 model trained with a 224 image size, 32 batch size, a learning rate of 3.102551277095900e-3 and a weight decay of 7.49113519525403e-05. This yielded a model with a training loss of 0.022758, valid loss of 0.065226, and error rate of 0.015762. These results show that the model is slightly overfitted but performing quite well. ( See Figure¬†4 )\n\n\n\nFigure¬†4. Best Resnet model results after optimization automations were the best performance."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#model-training-video-walkthrough",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#model-training-video-walkthrough",
    "title": "Deep Learning FastAI Model",
    "section": "Model Training Video Walkthrough",
    "text": "Model Training Video Walkthrough\nVideo"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#references",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#references",
    "title": "Deep Learning FastAI Model",
    "section": "References",
    "text": "References\n\nOptimization Library\nhttps://optuna.org/"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#purpose-of-document",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#purpose-of-document",
    "title": "Hotel Booking System Mock Design",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide system designs to use in the development of the hotel booking system for City and Resort hotels."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#scope",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#scope",
    "title": "Hotel Booking System Mock Design",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an examination of the business scenario to ascertain how best to design, develop, and improve upon the proposed hotel booking system. In addition, creation of system designs and implementation documentation must be produced to drive the development of the project. The proposed solutions must include detailed logical & physical Entity-Relationship Diagrams, a Context Diagram, and up to Level 2 Data Flow Diagrams.\n\n\n\n\n\n\nExpand To Learn About Business Scenario\n\n\n\n\n\nThe purpose of the Hotel Booking System is to assist with room booking for two of the Hotels, the RESORT HOTEL and CITY HOTEL. Through the system, the Customers book the hotel with their choice of hotel and booking details such as arrival date, number of adults and children. The hotel can ask for any missing information such as meal choice, and country details the customer through email. The customer replies to the hotel with all the missing information. Both the hotel stores the customer details in their customer detail database. The hotel generates the bills on the arrival of the customer. While generating the RESORT HOTEL gives a 10% discount if the customer has stayed at RESORT HOTEL before and the CITY HOTEL gives a 12 % discount to any customer who has stayed at that hotel before. But, if any customer has a history of any cancellations in the past year then the RESORT HOTEL does not give any discount. All the payments are handled by ABC Bank. The bank generates the invoice of the payment and sends copies to the customer and the hotel. The system also communicates with a nearby Restaurant which looks after all the meal requests of the customer. The restaurant gets a notification about the food requirements of customers once the booking confirmation is generated. In addition, if any customer needs the car parking facility then the customer needs to notify while generating the booking details. The Car Parking Agency, co-partner of the hotel group handles the parking facility. The car parking invoice is generated separately and sent to the customer for payment at the time of departure."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#waterfall",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#waterfall",
    "title": "Hotel Booking System Mock Design",
    "section": "2.1 Waterfall",
    "text": "2.1 Waterfall\nThe Waterfall methodology contains five phases that are as follows: Requirements, Design, Implementation, Verification and Maintenance. Each of these phases are present in most methodologies however with the Waterfall approach each phase is followed by the next in strict fashion. The team can only progress to the next phase when the current phase has been completed and you cannot revert back to a previous phase. This approach is useful when a high level of reliability is needed for a project."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#agile",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#agile",
    "title": "Hotel Booking System Mock Design",
    "section": "2.2 Agile",
    "text": "2.2 Agile\nThe Agile method is an iterative approach that allows for similar phases as Waterfall to be engaged simultaneously and in no specific order. The team is able to work in small increments allowing for customer input regularly. This methodology is far more flexible in terms of adapting to changes in customer wants and needs as they see the project develop."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#preferred-software-development-methodology",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#preferred-software-development-methodology",
    "title": "Hotel Booking System Mock Design",
    "section": "2.3 Preferred Software Development Methodology",
    "text": "2.3 Preferred Software Development Methodology\nThe fact that there are two hotels that will be using this system in essence means that there two clients to consider in the development of this project. Each client has unique requirements that differ from one another and are subject to change. As the hospitality industry is fast paced and endeavours to adapt to ever changing customer opinions, the Agile methodology is best suited for this project. As stated above, Waterfall is great for certain projects but is highly inflexible while Agile allows for development teams to incorporate client input in small, digestible increments. This approach will translate to a product that provides more utility to the customer which in turn will result in higher customer satisfaction. ( See Appendix ‚Äì Waterfall Methodology & Agile Methodology )"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#logical-entity-relationship-diagram",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#logical-entity-relationship-diagram",
    "title": "Hotel Booking System Mock Design",
    "section": "3.1 Logical Entity-Relationship Diagram",
    "text": "3.1 Logical Entity-Relationship Diagram\nThe four entities in this diagram are as follows: Customer, City Reservation, Resort Reservation and Invoice. Each entity has a primary key that consists of a uniquely generated number. The system is purposefully designed with no weak entities and foreign keys as this is not advisable to create a robust system. The Customer can either book a reservation at the City or Resort hotel. There is a need to delineate the City and Resort hotel reservations because of their individual discount policies. Each hotel will only reward their customers with a discount when they have previously stayed at their specific location before. In addition, the Resort hotel will only provide a discount to customers who haven‚Äôt had any cancellations at their location in the past year. Therefore the Resort Reservation entity has an addition attribute labelled ‚ÄúCancelled‚Äù that records whether any of the reservations have been cancelled. With the information supplied from the reservation entities the Invoice is populated. ( See Figure¬†1 )\n\n\n\nFigure¬†1. Logical Entity-Relationship Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#physical-entity-relationship-diagram",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#physical-entity-relationship-diagram",
    "title": "Hotel Booking System Mock Design",
    "section": "3.2 Physical Entity-Relationship Diagram",
    "text": "3.2 Physical Entity-Relationship Diagram\nThe invoice entity contains a composite and multivalued attribute labelled ‚ÄúPayment‚Äù that allows for storage of the date and amount for payments made towards the total. When payments are made the ‚ÄúAmount Owing‚Äù attribute on the entry is updated accordingly. ( See Figure¬†2 )\n\n\n\nFigure¬†2. Physical Entity-Relationship Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#data-fields-with-sample-data",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#data-fields-with-sample-data",
    "title": "Hotel Booking System Mock Design",
    "section": "3.3 Data Fields with Sample Data",
    "text": "3.3 Data Fields with Sample Data\n\n\n\nFigure¬†3. Data Fields with Sample Data"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#context-diagram",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#context-diagram",
    "title": "Hotel Booking System Mock Design",
    "section": "4.1 Context Diagram",
    "text": "4.1 Context Diagram\nSix (6) entities: Customer, City Hotel, Resort Hotel, Car Parking Agency, Restaurant and ABC Bank interact with the system from a data flow perspective. A hotel only interacts with the system if the customer has chosen the specified hotel. Each entity‚Äôs interactions with the system will be described in further depth in the items to follow. ( See Figure¬†4 )\n\n\n\nFigure¬†4. Context Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-1-dfd",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-1-dfd",
    "title": "Hotel Booking System Mock Design",
    "section": "4.2 Level 1 DFD",
    "text": "4.2 Level 1 DFD\nThere are three (3) processes depicted at level 1 that handle the flow of data for the system. Process 1, ‚ÄúBooking‚Äù, involves making a reservation, obtaining missing information for the reservation and cancelling the reservation. Process 2, ‚ÄúCheck-In‚Äù, is where the customer checks into their room and makes an initial payment. Process 3, ‚ÄúCheck-Out‚Äù, ends the customer‚Äôs interaction with the system by settling the remaining balance on the invoice and paying the parking invoice if the customer has opted for parking. ( See Figure¬†5 )\n\n\n\nFigure¬†5. Level 1 Data Flow Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-booking-process-1",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-booking-process-1",
    "title": "Hotel Booking System Mock Design",
    "section": "4.3 Level 2 DFD ‚Äì Booking Process 1",
    "text": "4.3 Level 2 DFD ‚Äì Booking Process 1\nThere are five (5) sub-processes depicted at level 2 of ‚ÄúBooking‚Äù Process 1. In Process 1.1 the customer submits their customer details which are then stored. Process 1.2 generates room and date availability information by examining current reservations which is then used by the customer to submit their reservation preferences. In Process 1.3 the reservation details are stored and the hotel sends the customer a reservation code and request, in the form of an email, to submit missing information. The only none mandatory fields that may be missing are ‚ÄúFood Requirements‚Äù and ‚ÄúParking‚Äù. ( See Figure¬†1, Figure¬†2 and Figure¬†3 ) Process 1.4 gives the customer an opportunity to submit the missing information by clicking a link provided in the information request email and filling in the missing fields. This new information is then sent to the hotel, restaurant and parking agency along with being saved in the customer‚Äôs reservation entry. Process 1.5 allows the customer to cancel the reservation if desired. When a cancellation is made the ‚ÄúCancellation‚Äù field is updated if it is a Resort reservation entry or it deletes the entry in the case of a City reservation. All relevant parties are then informed of the cancellation. ( See Figure¬†6 )\n\n\n\nFigure¬†6. Level 2 Data Flow Diagram Booking Process 1"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-in-process-2",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-in-process-2",
    "title": "Hotel Booking System Mock Design",
    "section": "4.4 Level 2 DFD ‚Äì Check-In Process 2",
    "text": "4.4 Level 2 DFD ‚Äì Check-In Process 2\nThere are four (4) sub-processes depicted at level 2 of ‚ÄúCheck-In‚Äù Process 2. In Process 2.1 the customer submits their reservation code which is then used to extract the customer‚Äôs details and reservation. This information is used by the hotel to generate an invoice which is then stored and a copy is sent to the customer in Process 2.2. In Process 2.3 the customer submits payment information which is then received by ABC bank along with the invoice. In Process 2.4 the bank handles the payment, sends payment confirmation to all relevant parties and the invoice entry is updated with payment details. ( See Figure¬†7 )\n\n\n\nFigure¬†7. Level 2 Data Flow Diagram Check-In Process 2"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-out-process-3",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-out-process-3",
    "title": "Hotel Booking System Mock Design",
    "section": "4.5 Level 2 DFD ‚Äì Check-Out Process 3",
    "text": "4.5 Level 2 DFD ‚Äì Check-Out Process 3\nThere are three (3) sub-processes depicted at level 2 of ‚ÄúCheck-Out‚Äù Process 3. In Process 3.1 the customer submits their reservation code which is then used by the hotel to bring up the customer‚Äôs details, reservation and invoice. The customer details and reservation are also sent to the Car Parking Agency which uses Process 3.2 to send the customer a parking invoice along with their hotel invoice. In Process 3.3 the customer submits payment information for both invoices, which is then received by ABC bank along with both invoices. In Process 3.4 the bank processes the payment and sends payment confirmation to all relevant parties and the hotel invoice entry is updated with payment details. ( See Figure¬†8 )\n\n\n\nFigure¬†8. Level 2 Data Flow Diagram Check-Out Process 3"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#additional-review-process",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#additional-review-process",
    "title": "Hotel Booking System Mock Design",
    "section": "5.1 Additional Review Process",
    "text": "5.1 Additional Review Process\nAn improvement upon the system could be achieved by adding a fourth process at level 1 to prompt the customer to provide a review. The review information could then be stored in an additional field in the reservation data stores. This review information could then be analyzed by AI technology to produce actionable suggestions on how to improve the customer‚Äôs experience."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#sentiment-intent-analysis",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#sentiment-intent-analysis",
    "title": "Hotel Booking System Mock Design",
    "section": "5.2 Sentiment & Intent Analysis",
    "text": "5.2 Sentiment & Intent Analysis\nNatural language processing could be employed to analyze thousands of reviews to identify recurring themes and topics reported on by customers. These can then be rated so that hotel staff can see how customers feel and why they feel that way. This would allow for staff to see where best to make improvements to their services thereby increasing customer satisfaction. ( See Appendix ‚Äì Sentiment & Intent Analysis )"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#references",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#references",
    "title": "Hotel Booking System Mock Design",
    "section": "References",
    "text": "References\n\nWaterfall Methodology\nhttps://www.projectmanager.com/guides/waterfall-methodology/\n\n\nAgile Methodology\nhttps://www.atlassian.com/agile\n\n\n\n\nSentiment & Intent Analysis\nhttps://www.lexalytics.com/technology/sentiment-analysis/\nhttps://www.lexalytics.com/technology/intentions/"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#purpose-of-document",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#purpose-of-document",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide a business case to assist ACME Inc in evaluating the viability and profitability of the proposed project. It details estimates on functions/activities, cost of development, costs of running and hardware/network requirements in developing and maintaining a machine learning software for ACME Inc."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#scope",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#scope",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nScope can be split into three parts: (1) development of machine learning software, (2) setup and deployment of data pipeline for feeding to the software and (3) maintaining the solution for sustained future company use. This proposed solution does not include training staff on machine learning software after deployment but can be negotiated."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#background",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#background",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "2.1 Background",
    "text": "2.1 Background\nACME Inc is increasingly accumulating data on the customers that interact with the company. As the company grows, profits have not been increasing in proportion to the rising costs of larger infrastructure to support growth. At the same time marketing is struggling to identify where their efforts are most effective as the customer base widens along with product lines. The proposed project provides a solution to these problems by making use of the ever growing asset the company possesses, customer data.\n\n\n\n\n\n\nNote\n\n\n\nSee 5 Preliminary Findings of this document to investigate some of the initial insights that the prototype software has been able to uncover."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#objectives",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#objectives",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "2.2 Objectives",
    "text": "2.2 Objectives\n2.2.1 Improve ACME‚Äôs data hygiene so that its data is better utilized.\n2.2.2 Produce new actionable insights with company data which can be used to benefit ACME.\n2.2.3 Provide concrete targets for future marketing campaigns.\n2.2.4 Increase company profits."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#functionsactivities",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#functionsactivities",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.1 Functions/Activities",
    "text": "3.1 Functions/Activities\nA centralized database server will need to be securely setup for data to be loaded into for the software to access. The database server will require at least one (1) database admin to deploy and maintain. Software development itself will require three (3) phases: (1) Program, system, operations and user planning/design/documentation. (2) Development of software based on designs and deployment. (3) Maintenance of software with scheduled patches, updates and new features. To successfully execute the software development portions staff required are as follows‚Ä¶ one (1) project manager, two (2) designers, one (1) UI developer and one (2) machine learning engineers. In the maintenance stage, staff can be reduced to just one (1) manager, one (1) database admin, one (1) developer and (1) machine learning engineer."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-development",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-development",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.2 Cost of Development",
    "text": "3.2 Cost of Development\n\n\n\n\n\n\n\n\n3.2.1\nOne (1) project manager ‚Äì 3 months\n¬£25,000\n\n\n3.2.2\nTwo (2) designers ‚Äì 1 month\n¬£10,000\n\n\n3.2.3\nOne (1) UI developer ‚Äì 1 month\n¬£5,000\n\n\n3.2.4\nOne (1) database admin ‚Äì 2 weeks\n¬£2,500\n\n\n3.2.5\nTwo (2) machine learning engineers ‚Äì 2 months\n¬£30,000\n\n\n\nTOTAL\n¬£72,500"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-running",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-running",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.3 Cost of Running",
    "text": "3.3 Cost of Running\n\n\n\n3.3.1\nOne (1) manager ‚Äì 3 months\n¬£100,000/year\n\n\n3.3.2\nOne (1) database admin - retainer\n¬£10,000/year\n\n\n3.3.3\nOne (1) developer\n¬£60,000/year\n\n\n3.3.4\nOne (1) machine learning engineer\n¬£90,000/year\n\n\n\nTOTAL\n¬£260,000/year\n\n\n\n\n\n\n\n\n\nTotals are estimated based on current market costs as of June 2023 and are subject to change."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#hardwarenetwork-costs",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#hardwarenetwork-costs",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.4 Hardware/Network Costs",
    "text": "3.4 Hardware/Network Costs\nDatabase deployment and software development can be achieved using the company‚Äôs current infrastructure and hardware available."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#database-implementation",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#database-implementation",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "4.1 Database Implementation",
    "text": "4.1 Database Implementation\nPrototype database has been setup using a free tier Oracle Standard.A1.Flex virtual machine instance(1 CPU, 6GB RAM, Ubuntu 22.04). A MariaDB server has been installed on the instance with a user account created for the software to access the server directly. Mysql.connector and SQLAlchemy python libraries are used by the prototype software to create, delete, and access tables in the server. The prototype software allows the user to open any CSV file and to create a table on the server to import the CSV data into."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#data-cleaning-and-exploration",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#data-cleaning-and-exploration",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "4.2 Data Cleaning and Exploration",
    "text": "4.2 Data Cleaning and Exploration\nOnce a user has selected a table using the ‚ÄúTraining Manager‚Äù toolbar, the table data is converted in a Pandas data frame for cleaning, exploration and model training. In the ‚ÄúDataFrame‚Äù toolbar the user is able to convert columns in the data frame to different data types in preparation for training. The user can also display the current data frame information and head data by using the ‚ÄúInfo‚Äù button on the ‚ÄúDataFrame‚Äù toolbar. In addition, the user can can explore the data by generating plots using the ‚ÄúDataFrame‚Äù toolbar. Finally, rows with NaN values are dropped from the data frame when the user clicks the ‚ÄúTrain Model‚Äù button so that the data fed into the algorithms doesn‚Äôt produce errors."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#training-models",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#training-models",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "4.3 Training Models",
    "text": "4.3 Training Models\nOnce a user has selected a table using the ‚ÄúTraining Manager‚Äù toolbar, they can then select columns that they want to use to train models by using the arrow buttons. The user can select whether they want to train a linear regression, classification or clustering model using the ‚ÄúAlgorithm‚Äù combobox. If either classification or clustering options are chosen they the user can further choose the method for training whether it be: (1) KNeighbors or Decision Tree for classification, or (2) KMeans, Gaussian Mixture, or Spectral Clustering for clustering. User is able to Standardize or Normalize the data for training by making a selection using the ‚ÄúScaling‚Äù combobox. Linear regression or classification models train starting at the test size selected by the user and decrement down by 10 until a test size of 10 has finally been trained. Clustering models train classify clusters based on how many clusters are set by the ‚ÄúCluster‚Äù slider. All training results are displayed for the user in the view window."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#predict-wine-purchases",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#predict-wine-purchases",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.1 Predict Wine Purchases",
    "text": "5.1 Predict Wine Purchases\nUsing a multivariate linear regression model trained on current company data the amount of wine a given customer will purchase can be predicted with 80% accuracy. This insight could be used to direct specific marketing campaigns and extend wine deals towards customers who are likely to buy more wine based on the model‚Äôs predictions. ( See Figure¬†1 )\n\n\n\nFigure¬†1. Predict Wine Purchases"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-kids-wine",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-kids-wine",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.2 Relationship Between Kids & Wine",
    "text": "5.2 Relationship Between Kids & Wine\nUsing a KNeighbors classification model trained on company data it is clear to see that there is a partial relationship between the amount of wine bought and how many kids are in the home. It seems that as the amount of children in the home goes up that less wine is purchased. With this insight in mind, marketing should spend less resources advertising wine towards households that have children. ( See Figure¬†2 )\n\n\n\nFigure¬†2. Relationship Between Kids & Wine"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-meats-sweets",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-meats-sweets",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.3 Relationship Between Meats & Sweets",
    "text": "5.3 Relationship Between Meats & Sweets\nUsing a KMeans clustering model trained on standardized company data, customers habits can be established between meats and sweets. As seen in Figure¬†3 customers could be placed in certain categories based on their meats and sweets purchases that would define where marketing efforts may best be spent. For example, those that buy a high amount of meats fit into a category of customer that doesn‚Äôt purchase as much sweets and visa versa. So if a customer fits into a category of high meat purchases then less efforts may be spent towards selling them sweets. ( See Figure¬†3 )\n\n\n\nFigure¬†3. Relationship Between Meats & Sweets"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-gold-wine",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-gold-wine",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.4 Relationship Between Gold & Wine",
    "text": "5.4 Relationship Between Gold & Wine\nUsing a Gaussian Mixture clustering model trained on normalized company data, customers habits can be established between gold and wine. As seen in Figure¬†4 it seems that customers who purchase less gold products tend to buy more wine but not the other way around. Knowing this, marketing on wine products should not be emphasized for customers that already purchase high amounts of gold products. ( See Figure¬†4 )\n\n\n\nFigure¬†4. Relationship Between Gold & Wine"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#software-video-walkthrough",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#software-video-walkthrough",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "Software Video Walkthrough",
    "text": "Software Video Walkthrough\nVideo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "portfolio",
    "section": "",
    "text": "Deep Learning FastAI Model\n\n\n\n\n\n\n\ndeep learning\n\n\nneural network\n\n\nfastai\n\n\nkaggle\n\n\ncolab\n\n\nvision\n\n\nhyperparameter\n\n\noptimization\n\n\n\n\nProject to train a deep learning model to correctly identify a wasp or a bee using transfer learning with the fastai library.\n\n\n\n\n\n\nJun 9, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nTkinter Machine Learning Prototype Software\n\n\n\n\n\n\n\ndesign\n\n\ndevelopment\n\n\nbusiness\n\n\nproject management\n\n\nmachine learning\n\n\ntkinter\n\n\nsoftware\n\n\nui\n\n\ndatabase\n\n\n\n\nProject to design and develop a prototype software with UI to import, transform and save data to and from a cloud database. Software built to apply machine learning algorithms to explore interesting findings from dataset and use to propose a business case for the design.\n\n\n\n\n\n\nJun 5, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression, Classification, Clustering\n\n\n\n\n\n\n\nlinear regression\n\n\nclassification\n\n\nclustering\n\n\nmachine learning\n\n\n\n\nProject to compare different AI algorithms and an exploration of how to improve their accuracy.\n\n\n\n\n\n\nDec 22, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nHotel Booking System Mock Design\n\n\n\n\n\n\n\ndesign\n\n\ndevelopment\n\n\nmethodologies\n\n\nentity-relationship diagram\n\n\ncontext diagram\n\n\ndata flow diagram\n\n\n\n\nProject to design a hotel booking system based on a mock system design case scenario.\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nModes of Motion Programming Mathematics\n\n\n\n\n\n\n\nmathematics\n\n\nc#\n\n\ngames\n\n\nunity\n\n\nlerp\n\n\n\n\nUnity Games Engine project to demonstrate various methods of movement, making use of custom linear interpolation (lerp) libraries and scripts using Unity physics.\n\n\n\n\n\n\nNov 30, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\nNo matching items"
  }
]