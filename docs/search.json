[
  {
    "objectID": "JuptyerBookFiles/Assignment1BrandonToews.html",
    "href": "JuptyerBookFiles/Assignment1BrandonToews.html",
    "title": "portfolio",
    "section": "",
    "text": "# Import libraries for analysis and plotting\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Save data in Pandas dataframe\ndataset = pd.read_csv(\"CarPrice_Assignment.csv\")\n\n# Print how many rows and columns are in dataset\nprint('Dataset Shape:',dataset.shape)\n\n# Turn of max columns so that head() displays all columns in dataset\npd.set_option('display.max_columns', None)\n\npd.set_option('display.max_rows', 5)\n\n# Display 1st five entries of dataset\ndataset.head()\n\nDataset Shape: (205, 26)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\nsix\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\nfour\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\nfive\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n# Print data types and how many null values are present\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   car_ID            205 non-null    int64  \n 1   symboling         205 non-null    int64  \n 2   CarName           205 non-null    object \n 3   fueltype          205 non-null    object \n 4   aspiration        205 non-null    object \n 5   doornumber        205 non-null    object \n 6   carbody           205 non-null    object \n 7   drivewheel        205 non-null    object \n 8   enginelocation    205 non-null    object \n 9   wheelbase         205 non-null    float64\n 10  carlength         205 non-null    float64\n 11  carwidth          205 non-null    float64\n 12  carheight         205 non-null    float64\n 13  curbweight        205 non-null    int64  \n 14  enginetype        205 non-null    object \n 15  cylindernumber    205 non-null    object \n 16  enginesize        205 non-null    int64  \n 17  fuelsystem        205 non-null    object \n 18  boreratio         205 non-null    float64\n 19  stroke            205 non-null    float64\n 20  compressionratio  205 non-null    float64\n 21  horsepower        205 non-null    int64  \n 22  peakrpm           205 non-null    int64  \n 23  citympg           205 non-null    int64  \n 24  highwaympg        205 non-null    int64  \n 25  price             205 non-null    float64\ndtypes: float64(8), int64(8), object(10)\nmemory usage: 41.8+ KB\n\n\n\n# Display some descriptive statistics\ndataset.describe().round(2)\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\ncount\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n\n\nmean\n103.00\n0.83\n98.76\n174.05\n65.91\n53.72\n2555.57\n126.91\n3.33\n3.26\n10.14\n104.12\n5125.12\n25.22\n30.75\n13276.71\n\n\nstd\n59.32\n1.25\n6.02\n12.34\n2.15\n2.44\n520.68\n41.64\n0.27\n0.31\n3.97\n39.54\n476.99\n6.54\n6.89\n7988.85\n\n\nmin\n1.00\n-2.00\n86.60\n141.10\n60.30\n47.80\n1488.00\n61.00\n2.54\n2.07\n7.00\n48.00\n4150.00\n13.00\n16.00\n5118.00\n\n\n25%\n52.00\n0.00\n94.50\n166.30\n64.10\n52.00\n2145.00\n97.00\n3.15\n3.11\n8.60\n70.00\n4800.00\n19.00\n25.00\n7788.00\n\n\n50%\n103.00\n1.00\n97.00\n173.20\n65.50\n54.10\n2414.00\n120.00\n3.31\n3.29\n9.00\n95.00\n5200.00\n24.00\n30.00\n10295.00\n\n\n75%\n154.00\n2.00\n102.40\n183.10\n66.90\n55.50\n2935.00\n141.00\n3.58\n3.41\n9.40\n116.00\n5500.00\n30.00\n34.00\n16503.00\n\n\nmax\n205.00\n3.00\n120.90\n208.10\n72.30\n59.80\n4066.00\n326.00\n3.94\n4.17\n23.00\n288.00\n6600.00\n49.00\n54.00\n45400.00\n\n\n\n\n\n\n\n\n# Convert object data types to category types\ndataset['CarName'] = dataset['CarName'].astype('category')\ndataset['fueltype'] = dataset['fueltype'].astype('category')\ndataset['aspiration'] = dataset['aspiration'].astype('category')\ndataset['doornumber'] = dataset['doornumber'].astype('category')\ndataset['carbody'] = dataset['carbody'].astype('category')\ndataset['drivewheel'] = dataset['drivewheel'].astype('category')\ndataset['enginelocation'] = dataset['enginelocation'].astype('category')\ndataset['enginetype'] = dataset['enginetype'].astype('category')\ndataset['fuelsystem'] = dataset['fuelsystem'].astype('category')\ndataset['curbweight'] = dataset['curbweight'].astype('int')\n\n# Convert strings to integers in cylindernumber column to potentially use in the regression models\ndataset['cylindernumber'] = dataset['cylindernumber'].replace(['two'], 2).replace(['three'], 3)\\\n.replace(['four'], 4).replace(['five'], 5).replace(['six'], 6).replace(['eight'], 8).replace(['twelve'], 12)\n\n#dataset['fuelsystem'] = dataset['fuelsystem'].replace(['1bbl'], 0).replace(['2bbl'], 1)\\\n#.replace(['4bbl'], 2).replace(['idi'], 3).replace(['mfi'], 4).replace(['mpfi'], 5).replace(['spdi'], 6)\\\n#.replace(['spfi'], 7)\n\n#dataset['enginetype'] = dataset['enginetype'].replace(['dohc'], 0).replace(['dohcv'], 1)\\\n#.replace(['l'], 2).replace(['ohc'], 3).replace(['ohcf'], 4).replace(['ohcv'], 5).replace(['rotor'], 6)\n\ndataset.head()\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\n6\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\n4\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\n5\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n# Print new data types and how many null values are present\ndataset.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   car_ID            205 non-null    int64   \n 1   symboling         205 non-null    int64   \n 2   CarName           205 non-null    category\n 3   fueltype          205 non-null    category\n 4   aspiration        205 non-null    category\n 5   doornumber        205 non-null    category\n 6   carbody           205 non-null    category\n 7   drivewheel        205 non-null    category\n 8   enginelocation    205 non-null    category\n 9   wheelbase         205 non-null    float64 \n 10  carlength         205 non-null    float64 \n 11  carwidth          205 non-null    float64 \n 12  carheight         205 non-null    float64 \n 13  curbweight        205 non-null    int64   \n 14  enginetype        205 non-null    int64   \n 15  cylindernumber    205 non-null    int64   \n 16  enginesize        205 non-null    int64   \n 17  fuelsystem        205 non-null    int64   \n 18  boreratio         205 non-null    float64 \n 19  stroke            205 non-null    float64 \n 20  compressionratio  205 non-null    float64 \n 21  horsepower        205 non-null    int64   \n 22  peakrpm           205 non-null    int64   \n 23  citympg           205 non-null    int64   \n 24  highwaympg        205 non-null    int64   \n 25  price             205 non-null    float64 \ndtypes: category(7), float64(8), int64(11)\nmemory usage: 38.2 KB\n\n\n\n# Display a pairplot to quickly see how varaiables relate to one another with 'fueltype' hue\nsns.pairplot(dataset, kind= 'scatter', hue= 'fueltype')\nplt.show()\n\n\n\n\n\n# Display a pairplot to quickly see how varaiables relate to one another with 'carbody' hue\nsns.pairplot(dataset, kind= 'scatter', hue= 'carbody')\nplt.show()\n\n\n\n\n\n# display pie chart data for carbody\ndataset['carbody'].value_counts().plot.pie(autopct='%1.3f%%')\n\n&lt;AxesSubplot: ylabel='carbody'&gt;\n\n\n\n\n\n\n# Display relationship between body style and price\ndataset.groupby('carbody')['price'].mean().round(2)\n\ncarbody\nconvertible    21890.50\nhardtop        22208.50\nhatchback      10376.65\nsedan          14344.27\nwagon          12371.96\nName: price, dtype: float64\n\n\n\n# display pie chart data for fueltype\ndataset['fueltype'].value_counts().plot.pie(autopct='%1.3f%%')\n\n&lt;AxesSubplot: ylabel='fueltype'&gt;\n\n\n\n\n\n\n# Display ralationship between body style and price\ndataset.groupby('fueltype')['price'].mean().round(2)\n\nfueltype\ndiesel    15838.15\ngas       12999.80\nName: price, dtype: float64\n\n\n\n# Carlength has moderate relationship to price\nplt.figure(figsize=(6,6))\nsns.regplot(data=dataset, x=\"carlength\", y=\"price\")\n\n&lt;AxesSubplot: xlabel='carlength', ylabel='price'&gt;\n\n\n\n\n\n\n# Carwidth has moderate relationship to price\nplt.figure(figsize=(6,6))\nsns.regplot(data=dataset, x=\"carwidth\", y=\"price\")\n\n&lt;AxesSubplot: xlabel='carwidth', ylabel='price'&gt;\n\n\n\n\n\n\n# Carweight has moderate/strong relationship to price\nplt.figure(figsize=(6,6))\nsns.regplot(data=dataset, x=\"curbweight\", y=\"price\")\n\n&lt;AxesSubplot: xlabel='curbweight', ylabel='price'&gt;\n\n\n\n\n\n\n# Engine size has strong relationship to price\nplt.figure(figsize=(6,6))\nsns.lmplot(data=dataset, x=\"enginesize\", y=\"price\", hue='fueltype')\n\n&lt;Figure size 432x432 with 0 Axes&gt;\n\n\n\n\n\n\n# Horsepower has strong relationship to price for both fuel types\nplt.figure(figsize=(6,6))\nsns.lmplot(data=dataset, x=\"horsepower\", y=\"price\", hue='fueltype')\n\n&lt;Figure size 432x432 with 0 Axes&gt;\n\n\n\n\n\n\n# Cylinder number has moderate/strong relationship to price\nsns.jointplot(data=dataset, x=\"cylindernumber\", y=\"price\", kind=\"reg\")\n\n\n\n\n\n# Clear classification relationship between compressionratio and fueltype\ng = sns.jointplot(data=dataset, x=\"compressionratio\", y=\"price\", hue='fueltype')\ng.plot_joint(sns.kdeplot, hue='fueltype')\n\n\n\n\n\n# Import regression training libraries and packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\n# Linear Regression training function that takes in X and Y arguments and displays results\ndef myLinRegModel(x, y, testSize):\n    \n    # While loop to iterate every 10% from given test size\n    while testSize&gt;0:\n        \n        # Splitting data into training and testing variables using the values passed into function\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=(testSize/100), random_state=0)\n\n        # Training model with LinearRegression function and training data\n        regressor = LinearRegression()\n        regressor.fit(x_train, y_train)\n    \n        # Print test size of current iteration\n        print('Test Size:', testSize, '%\\n')\n\n        # Print intercept and CoEfficient values of model\n        print(\"a =\", regressor.intercept_)\n        print(\"b =\", regressor.coef_)\n\n        # Test the trained model with test data and store in variable\n        y_pred = regressor.predict(x_test)\n\n        # Display predicted values next to actual values for comparison\n        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n        print(df)\n    \n        # Display accuracy of model predictions in the form of Mean Absolute Error, Mean Squared Error,\n        # Root Mean Squared Error using the difference between actual and predicted values\n        print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n        print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n        print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n        print('R2 Score: ', metrics.r2_score(y_test,y_pred)*100, '%\\n', sep='')\n        \n        # Decrease test size by 10\n        testSize -= 10\n\n\n# Carlength Column\ncarLength = dataset.iloc[:, 10:-15].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carLength, price, 30)\n\nTest Size: 30 %\n\na = -63541.11342037626\nb = [440.33603117]\n     Actual     Predicted\n0    6795.0   6516.349139\n1   15750.0  19153.993234\n..      ...           ...\n60   6479.0    131.476687\n61  15510.0  18625.589997\n\n[62 rows x 2 columns]\nMean Absolute Error: 3981.584437549869\nMean Squared Error: 32715085.38508641\nRoot Mean Squared Error: 5719.71025359558\nR2 Score: 50.45973947401606%\n\nTest Size: 20 %\n\na = -63738.09854118214\nb = [441.42013341]\n     Actual     Predicted\n0    6795.0   6491.844685\n1   15750.0  19160.602514\n..      ...           ...\n39  45400.0  24192.792035\n40   8916.5   5079.300258\n\n[41 rows x 2 columns]\nMean Absolute Error: 4528.295484564718\nMean Squared Error: 43469954.12056989\nRoot Mean Squared Error: 6593.174813439266\nR2 Score: 43.84913586892223%\n\nTest Size: 10 %\n\na = -66742.8631493445\nb = [459.19742348]\n     Actual     Predicted\n0    6795.0   6315.446927\n1   15750.0  19494.412981\n..      ...           ...\n19   6488.0   6131.767957\n20   9959.0  12698.291113\n\n[21 rows x 2 columns]\nMean Absolute Error: 3945.9317457788898\nMean Squared Error: 29396652.85426334\nRoot Mean Squared Error: 5421.868022578873\nR2 Score: 26.410928631260454%\n\n\n\n\n# Carwidth Column\ncarWidth = dataset.iloc[:, 11:-14].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWidth, price, 30)\n\nTest Size: 30 %\n\na = -172630.60948546475\nb = [2822.14912394]\n     Actual     Predicted\n0    6795.0   8551.364271\n1   15750.0  15042.307256\n..      ...           ...\n60   6479.0   7704.719534\n61  15510.0  15042.307256\n\n[62 rows x 2 columns]\nMean Absolute Error: 3036.57768015824\nMean Squared Error: 22710512.087679498\nRoot Mean Squared Error: 4765.554751304354\nR2 Score: 65.60960571372881%\n\nTest Size: 20 %\n\na = -172526.22359994025\nb = [2819.03318321]\n     Actual     Predicted\n0    6795.0   8455.706762\n1   15750.0  14939.483084\n..      ...           ...\n39  45400.0  30444.165591\n40   8916.5   6764.286852\n\n[41 rows x 2 columns]\nMean Absolute Error: 3674.9155902799166\nMean Squared Error: 31370813.470780104\nRoot Mean Squared Error: 5600.965405247573\nR2 Score: 59.47779746918066%\n\nTest Size: 10 %\n\na = -181627.87173597398\nb = [2957.89666431]\n     Actual     Predicted\n0    6795.0   8269.094113\n1   15750.0  15072.256441\n..      ...           ...\n19   6488.0   6494.356114\n20   9959.0  11818.570110\n\n[21 rows x 2 columns]\nMean Absolute Error: 3197.214272696799\nMean Squared Error: 19783555.22362383\nRoot Mean Squared Error: 4447.87086409035\nR2 Score: 50.47553663690271%\n\n\n\n\n# Curbweight Column\ncarWeight = dataset.iloc[:, 13:-12].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWeight, price, 30)\n\nTest Size: 30 %\n\na = -18679.037713196016\nb = [12.40359272]\n     Actual     Predicted\n0    6795.0   4949.806413\n1   15750.0  20404.682939\n..      ...           ...\n60   6479.0   2568.316611\n61  15510.0  15530.071001\n\n[62 rows x 2 columns]\nMean Absolute Error: 2670.404540077829\nMean Squared Error: 18443910.151758883\nRoot Mean Squared Error: 4294.637371392244\nR2 Score: 72.0704958192619%\n\nTest Size: 20 %\n\na = -18833.605447325583\nb = [12.47623193]\n     Actual     Predicted\n0    6795.0   4933.616372\n1   15750.0  20479.001353\n..      ...           ...\n39  45400.0  27515.596159\n40   8916.5   4546.853183\n\n[41 rows x 2 columns]\nMean Absolute Error: 3256.3206631106873\nMean Squared Error: 25249391.034916148\nRoot Mean Squared Error: 5024.877215904499\nR2 Score: 67.3849408384091%\n\nTest Size: 10 %\n\na = -19880.405624111718\nb = [12.9537027]\n     Actual     Predicted\n0    6795.0   4796.398026\n1   15750.0  20936.711595\n..      ...           ...\n19   6488.0   6221.305324\n20   9959.0  10819.869783\n\n[21 rows x 2 columns]\nMean Absolute Error: 2695.197926817389\nMean Squared Error: 11737364.677960433\nRoot Mean Squared Error: 3425.9837533123873\nR2 Score: 70.61768320191304%\n\n\n\n\n# Cylinder Number Column\ncylinderNumber = dataset.iloc[:, 15:-10].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(cylinderNumber, price, 30)\n\nTest Size: 30 %\n\na = -8750.74345729567\nb = [5045.13677503]\n     Actual     Predicted\n0    6795.0  11429.803643\n1   15750.0  21520.077193\n..      ...           ...\n60   6479.0  11429.803643\n61  15510.0  11429.803643\n\n[62 rows x 2 columns]\nMean Absolute Error: 3944.3868255082953\nMean Squared Error: 26684225.038138304\nRoot Mean Squared Error: 5165.677597192676\nR2 Score: 59.59223566856468%\n\nTest Size: 20 %\n\na = -9046.162097201766\nb = [5112.35112126]\n     Actual     Predicted\n0    6795.0  11403.242388\n1   15750.0  21627.944630\n..      ...           ...\n39  45400.0  31852.646873\n40   8916.5  11403.242388\n\n[41 rows x 2 columns]\nMean Absolute Error: 4280.5628888250285\nMean Squared Error: 32605207.611888204\nRoot Mean Squared Error: 5710.096987958103\nR2 Score: 57.88330998687709%\n\nTest Size: 10 %\n\na = -10564.254121382277\nb = [5479.84028365]\n     Actual     Predicted\n0    6795.0  11355.107013\n1   15750.0  22314.787580\n..      ...           ...\n19   6488.0  11355.107013\n20   9959.0  11355.107013\n\n[21 rows x 2 columns]\nMean Absolute Error: 3464.088015146232\nMean Squared Error: 18346836.560473613\nRoot Mean Squared Error: 4283.32073985519\nR2 Score: 54.072095495610604%\n\n\n\n\n# Engine Size Column\nengineSize = dataset['enginesize'].values.reshape(-1, 1)\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(engineSize, price, 30)\n\nTest Size: 30 %\n\na = -7574.131488222356\nb = [163.29075344]\n     Actual     Predicted\n0    6795.0   7285.327074\n1   15750.0  18715.679815\n..      ...           ...\n60   6479.0   7448.617828\n61  15510.0  12184.049678\n\n[62 rows x 2 columns]\nMean Absolute Error: 2898.9726929694702\nMean Squared Error: 14541824.65222288\nRoot Mean Squared Error: 3813.374444271488\nR2 Score: 77.97940083865093%\n\nTest Size: 20 %\n\na = -7613.370926304753\nb = [164.31545176]\n     Actual     Predicted\n0    6795.0   7339.335184\n1   15750.0  18841.416808\n..      ...           ...\n39  45400.0  42338.526410\n40   8916.5   7175.019732\n\n[41 rows x 2 columns]\nMean Absolute Error: 3195.031241401546\nMean Squared Error: 16835544.028987687\nRoot Mean Squared Error: 4103.113942969131\nR2 Score: 78.25324722629195%\n\nTest Size: 10 %\n\na = -8207.420855494747\nb = [169.490971]\n     Actual     Predicted\n0    6795.0   7216.257505\n1   15750.0  19080.625475\n..      ...           ...\n19   6488.0   7385.748476\n20   9959.0  10436.585954\n\n[21 rows x 2 columns]\nMean Absolute Error: 2877.111549011615\nMean Squared Error: 12997474.409783443\nRoot Mean Squared Error: 3605.2010221045157\nR2 Score: 67.46323206602058%\n\n\n\n\n# Horsepower Column\nhorsepower = dataset.iloc[:, 21:-4].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(horsepower, price, 30)\n\nTest Size: 30 %\n\na = -4438.686268723588\nb = [170.53827527]\n     Actual     Predicted\n0    6795.0   7157.916450\n1   15750.0  22165.284674\n..      ...           ...\n60   6479.0   5452.533697\n61  15510.0  14320.524011\n\n[62 rows x 2 columns]\nMean Absolute Error: 3518.2488303322393\nMean Squared Error: 25821021.51495541\nRoot Mean Squared Error: 5081.438921698795\nR2 Score: 60.89937966412712%\n\nTest Size: 20 %\n\na = -4053.153036276188\nb = [166.64923709]\n     Actual     Predicted\n0    6795.0   7278.995086\n1   15750.0  21944.127950\n..      ...           ...\n39  45400.0  26610.306588\n40   8916.5   7612.293560\n\n[41 rows x 2 columns]\nMean Absolute Error: 3733.6933754512147\nMean Squared Error: 29626244.692692798\nRoot Mean Squared Error: 5442.999604325983\nR2 Score: 61.73128603174041%\n\nTest Size: 10 %\n\na = -4796.241165629246\nb = [174.95075436]\n     Actual     Predicted\n0    6795.0   7100.410131\n1   15750.0  22496.076514\n..      ...           ...\n19   6488.0   6050.705605\n20   9959.0  15498.046340\n\n[21 rows x 2 columns]\nMean Absolute Error: 3839.1982159225827\nMean Squared Error: 26172943.363739382\nRoot Mean Squared Error: 5115.949898478227\nR2 Score: 34.48088778430891%\n\n\n\n\n# Create copy of dataset and drop all columns not used for multivariate regression models\ndatasetCopy = dataset\ndatasetCopy.drop(['carheight', 'enginetype', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio'],\\\ninplace=True, axis=1)\n\n# Store carlength, carwidth & curbweight columns in X\nX1 = datasetCopy.iloc[:, 10:-7].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X1, price, 30)\n\nTest Size: 30 %\n\na = -36739.21951780665\nb = [-208.48890057  764.98885537   13.97180015]\n     Actual     Predicted\n0    6795.0   5818.760203\n1   15750.0  19003.466111\n..      ...           ...\n60   6479.0   5929.766976\n61  15510.0  13762.735333\n\n[62 rows x 2 columns]\nMean Absolute Error: 2458.5442776902337\nMean Squared Error: 16492573.815910544\nRoot Mean Squared Error: 4061.1049993703123\nR2 Score: 75.02539290462342%\n\nTest Size: 20 %\n\na = -44634.67127094674\nb = [-188.42001434  856.204069     13.34802623]\n     Actual     Predicted\n0    6795.0   5783.995638\n1   15750.0  18977.251262\n..      ...           ...\n39  45400.0  29066.672270\n40   8916.5   5459.428429\n\n[41 rows x 2 columns]\nMean Absolute Error: 2943.0381053387778\nMean Squared Error: 22423198.502769\nRoot Mean Squared Error: 4735.313981434494\nR2 Score: 71.03558082852051%\n\nTest Size: 10 %\n\na = -51021.53652492876\nb = [-194.19115484  961.28023332   13.59661598]\n     Actual     Predicted\n0    6795.0   5698.395155\n1   15750.0  19277.437054\n..      ...           ...\n19   6488.0   6694.931234\n20   9959.0  10475.100811\n\n[21 rows x 2 columns]\nMean Absolute Error: 2357.566870487648\nMean Squared Error: 9101964.422986511\nRoot Mean Squared Error: 3016.9462081691995\nR2 Score: 77.21491923452972%\n\n\n\n\n# Store cylindernumber, enginesize & horsepower columns in X\nX2 = datasetCopy.iloc[:, 13:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X2, price, 30)\n\nTest Size: 30 %\n\na = -6717.131795698624\nb = [-875.82889691  133.38667558   65.31455209]\n     Actual     Predicted\n0    6795.0   6359.129636\n1   15750.0  19692.219717\n..      ...           ...\n60   6479.0   5839.370791\n61  15510.0  13103.941091\n\n[62 rows x 2 columns]\nMean Absolute Error: 2681.2430726638395\nMean Squared Error: 13246002.119140355\nRoot Mean Squared Error: 3639.505752041114\nR2 Score: 79.941657245098%\n\nTest Size: 20 %\n\na = -7307.824968281864\nb = [-522.31275964  128.16628088   63.17240963]\n     Actual     Predicted\n0    6795.0   6561.779408\n1   15750.0  20047.965597\n..      ...           ...\n39  45400.0  39099.945713\n40   8916.5   6559.957946\n\n[41 rows x 2 columns]\nMean Absolute Error: 3028.44528450474\nMean Squared Error: 15255724.671464592\nRoot Mean Squared Error: 3905.8577382522003\nR2 Score: 80.29392621688581%\n\nTest Size: 10 %\n\na = -7824.384102535303\nb = [-613.42877141  135.45474355   63.82356299]\n     Actual     Predicted\n0    6795.0   6388.284759\n1   15750.0  20259.732808\n..      ...           ...\n19   6488.0   6140.798124\n20   9959.0  12025.455910\n\n[21 rows x 2 columns]\nMean Absolute Error: 2944.3040595022885\nMean Squared Error: 12712894.325743863\nRoot Mean Squared Error: 3565.5145948016907\nR2 Score: 68.17562555579416%\n\n\n\n\n# Store carlength, carwidth, curbweight, cylindernumber,\n# enginesize & horsepower columns in X\nX3 = datasetCopy.iloc[:, 10:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X3, price, 30)\n\nTest Size: 30 %\n\na = -50133.27454870472\nb = [-62.20404054 772.47835707   3.18527494  18.99449375  65.77507898\n  64.33402142]\n     Actual     Predicted\n0    6795.0   6067.345502\n1   15750.0  20331.280734\n..      ...           ...\n60   6479.0   5548.422659\n61  15510.0  13525.755400\n\n[62 rows x 2 columns]\nMean Absolute Error: 2536.7293535638096\nMean Squared Error: 12555624.008739235\nRoot Mean Squared Error: 3543.39159686581\nR2 Score: 80.98709273909488%\n\nTest Size: 20 %\n\na = -54793.72590677004\nb = [-38.92156396 789.71920542   2.95208156 366.09875078  59.3369646\n  58.3182745 ]\n     Actual     Predicted\n0    6795.0   6167.243070\n1   15750.0  20562.635157\n..      ...           ...\n39  45400.0  36977.654082\n40   8916.5   5783.745608\n\n[41 rows x 2 columns]\nMean Absolute Error: 2873.7239149228203\nMean Squared Error: 16025434.859390952\nRoot Mean Squared Error: 4003.178094887979\nR2 Score: 79.29967874050975%\n\nTest Size: 10 %\n\na = -55531.82635570387\nb = [-30.01857827 784.53201154   2.29960226 213.59515415  74.83936968\n  58.2469381 ]\n     Actual     Predicted\n0    6795.0   6065.470340\n1   15750.0  20665.341927\n..      ...           ...\n19   6488.0   5585.072554\n20   9959.0  11876.766620\n\n[21 rows x 2 columns]\nMean Absolute Error: 3020.6881171033187\nMean Squared Error: 13605511.765351668\nRoot Mean Squared Error: 3688.5650008304947\nR2 Score: 65.94112325398689%\n\n\n\n\n# Import classification training libraries and packages\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Packages for displaying classification accuracy\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nnp.set_printoptions(suppress=True)\n\n# Classification training function that takes in X values to classify according to Y values\n# and takes what scalar should be used\ndef myClassModel(X, y, scale):\n    \n    # Split dataset into random train and test subsets:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) \n\n    # Standardizes data if specified when calling function\n    if scale == 'Standardize':\n    \n        # Standardize features by removing mean and scaling to unit variance:\n        scaler = StandardScaler()\n        scaler.fit(X_train)\n        X_train = scaler.transform(X_train)\n        X_test = scaler.transform(X_test)\n    \n    # Normalizes data if specified when calling function\n    elif scale == \"Normalize\":\n    \n        # Normalize features by shrinking data range between 0 & 1:\n        scaler = MinMaxScaler()\n        scaler.fit(X_train)\n\n        X_train = scaler.transform(X_train)\n        X_test = scaler.transform(X_test)\n\n    # Use the KNN classifier to fit data:\n    knclassifier = KNeighborsClassifier(n_neighbors=5)\n    knclassifier.fit(X_train, y_train) \n\n    # Predict y data with KNN classifier: \n    y_predict = knclassifier.predict(X_test)\n\n    # Print KNN classifier results:\n    print(\"KNeigbors Classifier - Scaling:\", scale)\n    cm = confusion_matrix(y_test, y_predict, labels=knclassifier.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knclassifier.classes_)\n    disp.plot()\n    plt.show()\n    print(classification_report(y_test, y_predict))\n    \n     # Use the Decision Tree classifier to fit data:\n    dtclassifier = DecisionTreeClassifier()\n    dtclassifier.fit(X_train, y_train) \n\n    # Predict y data with Decision Tree classifier: \n    y_predict = dtclassifier.predict(X_test)\n\n    # Print Decision Tree classifier results:\n    print(\"Decision Tree Classifier - Scaling:\", scale)\n    cm = confusion_matrix(y_test, y_predict, labels=dtclassifier.classes_)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dtclassifier.classes_)\n    disp.plot()\n    plt.show()\n    print(classification_report(y_test, y_predict))\n\n\npd.set_option('display.max_rows', None)\n\n\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'None')\n\nKNeigbors Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       0.00      0.00      0.00         6\n         gas       0.85      0.94      0.89        35\n\n    accuracy                           0.80        41\n   macro avg       0.42      0.47      0.45        41\nweighted avg       0.72      0.80      0.76        41\n\nDecision Tree Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         6\n         gas       1.00      1.00      1.00        35\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n\n\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Standardize')\n\nKNeigbors Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n\n\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Normalize')\n\nKNeigbors Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n\n\n# Import clustering packages\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import SpectralClustering\n\n# Cluster training function that takes in X values to cluster, along with\n# what model should be used and how many clusters should be created\ndef myClusterModel(X, model, num_clusters):\n    \n    # Store columns names of features\n    column_name = list(X.columns)\n    # Stores feature values fro use in some models\n    features = X.values\n    \n    # Takes given features and creates dataframe for some models\n    X = pd.DataFrame(X)\n    \n    # Normalize features\n    scaler = MinMaxScaler()\n    scaler.fit(features)\n    scaled = scaler.transform(features)\n    \n    # For KMeans model\n    if model=='KM':\n        # Initialize KMeans model with given number of clusters\n        kmeans = KMeans(n_clusters=num_clusters)\n        \n        # Produce clusters with model and append cluster label info to DataFrame X\n        X['cluster'] = kmeans.fit_predict(features)\n        \n        # Set plot size\n        plt.figure(figsize=(6, 6))\n        # Plot data with given features\n        plt.scatter(X[column_name[0]], X[column_name[1]])\n\n        # Plot KMeans cluster centers\n        plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n        # Display plot\n        plt.xlabel(column_name[0])\n        plt.ylabel(column_name[1])\n        plt.xlim(0,31)\n        plt.ylim(0,55000)\n        plt.title('KMeans Cluster Model - No Scaling')\n        plt.show()\n\n        # Display scatter plot with KDE to see compare how well\n        # model performed at creating relevant clusters\n        g = sns.jointplot(data=X, x=column_name[0], y=column_name[1], hue='cluster')\n        g.fig.suptitle(\"KMeans Cluster Model - No Scaling\")\n        g.plot_joint(sns.kdeplot, levels=3)\n            \n        # Delete cluster column so we can add scaled cluster labels to plot \n        X.drop('cluster', inplace=True, axis=1)\n        \n        # Appends new scaled cluster label info to DataFrame X\n        X['cluster'] = kmeans.fit_predict(scaled)\n            \n        # Display scatter plot with KDE to see compare how well\n        # model performed at creating relevant clusters with scaled data\n        g = sns.jointplot(data=X, x=column_name[0], y=column_name[1], hue='cluster', xlim=(0,31))\n        g.fig.suptitle(\"KMeans Cluster Model - Normalized Features\")\n        g.plot_joint(sns.kdeplot, levels=num_clusters)\n            \n    # For Gaussian Mixture model    \n    elif model=='GMM':\n        # Initialize Gaussian Mixture with given number of clusters\n        gmm_model = GaussianMixture(n_components=num_clusters)\n        gmm_model.fit(features)\n\n        # Produce clusters with model and append cluster label info to DataFrame X\n        X['cluster'] = gmm_model.predict(features)\n\n        # Display scatter plot with KDE to see compare how well\n        # model performed at creating relevant clusters\n        g = sns.jointplot(data=X, x='compressionratio', y='price', hue=\"cluster\")\n        g.fig.suptitle(\"Gaussian Mixture Model - No Scaling\")\n        g.plot_joint(sns.kdeplot, levels=num_clusters, common_norm=False)\n        \n        # Feed scaled data into model\n        gmm_model.fit(scaled)\n \n        # Delete cluster column so we can add scaled cluster labels to plot\n        X.drop('cluster', inplace=True, axis=1)\n    \n        # Appends new scaled cluster label info to DataFrame X\n        X['cluster'] = gmm_model.predict(scaled)\n\n        # Display scatter plot with KDE to see compare how well\n        # model performed at creating relevant clusters with scaled data\n        g = sns.jointplot(data=X, x='compressionratio', y='price', hue='cluster', xlim=(0,31))\n        g.fig.suptitle(\"Gaussian Mixture Model - Normalized Features\")\n        g.plot_joint(sns.kdeplot, levels=num_clusters)\n    \n    elif model=='SC':\n        # Initialize KMeans model with given number of clusters\n        sc = SpectralClustering(n_clusters=num_clusters, random_state=25, n_neighbors=25,\\\n        affinity='nearest_neighbors')\n        \n        # Appends cluster label info to DataFrame X\n        X['cluster'] = sc.fit_predict(X[[column_name[0], column_name[1]]])\n        \n        # Display scatter plot with KDE to see compare how well\n        # model performed at creating relevant clusters\n        g = sns.jointplot(data=X, x='compressionratio', y='price', hue=\"cluster\")\n        g.fig.suptitle(\"Spectral Clustering Model - No Scaling\")\n        g.plot_joint(sns.kdeplot, levels=num_clusters, common_norm=False)\n        \n        # Delete cluster column so we can add scaled cluster labels to plot\n        X.drop('cluster', inplace=True, axis=1)\n        \n        # convert scaled values to dataframe to be used by model\n        scaled = pd.DataFrame(scaled)\n        \n        # Appends new scaled cluster label info to DataFrame X\n        X['cluster'] = sc.fit_predict(scaled[[0, 1]])\n        \n        # Display scatter plot with KDE to see compare how well\n        # model performed at creating relevant clusters with scaled data\n        g = sns.jointplot(data=X, x='compressionratio', y='price', hue=\"cluster\", xlim=(0,31))\n        g.fig.suptitle(\"Spectral Clustering Model - Normalized Features\")\n        g.plot_joint(sns.kdeplot, levels=num_clusters, common_norm=False)\n\n\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# KMeans Cluster Model to be used\nmodel = 'KM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n\n\n\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Gaussian Mixture Model to be used\nmodel = 'GMM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Spectral Clustering Model to be used\nmodel = 'SC'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "portfolio",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nIntro to AI - Linear Regression, Classification, Clustering\n\n\n\n\n\n\n\nlinear regression\n\n\nclassification\n\n\nclustering\n\n\n\n\nProject to compare different AI models and an exploration of how to improve their accuracy.\n\n\n\n\n\n\nDec 22, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#purpose-of-document",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#purpose-of-document",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide a comparison between different AI models for a given dataset to determine which models are most accurate. This document also explores what measures can be taken to improve accuracy in various AI models."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#scope",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#scope",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an exploratory examination of a dataset to determine how best to sample and clean the data for AI training and testing purposes. Various data visualizations are needed to properly understand the dataset and how best to proceed with training models. Training of various models and algorithms are required to produce sufficient comparisons with the ultimate goal of improving accuracy."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#descriptive-analysis",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#descriptive-analysis",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "2.1 Descriptive Analysis",
    "text": "2.1 Descriptive Analysis\nThe dataset used in this project consists of available independent variables for a variety of cars to ascertain how they affect the price. The chosen dataset contains 26 columns and 205 rows of data with no null values. It is a sufficient dataset in terms of size and types of data for use in training univariate & multivariate linear regression, classification and clustering models.\nThe Columns\n\nCar_ID : Unique id of each observation (Integer)\nSymboling : Its assigned insurance risk rating, A value of +3 - Indicates that the auto is risky, -3 that it is probably pretty safe.\ncarCompany : Name of car company (Categorical)\nfueltype : Car fuel type i.e gas or diesel (Categorical)\naspiration : Aspiration used in a car (Categorical)\ndoornumber : Number of doors in a car (Categorical)\ncarbody : Body of car (Categorical)\ndrivewheel : Type of drive wheel (Categorical)\nenginelocation : Location of car engine (Categorical)\nwheelbase : Wheelbase of car (Numeric)\ncarlength : Length of car (Numeric)\ncarwidth : Width of car (Numeric)\ncarheight : Height of car (Numeric)\ncurbweight : The weight of a car without occupants or baggage. (Numeric)\nenginetype : Type of engine. (Categorical)\ncylindernumber : Cylinder placed in the car (Numeric)\nenginesize : Size of car (Numeric)\nfuelsystem : Fuel system of car (Categorical)\nboreratio : Boreratio of car (Numeric)\nstroke : Stroke or volume inside the engine (Numeric)\ncompressionratio : Compression ratio of car (Numeric)\nhorsepower : Horsepower (Numeric)\npeakrpm : Car peak rpm (Numeric)\ncitympg : Mileage in city (Numeric)\nhighwaympg : Mileage on highway (Numeric)\nprice(Dependent variable) : Price of car (Numeric)\n\n\n\nCode\n# Import libraries for analysis and plotting\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Save data in Pandas dataframe\ndataset = pd.read_csv(\"CarPrice_Assignment.csv\")\n\n# Print how many rows and columns are in dataset\nprint('Dataset Shape:',dataset.shape)\n\n# Turn of max columns so that head() displays all columns in dataset\npd.set_option('display.max_columns', None)\n\npd.set_option('display.max_rows', 5)\n\n# Display 1st five entries of dataset\ndataset.head()\n\n\nDataset Shape: (205, 26)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\nsix\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\nfour\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\nfive\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n\nCode\n# Print data types and how many null values are present\ndataset.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   car_ID            205 non-null    int64  \n 1   symboling         205 non-null    int64  \n 2   CarName           205 non-null    object \n 3   fueltype          205 non-null    object \n 4   aspiration        205 non-null    object \n 5   doornumber        205 non-null    object \n 6   carbody           205 non-null    object \n 7   drivewheel        205 non-null    object \n 8   enginelocation    205 non-null    object \n 9   wheelbase         205 non-null    float64\n 10  carlength         205 non-null    float64\n 11  carwidth          205 non-null    float64\n 12  carheight         205 non-null    float64\n 13  curbweight        205 non-null    int64  \n 14  enginetype        205 non-null    object \n 15  cylindernumber    205 non-null    object \n 16  enginesize        205 non-null    int64  \n 17  fuelsystem        205 non-null    object \n 18  boreratio         205 non-null    float64\n 19  stroke            205 non-null    float64\n 20  compressionratio  205 non-null    float64\n 21  horsepower        205 non-null    int64  \n 22  peakrpm           205 non-null    int64  \n 23  citympg           205 non-null    int64  \n 24  highwaympg        205 non-null    int64  \n 25  price             205 non-null    float64\ndtypes: float64(8), int64(8), object(10)\nmemory usage: 41.8+ KB\n\n\n\n\nCode\n# Display some descriptive statistics\ndataset.describe().round(2)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\ncount\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n\n\nmean\n103.00\n0.83\n98.76\n174.05\n65.91\n53.72\n2555.57\n126.91\n3.33\n3.26\n10.14\n104.12\n5125.12\n25.22\n30.75\n13276.71\n\n\nstd\n59.32\n1.25\n6.02\n12.34\n2.15\n2.44\n520.68\n41.64\n0.27\n0.31\n3.97\n39.54\n476.99\n6.54\n6.89\n7988.85\n\n\nmin\n1.00\n-2.00\n86.60\n141.10\n60.30\n47.80\n1488.00\n61.00\n2.54\n2.07\n7.00\n48.00\n4150.00\n13.00\n16.00\n5118.00\n\n\n25%\n52.00\n0.00\n94.50\n166.30\n64.10\n52.00\n2145.00\n97.00\n3.15\n3.11\n8.60\n70.00\n4800.00\n19.00\n25.00\n7788.00\n\n\n50%\n103.00\n1.00\n97.00\n173.20\n65.50\n54.10\n2414.00\n120.00\n3.31\n3.29\n9.00\n95.00\n5200.00\n24.00\n30.00\n10295.00\n\n\n75%\n154.00\n2.00\n102.40\n183.10\n66.90\n55.50\n2935.00\n141.00\n3.58\n3.41\n9.40\n116.00\n5500.00\n30.00\n34.00\n16503.00\n\n\nmax\n205.00\n3.00\n120.90\n208.10\n72.30\n59.80\n4066.00\n326.00\n3.94\n4.17\n23.00\n288.00\n6600.00\n49.00\n54.00\n45400.00"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#cleaning",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#cleaning",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "2.2 Cleaning",
    "text": "2.2 Cleaning\nMultiple columns are object data types but for classification and clustering purposes they were converted to category types. Column 16, “cylindernumber”, values were changed from strings to integers to assist in training some of the linear regression models.\n\n\nCode\n# Convert object data types to category types\ndataset['CarName'] = dataset['CarName'].astype('category')\ndataset['fueltype'] = dataset['fueltype'].astype('category')\ndataset['aspiration'] = dataset['aspiration'].astype('category')\ndataset['doornumber'] = dataset['doornumber'].astype('category')\ndataset['carbody'] = dataset['carbody'].astype('category')\ndataset['drivewheel'] = dataset['drivewheel'].astype('category')\ndataset['enginelocation'] = dataset['enginelocation'].astype('category')\ndataset['enginetype'] = dataset['enginetype'].astype('category')\ndataset['fuelsystem'] = dataset['fuelsystem'].astype('category')\ndataset['curbweight'] = dataset['curbweight'].astype('int')\n\n# Convert strings to integers in cylindernumber column to potentially use in the regression models\ndataset['cylindernumber'] = dataset['cylindernumber'].replace(['two'], 2).replace(['three'], 3)\\\n.replace(['four'], 4).replace(['five'], 5).replace(['six'], 6).replace(['eight'], 8).replace(['twelve'], 12)\n\ndataset.head()\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\n6\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\n4\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\n5\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n\nCode\n# Print new data types and how many null values are present\ndataset.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   car_ID            205 non-null    int64   \n 1   symboling         205 non-null    int64   \n 2   CarName           205 non-null    category\n 3   fueltype          205 non-null    category\n 4   aspiration        205 non-null    category\n 5   doornumber        205 non-null    category\n 6   carbody           205 non-null    category\n 7   drivewheel        205 non-null    category\n 8   enginelocation    205 non-null    category\n 9   wheelbase         205 non-null    float64 \n 10  carlength         205 non-null    float64 \n 11  carwidth          205 non-null    float64 \n 12  carheight         205 non-null    float64 \n 13  curbweight        205 non-null    int64   \n 14  enginetype        205 non-null    category\n 15  cylindernumber    205 non-null    int64   \n 16  enginesize        205 non-null    int64   \n 17  fuelsystem        205 non-null    category\n 18  boreratio         205 non-null    float64 \n 19  stroke            205 non-null    float64 \n 20  compressionratio  205 non-null    float64 \n 21  horsepower        205 non-null    int64   \n 22  peakrpm           205 non-null    int64   \n 23  citympg           205 non-null    int64   \n 24  highwaympg        205 non-null    int64   \n 25  price             205 non-null    float64 \ndtypes: category(9), float64(8), int64(9)\nmemory usage: 36.1 KB"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#visualizations",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#visualizations",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nA pairplot ( See Figure 1 ) provides a quick overview of how the variables relate, showing some possibilities for training models. The ‘carbody’ ( See Figure 2 ) and ‘fueltype’ ( See Figure 3 ) columns show promise for use with the classification and clustering models ( See Figure 4 (f) & Figure 4 (g) ). Clear linear relationships exist between ‘carlength’, ‘carwidth’, ‘curbweight’, ‘enginesize’, ‘cylindernumber’ and ‘horsepower’ independent variables and the dependent variable ‘price’ ( See Figure 4 (a) - (f) ).\n\n\nCode\n# Display a pairplot to quickly see how varaiables relate to one another with 'fueltype' hue\nsns.pairplot(dataset, kind= 'scatter', hue= 'fueltype')\nplt.show()\n\n\n\n\n\nFigure 1. Pairplot with Fuel Type Hue\n\n\n\n\n\n\nCode\n# display pie chart data for carbody\ndataset['carbody'].value_counts().plot.pie(autopct='%1.3f%%');\n\n# Display relationship between body style and price\ndataset.groupby('carbody')['price'].mean().round(2)\n\n\n\n\n\nFigure 2. Carbody Pie Plot\n\n\n\n\n\n\ncarbody\nconvertible    21890.50\nhardtop        22208.50\nhatchback      10376.65\nsedan          14344.27\nwagon          12371.96\nName: price, dtype: float64\n\n\n\n\nCode\n# display pie chart data for fueltype\ndataset['fueltype'].value_counts().plot.pie(autopct='%1.3f%%');\n\n# Display ralationship between body style and price\ndataset.groupby('fueltype')['price'].mean().round(2)\n\n\n\n\n\nFigure 3. Fuel Type Pie Plot\n\n\n\n\n\n\nfueltype\ndiesel    15838.15\ngas       12999.80\nName: price, dtype: float64\n\n\n\n\nCode\n# Carlength has moderate relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"carlength\", y=\"price\");\n\n# Carwidth has moderate relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"carwidth\", y=\"price\");\n\n# Carweight has moderate/strong relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"curbweight\", y=\"price\");\n\n# Engine size has strong relationship to price\nsns.lmplot(data=dataset, x=\"enginesize\", y=\"price\", hue='fueltype', height=6)\n\n# Horsepower has strong relationship to price for both fuel types\nsns.lmplot(data=dataset, x=\"horsepower\", y=\"price\", hue='fueltype', height=6)\n\n# Cylinder number has moderate/strong relationship to price\nsns.jointplot(data=dataset, x=\"cylindernumber\", y=\"price\", kind=\"reg\");\n\n# Clear classification relationship between compressionratio and fueltype\ng = sns.jointplot(data=dataset, x=\"compressionratio\", y=\"price\", hue='fueltype');\ng.plot_joint(sns.kdeplot, hue='fueltype');\n\n\n\n\n\n\n\n\n(a) Car Length vs Price\n\n\n\n\n\n\n\n(b) Car Width vs Price\n\n\n\n\n\n\n\n\n\n(c) Curb Weight vs Price\n\n\n\n\n\n\n\n(d) Engine Size vs Price\n\n\n\n\n\n\n\n\n\n(e) Horsepower vs Price\n\n\n\n\n\n\n\n(f) Cylinder Number vs Price\n\n\n\n\n\n\n\n\n\n(g) Compression Ratio vs Price\n\n\n\n\nFigure 4. Plots for visualizing"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#univariate-models",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#univariate-models",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "3.1 Univariate Models",
    "text": "3.1 Univariate Models\nMultiple univariate models were trained for comparison using a custom Linear Regression training function. Models were trained with ‘carlength’, ‘carwidth’, ‘curbweight’, ‘cylindernumber’, ‘enginesize’ and ‘horsepower’ independent variables. In most cases the model accuracy was the best with a 70% training and 30% testing split ( See Items 3.1.1 - 3.1.4 ). However, with engine size and horsepower models more accuracy was achieved with an 80% training and 20% testing split ( See Items 3.1.5 - 3.1.6 ).\n\n\nCode\n# Import regression training libraries and packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\n# Linear Regression training function that takes in X and Y arguments and displays results\ndef myLinRegModel(x, y, testSize):\n    \n    # While loop to iterate every 10% from given test size\n    while testSize&gt;0:\n        \n        # Splitting data into training and testing variables using the values passed into function\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=(testSize/100), random_state=0)\n\n        # Training model with LinearRegression function and training data\n        regressor = LinearRegression()\n        regressor.fit(x_train, y_train)\n    \n        # Print test size of current iteration\n        print('Test Size:', testSize, '%\\n')\n\n        # Print intercept and CoEfficient values of model\n        print(\"a =\", regressor.intercept_)\n        print(\"b =\", regressor.coef_)\n\n        # Test the trained model with test data and store in variable\n        y_pred = regressor.predict(x_test)\n\n        # Display predicted values next to actual values for comparison\n        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n        print(df)\n    \n        # Display accuracy of model predictions in the form of Mean Absolute Error, Mean Squared Error,\n        # Root Mean Squared Error using the difference between actual and predicted values\n        print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n        print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n        print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n        print('R2 Score: ', metrics.r2_score(y_test,y_pred)*100, '%\\n', sep='')\n        \n        # Decrease test size by 10\n        testSize -= 10\n\n\n\n3.1.1 Car Length vs Price\n\n\nCode\n# Carlength Column\ncarLength = dataset.iloc[:, 10:-15].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carLength, price, 30)\n\n\nTest Size: 30 %\n\na = -63541.11342037626\nb = [440.33603117]\n     Actual     Predicted\n0    6795.0   6516.349139\n1   15750.0  19153.993234\n..      ...           ...\n60   6479.0    131.476687\n61  15510.0  18625.589997\n\n[62 rows x 2 columns]\nMean Absolute Error: 3981.584437549869\nMean Squared Error: 32715085.38508641\nRoot Mean Squared Error: 5719.71025359558\nR2 Score: 50.45973947401606%\n\nTest Size: 20 %\n\na = -63738.09854118214\nb = [441.42013341]\n     Actual     Predicted\n0    6795.0   6491.844685\n1   15750.0  19160.602514\n..      ...           ...\n39  45400.0  24192.792035\n40   8916.5   5079.300258\n\n[41 rows x 2 columns]\nMean Absolute Error: 4528.295484564718\nMean Squared Error: 43469954.12056989\nRoot Mean Squared Error: 6593.174813439266\nR2 Score: 43.84913586892223%\n\nTest Size: 10 %\n\na = -66742.8631493445\nb = [459.19742348]\n     Actual     Predicted\n0    6795.0   6315.446927\n1   15750.0  19494.412981\n..      ...           ...\n19   6488.0   6131.767957\n20   9959.0  12698.291113\n\n[21 rows x 2 columns]\nMean Absolute Error: 3945.9317457788898\nMean Squared Error: 29396652.85426334\nRoot Mean Squared Error: 5421.868022578873\nR2 Score: 26.410928631260454%\n\n\n\n\n\n3.1.2 Car Width vs Price\n\n\nCode\n# Carwidth Column\ncarWidth = dataset.iloc[:, 11:-14].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWidth, price, 30)\n\n\nTest Size: 30 %\n\na = -172630.60948546475\nb = [2822.14912394]\n     Actual     Predicted\n0    6795.0   8551.364271\n1   15750.0  15042.307256\n..      ...           ...\n60   6479.0   7704.719534\n61  15510.0  15042.307256\n\n[62 rows x 2 columns]\nMean Absolute Error: 3036.57768015824\nMean Squared Error: 22710512.087679498\nRoot Mean Squared Error: 4765.554751304354\nR2 Score: 65.60960571372881%\n\nTest Size: 20 %\n\na = -172526.22359994025\nb = [2819.03318321]\n     Actual     Predicted\n0    6795.0   8455.706762\n1   15750.0  14939.483084\n..      ...           ...\n39  45400.0  30444.165591\n40   8916.5   6764.286852\n\n[41 rows x 2 columns]\nMean Absolute Error: 3674.9155902799166\nMean Squared Error: 31370813.470780104\nRoot Mean Squared Error: 5600.965405247573\nR2 Score: 59.47779746918066%\n\nTest Size: 10 %\n\na = -181627.87173597398\nb = [2957.89666431]\n     Actual     Predicted\n0    6795.0   8269.094113\n1   15750.0  15072.256441\n..      ...           ...\n19   6488.0   6494.356114\n20   9959.0  11818.570110\n\n[21 rows x 2 columns]\nMean Absolute Error: 3197.214272696799\nMean Squared Error: 19783555.22362383\nRoot Mean Squared Error: 4447.87086409035\nR2 Score: 50.47553663690271%\n\n\n\n\n\n3.1.3 Curb Weight vs Price\n\n\nCode\n# Curbweight Column\ncarWeight = dataset.iloc[:, 13:-12].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWeight, price, 30)\n\n\nTest Size: 30 %\n\na = -18679.037713196016\nb = [12.40359272]\n     Actual     Predicted\n0    6795.0   4949.806413\n1   15750.0  20404.682939\n..      ...           ...\n60   6479.0   2568.316611\n61  15510.0  15530.071001\n\n[62 rows x 2 columns]\nMean Absolute Error: 2670.404540077829\nMean Squared Error: 18443910.151758883\nRoot Mean Squared Error: 4294.637371392244\nR2 Score: 72.0704958192619%\n\nTest Size: 20 %\n\na = -18833.605447325583\nb = [12.47623193]\n     Actual     Predicted\n0    6795.0   4933.616372\n1   15750.0  20479.001353\n..      ...           ...\n39  45400.0  27515.596159\n40   8916.5   4546.853183\n\n[41 rows x 2 columns]\nMean Absolute Error: 3256.3206631106873\nMean Squared Error: 25249391.034916148\nRoot Mean Squared Error: 5024.877215904499\nR2 Score: 67.3849408384091%\n\nTest Size: 10 %\n\na = -19880.405624111718\nb = [12.9537027]\n     Actual     Predicted\n0    6795.0   4796.398026\n1   15750.0  20936.711595\n..      ...           ...\n19   6488.0   6221.305324\n20   9959.0  10819.869783\n\n[21 rows x 2 columns]\nMean Absolute Error: 2695.197926817389\nMean Squared Error: 11737364.677960433\nRoot Mean Squared Error: 3425.9837533123873\nR2 Score: 70.61768320191304%\n\n\n\n\n\n3.1.4 Cylinder Number vs Price\n\n\nCode\n# Cylinder Number Column\ncylinderNumber = dataset.iloc[:, 15:-10].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(cylinderNumber, price, 30)\n\n\nTest Size: 30 %\n\na = -8750.74345729567\nb = [5045.13677503]\n     Actual     Predicted\n0    6795.0  11429.803643\n1   15750.0  21520.077193\n..      ...           ...\n60   6479.0  11429.803643\n61  15510.0  11429.803643\n\n[62 rows x 2 columns]\nMean Absolute Error: 3944.3868255082953\nMean Squared Error: 26684225.038138304\nRoot Mean Squared Error: 5165.677597192676\nR2 Score: 59.59223566856468%\n\nTest Size: 20 %\n\na = -9046.162097201766\nb = [5112.35112126]\n     Actual     Predicted\n0    6795.0  11403.242388\n1   15750.0  21627.944630\n..      ...           ...\n39  45400.0  31852.646873\n40   8916.5  11403.242388\n\n[41 rows x 2 columns]\nMean Absolute Error: 4280.5628888250285\nMean Squared Error: 32605207.611888204\nRoot Mean Squared Error: 5710.096987958103\nR2 Score: 57.88330998687709%\n\nTest Size: 10 %\n\na = -10564.254121382277\nb = [5479.84028365]\n     Actual     Predicted\n0    6795.0  11355.107013\n1   15750.0  22314.787580\n..      ...           ...\n19   6488.0  11355.107013\n20   9959.0  11355.107013\n\n[21 rows x 2 columns]\nMean Absolute Error: 3464.088015146232\nMean Squared Error: 18346836.560473613\nRoot Mean Squared Error: 4283.32073985519\nR2 Score: 54.072095495610604%\n\n\n\n\n\n3.1.5 Engine Size vs Price\n\n\nCode\n# Engine Size Column\nengineSize = dataset['enginesize'].values.reshape(-1, 1)\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(engineSize, price, 30)\n\n\nTest Size: 30 %\n\na = -7574.131488222356\nb = [163.29075344]\n     Actual     Predicted\n0    6795.0   7285.327074\n1   15750.0  18715.679815\n..      ...           ...\n60   6479.0   7448.617828\n61  15510.0  12184.049678\n\n[62 rows x 2 columns]\nMean Absolute Error: 2898.9726929694702\nMean Squared Error: 14541824.65222288\nRoot Mean Squared Error: 3813.374444271488\nR2 Score: 77.97940083865093%\n\nTest Size: 20 %\n\na = -7613.370926304753\nb = [164.31545176]\n     Actual     Predicted\n0    6795.0   7339.335184\n1   15750.0  18841.416808\n..      ...           ...\n39  45400.0  42338.526410\n40   8916.5   7175.019732\n\n[41 rows x 2 columns]\nMean Absolute Error: 3195.031241401546\nMean Squared Error: 16835544.028987687\nRoot Mean Squared Error: 4103.113942969131\nR2 Score: 78.25324722629195%\n\nTest Size: 10 %\n\na = -8207.420855494747\nb = [169.490971]\n     Actual     Predicted\n0    6795.0   7216.257505\n1   15750.0  19080.625475\n..      ...           ...\n19   6488.0   7385.748476\n20   9959.0  10436.585954\n\n[21 rows x 2 columns]\nMean Absolute Error: 2877.111549011615\nMean Squared Error: 12997474.409783443\nRoot Mean Squared Error: 3605.2010221045157\nR2 Score: 67.46323206602058%\n\n\n\n\n\n3.1.6 Horsepower vs Price\n\n\nCode\n# Horsepower Column\nhorsepower = dataset.iloc[:, 21:-4].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(horsepower, price, 30)\n\n\nTest Size: 30 %\n\na = -4438.686268723588\nb = [170.53827527]\n     Actual     Predicted\n0    6795.0   7157.916450\n1   15750.0  22165.284674\n..      ...           ...\n60   6479.0   5452.533697\n61  15510.0  14320.524011\n\n[62 rows x 2 columns]\nMean Absolute Error: 3518.2488303322393\nMean Squared Error: 25821021.51495541\nRoot Mean Squared Error: 5081.438921698795\nR2 Score: 60.89937966412712%\n\nTest Size: 20 %\n\na = -4053.153036276188\nb = [166.64923709]\n     Actual     Predicted\n0    6795.0   7278.995086\n1   15750.0  21944.127950\n..      ...           ...\n39  45400.0  26610.306588\n40   8916.5   7612.293560\n\n[41 rows x 2 columns]\nMean Absolute Error: 3733.6933754512147\nMean Squared Error: 29626244.692692798\nRoot Mean Squared Error: 5442.999604325983\nR2 Score: 61.73128603174041%\n\nTest Size: 10 %\n\na = -4796.241165629246\nb = [174.95075436]\n     Actual     Predicted\n0    6795.0   7100.410131\n1   15750.0  22496.076514\n..      ...           ...\n19   6488.0   6050.705605\n20   9959.0  15498.046340\n\n[21 rows x 2 columns]\nMean Absolute Error: 3839.1982159225827\nMean Squared Error: 26172943.363739382\nRoot Mean Squared Error: 5115.949898478227\nR2 Score: 34.48088778430891%"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#multivariate-models",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#multivariate-models",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "3.2 Multivariate Models",
    "text": "3.2 Multivariate Models\nMultiple univariate models were trained for comparison using a custom Linear Regression training function. Accuracy varies in each model with changes in training/test splits and would most likely would be benefitted with more rows of data ( See Items 3.2.1 - 3.2.3 ). The highest accurracy is seen with the model that takes in the most columns for the independent variables ( See Items 3.2.3 ).\n\n3.2.1 Carlength, Carwidth, Curbweight vs Price\n\n\nCode\n# Create copy of dataset and drop all columns not used for multivariate regression models\ndatasetCopy = dataset\ndatasetCopy.drop(['carheight', 'enginetype', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio'],\\\ninplace=True, axis=1)\n\n# Store carlength, carwidth & curbweight columns in X\nX1 = datasetCopy.iloc[:, 10:-7].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X1, price, 30)\n\n\nTest Size: 30 %\n\na = -36739.21951780665\nb = [-208.48890057  764.98885537   13.97180015]\n     Actual     Predicted\n0    6795.0   5818.760203\n1   15750.0  19003.466111\n..      ...           ...\n60   6479.0   5929.766976\n61  15510.0  13762.735333\n\n[62 rows x 2 columns]\nMean Absolute Error: 2458.5442776902337\nMean Squared Error: 16492573.815910544\nRoot Mean Squared Error: 4061.1049993703123\nR2 Score: 75.02539290462342%\n\nTest Size: 20 %\n\na = -44634.67127094674\nb = [-188.42001434  856.204069     13.34802623]\n     Actual     Predicted\n0    6795.0   5783.995638\n1   15750.0  18977.251262\n..      ...           ...\n39  45400.0  29066.672270\n40   8916.5   5459.428429\n\n[41 rows x 2 columns]\nMean Absolute Error: 2943.0381053387778\nMean Squared Error: 22423198.502769\nRoot Mean Squared Error: 4735.313981434494\nR2 Score: 71.03558082852051%\n\nTest Size: 10 %\n\na = -51021.53652492876\nb = [-194.19115484  961.28023332   13.59661598]\n     Actual     Predicted\n0    6795.0   5698.395155\n1   15750.0  19277.437054\n..      ...           ...\n19   6488.0   6694.931234\n20   9959.0  10475.100811\n\n[21 rows x 2 columns]\nMean Absolute Error: 2357.566870487648\nMean Squared Error: 9101964.422986511\nRoot Mean Squared Error: 3016.9462081691995\nR2 Score: 77.21491923452972%\n\n\n\n\n\n3.2.2 Cylinder Number, Engine Size, Horsepower vs Price\n\n\nCode\n# Store cylindernumber, enginesize & horsepower columns in X\nX2 = datasetCopy.iloc[:, 13:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X2, price, 30)\n\n\nTest Size: 30 %\n\na = -6717.131795698624\nb = [-875.82889691  133.38667558   65.31455209]\n     Actual     Predicted\n0    6795.0   6359.129636\n1   15750.0  19692.219717\n..      ...           ...\n60   6479.0   5839.370791\n61  15510.0  13103.941091\n\n[62 rows x 2 columns]\nMean Absolute Error: 2681.2430726638395\nMean Squared Error: 13246002.119140355\nRoot Mean Squared Error: 3639.505752041114\nR2 Score: 79.941657245098%\n\nTest Size: 20 %\n\na = -7307.824968281864\nb = [-522.31275964  128.16628088   63.17240963]\n     Actual     Predicted\n0    6795.0   6561.779408\n1   15750.0  20047.965597\n..      ...           ...\n39  45400.0  39099.945713\n40   8916.5   6559.957946\n\n[41 rows x 2 columns]\nMean Absolute Error: 3028.44528450474\nMean Squared Error: 15255724.671464592\nRoot Mean Squared Error: 3905.8577382522003\nR2 Score: 80.29392621688581%\n\nTest Size: 10 %\n\na = -7824.384102535303\nb = [-613.42877141  135.45474355   63.82356299]\n     Actual     Predicted\n0    6795.0   6388.284759\n1   15750.0  20259.732808\n..      ...           ...\n19   6488.0   6140.798124\n20   9959.0  12025.455910\n\n[21 rows x 2 columns]\nMean Absolute Error: 2944.3040595022885\nMean Squared Error: 12712894.325743863\nRoot Mean Squared Error: 3565.5145948016907\nR2 Score: 68.17562555579416%\n\n\n\n\n\n3.2.3 Carlength, Carwidth, Curbweight, Cylinder Number, Engine Size, Horsepower vs Price\n\n\nCode\n# Store carlength, carwidth, curbweight, cylindernumber,\n# enginesize & horsepower columns in X\nX3 = datasetCopy.iloc[:, 10:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X3, price, 30)\n\n\nTest Size: 30 %\n\na = -50133.27454870472\nb = [-62.20404054 772.47835707   3.18527494  18.99449375  65.77507898\n  64.33402142]\n     Actual     Predicted\n0    6795.0   6067.345502\n1   15750.0  20331.280734\n..      ...           ...\n60   6479.0   5548.422659\n61  15510.0  13525.755400\n\n[62 rows x 2 columns]\nMean Absolute Error: 2536.7293535638096\nMean Squared Error: 12555624.008739235\nRoot Mean Squared Error: 3543.39159686581\nR2 Score: 80.98709273909488%\n\nTest Size: 20 %\n\na = -54793.72590677004\nb = [-38.92156396 789.71920542   2.95208156 366.09875078  59.3369646\n  58.3182745 ]\n     Actual     Predicted\n0    6795.0   6167.243070\n1   15750.0  20562.635157\n..      ...           ...\n39  45400.0  36977.654082\n40   8916.5   5783.745608\n\n[41 rows x 2 columns]\nMean Absolute Error: 2873.7239149228203\nMean Squared Error: 16025434.859390952\nRoot Mean Squared Error: 4003.178094887979\nR2 Score: 79.29967874050975%\n\nTest Size: 10 %\n\na = -55531.82635570387\nb = [-30.01857827 784.53201154   2.29960226 213.59515415  74.83936968\n  58.2469381 ]\n     Actual     Predicted\n0    6795.0   6065.470340\n1   15750.0  20665.341927\n..      ...           ...\n19   6488.0   5585.072554\n20   9959.0  11876.766620\n\n[21 rows x 2 columns]\nMean Absolute Error: 3020.6881171033187\nMean Squared Error: 13605511.765351668\nRoot Mean Squared Error: 3688.5650008304947\nR2 Score: 65.94112325398689%"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#no-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#no-scaling",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "4.1 No Scaling",
    "text": "4.1 No Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'None')\n\n\n\n\n\nKNeigbors Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       0.00      0.00      0.00         6\n         gas       0.85      0.94      0.89        35\n\n    accuracy                           0.80        41\n   macro avg       0.42      0.47      0.45        41\nweighted avg       0.72      0.80      0.76        41\n\nDecision Tree Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         6\n         gas       1.00      1.00      1.00        35\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure 5. Classifier Models Confusion Matrices - No Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#standardized-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#standardized-scaling",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "4.2 Standardized Scaling",
    "text": "4.2 Standardized Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Standardize')\n\n\n\n\n\nKNeigbors Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure 6. Classifier Models Confusion Matrices - Standard Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#normalized-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#normalized-scaling",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "4.3 Normalized Scaling",
    "text": "4.3 Normalized Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Normalize')\n\n\n\n\n\nKNeigbors Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure 7. Classifier Models Confusion Matrices - Normalized Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#kmeans-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#kmeans-model",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "5.1 KMeans Model",
    "text": "5.1 KMeans Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# KMeans Cluster Model to be used\nmodel = 'KM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Scatter Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - No Scaling\n\n\n\n\n\n\n\n\n\n(c) Joint Plot - Normalized Scaling\n\n\n\n\nFigure 8. KMeans Cluster Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#gaussian-mixture-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#gaussian-mixture-model",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "5.2 Gaussian Mixture Model",
    "text": "5.2 Gaussian Mixture Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Gaussian Mixture Model to be used\nmodel = 'GMM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Joint Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - Normalized Scaling\n\n\n\n\nFigure 9. Gaussian Mixture Cluster Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#spectral-clustering-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#spectral-clustering-model",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "5.3 Spectral Clustering Model",
    "text": "5.3 Spectral Clustering Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Spectral Clustering Model to be used\nmodel = 'SC'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Joint Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - Normalized Scaling\n\n\n\n\nFigure 10. Spectral Clustering Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#references",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#references",
    "title": "Intro to AI - Linear Regression, Classification, Clustering",
    "section": "References",
    "text": "References\nDataset\nhttps://www.kaggle.com/datasets/hellbuoy/car-price-prediction?select=CarPrice_Assignment.csv\nData Cleaning\nhttps://datatofish.com/category/python/\nData Scaling\nhttps://dataakkadian.medium.com/standardization-vs-normalization-da7a3a308c64\nhttps://medium.datadriveninvestor.com/data-pre-processing-with-scikit-learn-9896c561ef2f\nMeasuring Accuracy\nhttps://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/\nVisualizations\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\nhttps://seaborn.pydata.org/api.html\nhttps://matplotlib.org/stable/api/pyplot_summary.html\nClassification & Clustering Models\nhttps://www.activestate.com/resources/quick-reads/how-to-classify-data-in-python/\nhttps://builtin.com/data-science/data-clustering-python\nhttps://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\nGeneral Python\nhttps://stackoverflow.com"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]