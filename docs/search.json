[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Currently in my final year of an Artificial Intelligence BSc degree. With a backdrop of years in the workforce, I’m embarking on an exciting journey to dive deep into the realm of Artificial Intelligence. My career shift isn’t just about a change—it’s about a genuine passion for learning new things. I’ve found immense joy in unraveling the mysteries of AI, blending the wisdom of experience with the curiosity of a beginner.\n\nAffiliations\nCurrent Student: Artificial Intelligence BSc, University of Gloucestershire\nPrevious Student: Network Administrator Specialist Diploma, Centre for Arts & Technology"
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#step-by-step-description",
    "href": "posts/drone-delivery/drone-delivery.html#step-by-step-description",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "1.1 Step-by-Step Description",
    "text": "1.1 Step-by-Step Description\n\nGraph Initialization: A Streamlit GUI is implemented to allow users to interactively specify the size and number of nodes for the graph. This setup enables users to customize the complexity and scale of the synthetic graph according to their specific testing needs.\nNode Creation: Nodes are dynamically generated based on the user-specified number. Each node represents a potential delivery location in a hypothetical urban environment. Each node is initialized with an initial delivery urgency of 0. However, when random goals are selected in the genetic algortithm (GA) tab of the GUI the nodes that are selected are randomly assigned delivery urgency values from 1 to 5.\nNode Interconnection: Nodes are interconnected based on their proximity to simulate realistic urban layouts. This proximity-based linking ensures that the graph realistically mimics city road networks where closer locations are more likely to be directly connected.\nEdge Assignment: Each edge between nodes is assigned a randomly generated traffic volume, on a scale from 1 to 5, where 1 represents minimal traffic and 5 represents heavy traffic. This randomness introduces variability in the dataset, simulating real-world unpredictability in urban traffic conditions.\nCost Metric Calculation: The cost to traverse each path (edge) is calculated based on a base drone speed, which is adjusted according to the traffic level on that path. Higher traffic conditions result in slower speeds, increasing the traversal time and thus the cost.\nData Visualization: The GUI visually represents the graph with color-coded edges according to traffic density. This visual representation aids in quick analysis and understanding of traffic conditions across different paths."
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#justifications",
    "href": "posts/drone-delivery/drone-delivery.html#justifications",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "1.2 Justifications",
    "text": "1.2 Justifications\n\nCost Metric Based on Real-World Conditions: Using adjusted drone speeds to calculate path costs introduces a practical element to the model, where the quickest path might not always be the shortest distance due to varying traffic conditions. This approach teaches the model to optimize time efficiency, which is more aligned with real-world objectives in drone delivery systems.\nVisualization for Clarity and Insight: Implementing color-coded visualizations of traffic conditions not only makes the data more accessible and understandable for users but also aids in quicker decision-making and algorithm adjustments based on visible traffic patterns."
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#step-by-step-description-1",
    "href": "posts/drone-delivery/drone-delivery.html#step-by-step-description-1",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "2.1 Step-by-Step Description",
    "text": "2.1 Step-by-Step Description\n\nDefinition of State Space, Initial State, and Goal State:\n\nState Space: The state space encompasses all possible positions of the drone within the city, represented by the nodes in the synthetic graph. Each node corresponds to a potential drone location, thus defining the complete set of states that the drone can occupy.\nInitial State: The initial state is defined as the drone’s starting position on the graph, set at a predefined node representing the central hub.\nGoal State: The goal state is the delivery destination, which is another node on the graph where the package needs to be delivered. This is selected in the GUI by the user on the A* tab of the toolbar.\n\nHeuristic Function: The heuristic function designed for the A* algorithm incorporates the following factors to estimate the remaining cost from the current state (node) to the goal state:\n\nEuclidean Distance: Computes the straight-line distance to the goal, providing a basic estimate of proximity.\nTraffic Conditions: Adjusts the distance estimate based on the traffic level associated with each path, acknowledging that higher traffic may increase travel time.\n\nAlgorithm Implementation: The A* algorithm is implemented to dynamically choose the path that minimizes the combined cost calculated by the heuristic function. It starts at the initial state and explores paths through the graph, continually updating the path costs based on the heuristic function until it reaches the goal state. Different heuristic options (Listing 1) are provided to the user via a GUI:\n\nNo Heuristic: Treats the search as a breadth-first search, exploring all possible paths equally without prioritization.\nEuclidean Distance: Prioritizes paths that reduce the physical distance to the goal.\nEuclidean Distance with Traffic Information: Default setting, prioritizes paths by combining distance and traffic data, effectively reducing unnecessary path explorations and enhancing computational efficiency (Figures 2-4).\n\n\n\nListing 1. Heuristic options.\n\na_star.py\n\n# Method to calculate the heuristic value of a node\n    def heuristic(self, node, goal_node, traffic_level):\n\n        # No heuristic\n        def zero(node, goal_node, traffic_level):\n            return 0\n\n        # Euclidean distance heuristic\n        def euclidean(node, goal_node, traffic_level):\n            # Calculate euclidean distance from node to goal node\n            return gp.euc_dist(node.pos, goal_node.pos)\n\n        # Euclidean distance with traffic awareness heuristic\n        def traffic_aware(node, goal_node, traffic_level):\n            # Calculate euclidean distance from node to goal node\n            distance = gp.euc_dist(node.pos, goal_node.pos)\n            # Calculate traffic factor based on traffic level,\n            # will be 50% higher for each unit increase in the traffic level\n            traffic_factor = 1 + traffic_level * 0.5\n            # Return euclidean distance multiplied by traffic factor\n            return distance * traffic_factor\n\n\n2.1.1 Justifications\n\nHeuristic Options: Offering different heuristic settings allows users to customize the algorithm’s behavior according to specific requirements or conditions, thereby enhancing the utility and adaptability of the algorithm across various operational contexts.\nComposite Heuristic Function: The design of the heuristic function is crucial for balancing different real-world considerations such as distance and traffic. By integrating these factors, the heuristic function provides a realistic and practical approach to estimating travel costs, leading to more efficient and applicable route optimizations in urban drone delivery scenarios (Figure 1 (c)).\nVisualization and Computational Efficiency: Visualizing the paths explored, highlighted in pink, and the optimal path, highlighted in cyan, not only provides transparency into the algorithm’s functioning but also demonstrates its efficiency in real-time, reinforcing the effectiveness of the chosen heuristic in reducing computational load (Figure 1).\n\n\n\n\n\n\n\n\n(a) No Heuristic\n\n\n\n\n\n\n\n(b) Euclidean Distance Heuristic\n\n\n\n\n\n\n\n\n\n(c) Euclidean Distance and Traffic Heuristic\n\n\n\n\nFigure 1. A* Heuristics"
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#user-interaction-and-setup",
    "href": "posts/drone-delivery/drone-delivery.html#user-interaction-and-setup",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "3.1 User Interaction and Setup",
    "text": "3.1 User Interaction and Setup\nThe genetic algorithm (GA) implemented in this project offers a highly interactive and user-friendly environment for experimenting with drone delivery strategies. Users initiate the simulation by clicking a button that randomly selects ten delivery goals, each assigned a random urgency value to simulate time-sensitive delivery requirements. These urgencies are visualized on the graph and detailed in the accompanying legend, clearly presenting the delivery priorities to the user (Figure 2). Users can configure the number of drones, ranging from 1 to 10, to distribute the delivery tasks, thereby optimizing the delivery schedule based on available resources and the urgency of deliveries.\n\n\n\nFigure 2. Delivery urgencies"
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#representation-of-solutions-chromosomes",
    "href": "posts/drone-delivery/drone-delivery.html#representation-of-solutions-chromosomes",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "3.2 Representation of Solutions (Chromosomes)",
    "text": "3.2 Representation of Solutions (Chromosomes)\n\n3.2.1 Step-by-Step Description\n\nChromosome Structure: Each chromosome in the GA represents a potential solution, where a solution is the sequence of nodes (goals) that each in the individual drone visits. This sequence directly influences the route that each drone will take through the city, with each gene in the chromosome corresponding to a delivery point.\nEncoding Details: The paths that each drone will follow are encoded within these chromosomes, with the specific order of nodes dictating the sequence of deliveries. The chromosome structure allows for easy manipulation during the genetic operations, which is crucial for effectively exploring the solution space.\n\n\n\n3.2.2 Justifications\nChromosome representation: This representation is chosen for its straightforward relation to the problem’s nature, which is essentially a routing problem. By representing routes as sequences of nodes, the genetic algorithm can efficiently apply crossover and mutation operations to explore new route configurations and optimize delivery sequences. This setup mirrors real-world routing optimizations and is integral to achieving practical, implementable solutions."
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#crossover-and-mutation-operations",
    "href": "posts/drone-delivery/drone-delivery.html#crossover-and-mutation-operations",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "3.3 Crossover and Mutation Operations",
    "text": "3.3 Crossover and Mutation Operations\n\n3.3.1 Step-by-Step Description\n\nCrossover Operations: The genetic algorithm offers three crossover methods* allowing users to select the strategy that best suits their operational goals:\n\nUniform Crossover: Each gene (node) from parent chromosomes has a 50% chance of being transferred to the offspring. This method ensures diversity in the gene pool by randomly mixing genes from two parent solutions.\nOne-Point Crossover: A random point is selected in the parents’ sequence. For a single drone configuration, the crossover point is at the goal level, splitting the sequence of goals. For multiple drones, the split occurs at the drone level, where one part of the drone fleet and their respective goals come from one parent, and the remainder from the other parent.\nHeuristic Crossover: Zhang, P. et al. (2022) suggests a novel approach to performing the crossover to assist a GA in finding the global optimal solution in the travelling salesmen problem. Following similar principles (Zhang, P. et al., 2022, pp. 4-5), we utilize a custom heuristic to assess the suitability of placing a selected goal next to another in the offspring. The heuristic evaluates compatibility based on the cost dictionary, favoring high-quality matches that are more likely to result in efficient routes (Listing 2).\n\n\n\n\n\n\n\n\nSource Code\n\n\n\nSee uniform_crossover(), one_point_crossover(), and heuristic_crossover() methods in gen_alg.py for details.\n\n\n\nListing 2. Method for calculating goal quality.\n\ngen_alg.py\n\n# Method to calculate the quality of a goal based on the heuristic\n    def is_high_quality(self, goal1, goal2):\n        # Get the cost of traveling from goal1 to goal2\n        cost = self.costs[goal1][goal2][1]\n        # Get the minimum and maximum cost of traveling to goal1 from any other goal\n        min_cost, max_cost = self.goal_cost_range[goal1]\n        # Calculate the quality of the goal based on the heuristic\n        # The closer the cost is to the minimum cost the higher the quality of the goal\n        where = 1 - ((cost - min_cost) / (max_cost - min_cost))\n        # Return the quality of the goal\n        return where\n\n\nMutation Process: The mutation process begins by checking if a mutation should occur, which is determined by a predetermined mutation rate set by the user. A drone is randomly selected from the fleet. The algorithm ensures that the selected drone has at least one goal. A specific goal is then chosen from the selected drone’s list of delivery locations, either by picking the only goal or randomly selecting one if multiple goals exist. This goal is removed from the chosen drone’s list of destinations. If there are drones with no assigned goals (empty drones), the removed goal is assigned to one of these drones, promoting distribution of delivery tasks. If all drones have assigned goals, the removed goal is added to a randomly selected drone’s list, increasing the diversity of route configurations explored by the algorithm (Listing 3).\n\n\nListing 3. Method for mutating individuals.\n\ngen_alg.py\n\n# Method to mutate individuals by moving goals from one drone to another\n    def mutate(self, individual):\n        # Percentage based on mutation rate given\n        if random.random() &lt; self.mutation_rate:\n            # Move a goal from one drone to another\n            for _ in range(2):\n                # Select a random drone\n                drone = random.randint(0, len(individual)-1)\n                # Ensure that the selected drone has goals\n                while len(individual[drone].locations) == 0:\n                    # Select a random drone\n                    drone = random.randint(0, len(individual)-1)\n\n                # If the drone has only one goal\n                if len(individual[drone].locations) == 1:\n                    # Select the only goal\n                    goal = 0\n                # If the drone has more than one goal\n                else:\n                    # Select a random goal\n                    goal = random.randint(0, len(individual[drone].locations)-1)\n\n                # Remove the goal from the drone\n                location = individual[drone].locations.pop(goal)\n                # Select drones with no goals\n                empty_drones = [d for d in individual if len(d.locations) == 0]\n\n                # If there are drones with no goals\n                if empty_drones:\n                    # Select a random drone with no goals\n                    drone = random.choice(empty_drones)\n                    # Add the goal to the drone\n                    drone.locations.append(location)\n                # If there are no drones with no goals\n                else:\n                    # Select a random drone to add the goal to\n                    individual[random.randint(0, len(individual)-1)].locations.append(location)\n        # Return the mutated individual\n        return individual\n\n\n\n3.3.2 Justifications\n\nCrossover Methods: Each crossover method offers different advantages: uniform crossover introduces high variability, one-point preserves existing good sequences, and heuristic crossover refines solutions by incorporating domain-specific knowledge, increasing the likelihood of finding optimal routes. These methods are selected to balance exploration and exploitation in the genetic search process, optimizing the algorithm’s performance across diverse scenarios."
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#fitness-function",
    "href": "posts/drone-delivery/drone-delivery.html#fitness-function",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "3.4 Fitness Function",
    "text": "3.4 Fitness Function\n\n3.4.1 Step-by-Step Description\n\nIntegration with A* Algorithm: Before the genetic algorithm commences, the A* algorithm is employed to calculate the most efficient paths and distances between each goal and the central hub node. These calculations and optimal routes between the goal nodes are then saved in a cost dictionary to be used during the fitness evaluation (Listing 4).\n\n\nListing 4. Method for creating cost dictionary.\n\ngen_alg.py\n\n# Method to calculate the travel times and paths between all goal states\n    # and store them in a dictionary for future reference\n    def calculate_travel_times(self):\n        # Initialize dictionary to store all travel times and paths from the hub to each goal state\n        hub_to_goals = {}\n        # Iterate through all goal states\n        for goal in self.goal_states:\n            # Create A* agent for the initial state(Hub) and goal\n            astar_agent = ar.Agent('A*',\n                                   self.state_space,\n                                   self.initial_state,\n                                   goal,\n                                   'Euclidean + Traffic Aware')\n            # Perform A* search of the initial state(hub) and goal\n            path, travel_time = astar_agent.astar_search()\n            # Add the travel time to the hub_to_goal costs dictionary\n            hub_to_goals[goal] = (path, travel_time)\n            # Iterate through all other goal states\n            for other_goal in self.goal_states:\n                # If the goal is not the same as the other goal\n                if not goal == other_goal:\n                    # Create A* agent for the goal and other goal\n                    astar_agent = ar.Agent('A*',\n                                           self.state_space,\n                                           goal,\n                                           other_goal,\n                                           'Euclidean + Traffic Aware')\n                    # Perform A* search of the goal and other goal\n                    path, travel_time = astar_agent.astar_search()\n                    # Add the travel time to the costs dictionary\n                    self.costs[goal][other_goal] = (path, travel_time)\n\n        # Add the travel times from the hub to the goals to the costs dictionary\n        self.costs[self.initial_state] = hub_to_goals\n\n        # Calculate the range of costs for each goal state\n        for key in self.costs:\n            # Get the maximum and minimum travel times for each goal state\n            max_time = max(self.costs[key][key2][1] for key2 in self.costs[key])\n            min_time = min(self.costs[key][key2][1] for key2 in self.costs[key])\n            # Add the range of costs to the goal_cost_range dictionary\n            self.goal_cost_range[key] = (min_time, max_time)\n\n\nFitness Calculation: The fitness of each chromosome is calculated using the cost dictionary, which includes the optimal paths and distances pre-computed by the A* algorithm. Penalties increase for delays in addressing more urgent goals, accumulating until each goal is reached (Listing 5). These factors are computed using the pre-defined costs and urgency values associated with each node in the chromosome.\n\n\nListing 5. Method for calculating individual fitness\n\ngen_alg.py\n\n# Method to calculate the fitness of an individual\n    def fitness_function(self, individual):\n        # Initialize total time and total utility cost\n        total_time = 0\n        total_utility_cost = 0\n        # Loop through each drone in the individual\n        for drone in individual:\n            # Initialize drone path, cumulative time, and cumulative utility cost\n            drone.path = []\n            cumulative_time = 0\n            cumulative_utility_cost = 0\n\n            # If the drone has more than one goal\n            if len(drone.locations) &gt; 1:\n                # Loop through each goal in the drone\n                for i in range(len(drone.locations)):\n                    # Grab precalculated travel time and paths from costs dictionary from current state to goal\n                    travel_time = self.costs[self.current_state][drone.locations[i]][1]\n                    paths = self.costs[self.current_state][drone.locations[i]][0]\n                    # For each path in the paths\n                    for n in range(len(paths)):\n                        # If the i which is the index of the goal is the last goal in the drone\n                        if i == len(drone.locations)-1:\n                            # Add the path to the drone\n                            drone.path.append(paths[n])\n                        # Else if the i which is the index of the goal is not the last goal in the drone\n                        # and the n which is the index of the path is not the last path in the paths\n                        elif n &lt; len(paths)-1:\n                            # Add the path to the drone\n                            drone.path.append(paths[n])\n                    # Add travel time to cumulative time\n                    cumulative_time += travel_time\n                    # Calculate utility cost for first goal based on travel time and delivery urgency\n                    utility_cost = cumulative_time * drone.locations[i].delivery_urgency/10\n                    # Add utility cost to cumulative utility cost\n                    cumulative_utility_cost += utility_cost\n                    # Update current state to goal\n                    self.current_state = drone.locations[i]\n            # If the drone has only one goal\n            elif len(drone.locations) == 1:\n                # Perform A* search from current state to goal\n                travel_time = self.costs[self.current_state][drone.locations[0]][1]\n                # For each node in the path from current state to goal\n                for node in self.costs[self.current_state][drone.locations[0]][0]:\n                    # Add the node to the drone path\n                    drone.path.append(node)\n                # Add travel time to cumulative time\n                cumulative_time += travel_time\n                # Calculate utility cost for first goal based on travel time and delivery urgency\n                utility_cost = cumulative_time * drone.locations[0].delivery_urgency/10\n                # Add utility cost to cumulative utility cost\n                cumulative_utility_cost += utility_cost\n                # Update current state to goal\n                self.current_state = drone.locations[0]\n\n            # Store the total cost of the drone\n            drone.cost = cumulative_time + cumulative_utility_cost\n            # Add cumulative time and cumulative utility cost to total time and total utility cost\n            total_time += cumulative_time\n            # Update total utility cost\n            total_utility_cost += cumulative_utility_cost\n            # Update current state to initial state\n            self.current_state = self.initial_state\n\n        # Return the fitness of the individual\n        return total_time + total_utility_cost\n\n\nImplementation: Fitness is inversely proportional to the total cost; thus, a lower cost results in higher fitness. This cost includes penalties for delayed deliveries, particularly for urgent goals, which accumulate until each delivery is completed.\n\n\n\n3.4.2 Justifications\n\nCost Dictionary: This pre-computation step is crucial because while the GA is adept at optimizing the order of goal visitations, it is not designed to determine the most efficient paths between these goals. The A* algorithm, known for its efficiency in finding the shortest paths in terms of time and space complexity, ensures that the paths used in the GA’s fitness evaluations are optimal. This separation of concerns allows the GA to focus solely on sequencing the deliveries, leveraging its strengths in evolving solutions over generations without the computational overhead of pathfinding (Listing 4).\nUtility Cost: The fitness function is designed to align closely with the operational goals of reducing delivery times and costs. By incorporating urgency into the fitness calculation, the genetic algorithm prioritizes more time-sensitive deliveries, reflecting real-world priorities in logistics operations. This method ensures that the algorithm not only finds feasible routes but also optimizes them according to practical business objectives (Listing 5)."
  },
  {
    "objectID": "posts/drone-delivery/drone-delivery.html#algorithm-execution-and-results",
    "href": "posts/drone-delivery/drone-delivery.html#algorithm-execution-and-results",
    "title": "Autonomous Drone Delivery Optimization",
    "section": "3.5 Algorithm Execution and Results",
    "text": "3.5 Algorithm Execution and Results\nUpon initiating the genetic algorithm, users can observe the evolution of solutions across generations in the console. The fittest solutions are displayed graphically, illustrating the cost efficiency and route effectiveness achieved by the algorithm. This visual feedback is crucial for verifying the algorithm’s efficacy and for potential adjustments based on operational feedback (Figure 3).\n\n\n\nFigure 3. Best solution visually represented"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#purpose-of-document",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#purpose-of-document",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide a business case to assist ACME Inc in evaluating the viability and profitability of the proposed project. It details estimates on functions/activities, cost of development, costs of running and hardware/network requirements in developing and maintaining a machine learning software for ACME Inc."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#scope",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#scope",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nScope can be split into three parts: (1) development of machine learning software, (2) setup and deployment of data pipeline for feeding to the software and (3) maintaining the solution for sustained future company use. This proposed solution does not include training staff on machine learning software after deployment but can be negotiated."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#background",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#background",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "2.1 Background",
    "text": "2.1 Background\nACME Inc is increasingly accumulating data on the customers that interact with the company. As the company grows, profits have not been increasing in proportion to the rising costs of larger infrastructure to support growth. At the same time marketing is struggling to identify where their efforts are most effective as the customer base widens along with product lines. The proposed project provides a solution to these problems by making use of the ever growing asset the company possesses, customer data.\n\n\n\n\n\n\nNote\n\n\n\nSee 5 Preliminary Findings of this document to investigate some of the initial insights that the prototype software has been able to uncover."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#objectives",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#objectives",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "2.2 Objectives",
    "text": "2.2 Objectives\n2.2.1 Improve ACME’s data hygiene so that its data is better utilized.\n2.2.2 Produce new actionable insights with company data which can be used to benefit ACME.\n2.2.3 Provide concrete targets for future marketing campaigns.\n2.2.4 Increase company profits."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#functionsactivities",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#functionsactivities",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.1 Functions/Activities",
    "text": "3.1 Functions/Activities\nA centralized database server will need to be securely setup for data to be loaded into for the software to access. The database server will require at least one (1) database admin to deploy and maintain. Software development itself will require three (3) phases: (1) Program, system, operations and user planning/design/documentation. (2) Development of software based on designs and deployment. (3) Maintenance of software with scheduled patches, updates and new features. To successfully execute the software development portions staff required are as follows… one (1) project manager, two (2) designers, one (1) UI developer and one (2) machine learning engineers. In the maintenance stage, staff can be reduced to just one (1) manager, one (1) database admin, one (1) developer and (1) machine learning engineer."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-development",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-development",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.2 Cost of Development",
    "text": "3.2 Cost of Development\n\n\n\n\n\n\n\n\n3.2.1\nOne (1) project manager – 3 months\n£25,000\n\n\n3.2.2\nTwo (2) designers – 1 month\n£10,000\n\n\n3.2.3\nOne (1) UI developer – 1 month\n£5,000\n\n\n3.2.4\nOne (1) database admin – 2 weeks\n£2,500\n\n\n3.2.5\nTwo (2) machine learning engineers – 2 months\n£30,000\n\n\n\nTOTAL\n£72,500"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-running",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#cost-of-running",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.3 Cost of Running",
    "text": "3.3 Cost of Running\n\n\n\n3.3.1\nOne (1) manager – 3 months\n£100,000/year\n\n\n3.3.2\nOne (1) database admin - retainer\n£10,000/year\n\n\n3.3.3\nOne (1) developer\n£60,000/year\n\n\n3.3.4\nOne (1) machine learning engineer\n£90,000/year\n\n\n\nTOTAL\n£260,000/year\n\n\n\n\n\n\n\n\n\nTotals are estimated based on current market costs as of June 2023 and are subject to change."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#hardwarenetwork-costs",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#hardwarenetwork-costs",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "3.4 Hardware/Network Costs",
    "text": "3.4 Hardware/Network Costs\nDatabase deployment and software development can be achieved using the company’s current infrastructure and hardware available."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#database-implementation",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#database-implementation",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "4.1 Database Implementation",
    "text": "4.1 Database Implementation\nPrototype database has been setup using a free tier Oracle Standard.A1.Flex virtual machine instance(1 CPU, 6GB RAM, Ubuntu 22.04). A MariaDB server has been installed on the instance with a user account created for the software to access the server directly. Mysql.connector and SQLAlchemy python libraries are used by the prototype software to create, delete, and access tables in the server. The prototype software allows the user to open any CSV file and to create a table on the server to import the CSV data into."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#data-cleaning-and-exploration",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#data-cleaning-and-exploration",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "4.2 Data Cleaning and Exploration",
    "text": "4.2 Data Cleaning and Exploration\nOnce a user has selected a table using the “Training Manager” toolbar, the table data is converted in a Pandas data frame for cleaning, exploration and model training. In the “DataFrame” toolbar the user is able to convert columns in the data frame to different data types in preparation for training. The user can also display the current data frame information and head data by using the “Info” button on the “DataFrame” toolbar. In addition, the user can can explore the data by generating plots using the “DataFrame” toolbar. Finally, rows with NaN values are dropped from the data frame when the user clicks the “Train Model” button so that the data fed into the algorithms doesn’t produce errors."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#training-models",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#training-models",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "4.3 Training Models",
    "text": "4.3 Training Models\nOnce a user has selected a table using the “Training Manager” toolbar, they can then select columns that they want to use to train models by using the arrow buttons. The user can select whether they want to train a linear regression, classification or clustering model using the “Algorithm” combobox. If either classification or clustering options are chosen they the user can further choose the method for training whether it be: (1) KNeighbors or Decision Tree for classification, or (2) KMeans, Gaussian Mixture, or Spectral Clustering for clustering. User is able to Standardize or Normalize the data for training by making a selection using the “Scaling” combobox. Linear regression or classification models train starting at the test size selected by the user and decrement down by 10 until a test size of 10 has finally been trained. Clustering models train classify clusters based on how many clusters are set by the “Cluster” slider. All training results are displayed for the user in the view window."
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#predict-wine-purchases",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#predict-wine-purchases",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.1 Predict Wine Purchases",
    "text": "5.1 Predict Wine Purchases\nUsing a multivariate linear regression model trained on current company data the amount of wine a given customer will purchase can be predicted with 80% accuracy. This insight could be used to direct specific marketing campaigns and extend wine deals towards customers who are likely to buy more wine based on the model’s predictions. ( See Figure 1 )\n\n\n\nFigure 1. Predict Wine Purchases"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-kids-wine",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-kids-wine",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.2 Relationship Between Kids & Wine",
    "text": "5.2 Relationship Between Kids & Wine\nUsing a KNeighbors classification model trained on company data it is clear to see that there is a partial relationship between the amount of wine bought and how many kids are in the home. It seems that as the amount of children in the home goes up that less wine is purchased. With this insight in mind, marketing should spend less resources advertising wine towards households that have children. ( See Figure 2 )\n\n\n\nFigure 2. Relationship Between Kids & Wine"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-meats-sweets",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-meats-sweets",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.3 Relationship Between Meats & Sweets",
    "text": "5.3 Relationship Between Meats & Sweets\nUsing a KMeans clustering model trained on standardized company data, customers habits can be established between meats and sweets. As seen in Figure 3 customers could be placed in certain categories based on their meats and sweets purchases that would define where marketing efforts may best be spent. For example, those that buy a high amount of meats fit into a category of customer that doesn’t purchase as much sweets and visa versa. So if a customer fits into a category of high meat purchases then less efforts may be spent towards selling them sweets. ( See Figure 3 )\n\n\n\nFigure 3. Relationship Between Meats & Sweets"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-gold-wine",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#relationship-between-gold-wine",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "5.4 Relationship Between Gold & Wine",
    "text": "5.4 Relationship Between Gold & Wine\nUsing a Gaussian Mixture clustering model trained on normalized company data, customers habits can be established between gold and wine. As seen in Figure 4 it seems that customers who purchase less gold products tend to buy more wine but not the other way around. Knowing this, marketing on wine products should not be emphasized for customers that already purchase high amounts of gold products. ( See Figure 4 )\n\n\n\nFigure 4. Relationship Between Gold & Wine"
  },
  {
    "objectID": "posts/dev-design-assignment2/design-dev-assignment2.html#software-video-walkthrough",
    "href": "posts/dev-design-assignment2/design-dev-assignment2.html#software-video-walkthrough",
    "title": "Tkinter Machine Learning Prototype Software",
    "section": "Software Video Walkthrough",
    "text": "Software Video Walkthrough\nVideo"
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#data-preprocessing",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#data-preprocessing",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "1.1 Data Preprocessing",
    "text": "1.1 Data Preprocessing\n\n1.1.1 Step-by-Step Description\n\nNormalization: Each feature in the dataset is scaled to a range between 0 and 1 based on its minimum and maximum values. This normalization ensures uniformity in feature scale, which is essential for neural network models like LSTM, which are sensitive to input scale variations (Listing 1).\n\n\nListing 1. Normalize data while extracting features for model training, validation, and testing.\n\nmain.py\n\n# Define a function to extract features from the data\ndef extract_features(data, features):\n    # Get the specified features\n    data = data[features]\n\n    # Find the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Normalize the data\n    data = (data - data_min) / (data_max - data_min)\n\n    # Split data into train (60%), val (20%), and test (20%) sets\n    train, val, test = (data[:int(0.6 * len(data))].values,\n                        data[int(0.6 * len(data)):int(0.8 * len(data))].values, data[int(0.8 * len(data)):].values)\n\n    # Convert to PyTorch tensors and reshape\n    # The shape of the data should be (batch_size, sequence_length, num_features)\n    train_tensor = torch.tensor(train).float().view(1, -1, len(features))\n    val_tensor = torch.tensor(val).float().view(1, -1, len(features))\n    test_tensor = torch.tensor(test).float().view(1, -1, len(features))\n    # Return the tensors and the min/max values\n    return train_tensor, val_tensor, test_tensor, data_min, data_max\n\n\nHandling Missing Data: Missing data points are interpolated linearly, maintaining the continuity and integrity of the time series, which is essential for accurate forecasting (Listing 2).\n\n\nListing 2. Interpolate missing data.\n\nmain.py\n\n# Set time period\nstart = datetime(2018, 1, 1)\nend = datetime(2018, 12, 31)\n\n# Create Point for Vancouver, BC\nvancouver = Point(49.2497, -123.1193, 70)\n\n# Get daily data for 2018\ndata = Daily(vancouver, start, end)\ndata = data.fetch()\n\ndata = data.interpolate(method='linear')\n\n\nCreating Sequences: The data is then segmented into sequences suitable for training the LSTM model. Through trial-and-error experimentation, it was discovered that a sequence length of 80 is ideal for this data set. Each sequence has consecutive data points that the LSTM will use to learn the weather data’s underlying patterns.\n\n\n\n1.1.2 Justifications\nNormalization and interpolation are crucial for preparing real-world data for machine learning applications, ensuring the model trains on clean and representative data. Creating structured sequences from this data enables the LSTM to effectively capture temporal dependencies, a core aspect of its predictive capability."
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#lstm-model-architecture",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#lstm-model-architecture",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "1.2 LSTM Model Architecture",
    "text": "1.2 LSTM Model Architecture\n\n1.2.1 Step-by-Step Description\n\nxLSTM Architecture: The xLSTM consists of two modified LSTM layers that are contained inside xLSTM blocks:\n\nmLSTM (Matrix LSTM) Layer: Incorporates a matrix memory state and covariance update rule, which allows for full parallelization and efficient memory utilization. This layer is designed to enhance the LSTM’s ability to store and retrieve information dynamically, making it highly effective for tasks requiring complex memory management (Beck, M. et al., 2024 pp. 4-5).\nsLSTM (Scalar LSTM) Layer: Features a scalar memory update and new memory mixing techniques that improve the model’s ability to handle long-range dependencies. Exponential gating is used in this layer to stabilize the training process and prevent gradient explosions (Beck, M. et al., 2024 pp. 3-4).\nxLSTM Blocks: Each xLSTM block (Listing 3), which may contain either or both types of layers, passes information through residual connections, allowing the model to learn deep representations without loss of performance due to vanishing or exploding gradients (Beck, M. et al., 2024 pp. 5-6).\n\n\n\nListing 3. xLSTM block class.\n\nxLSTM.py\n\n# Define the xLSTM block\nclass xLSTM_Block(nn.Module):\n    def __init__(self, block_type, input_size, hidden_size, layers=2, mem_size=None):\n        super(xLSTM_Block, self).__init__()\n        # initialize the dropout layer\n        self.dropout = DynamicDropout()\n        # Create multiple mLSTM and sLSTM layers depending on the block type\n        if block_type == 'mLSTM':\n            # Create multiple mLSTM layers\n            self.layers = nn.ModuleList([mLSTM(input_size if i == 0 else hidden_size, hidden_size, mem_size)\n                                         for i in range(layers)])\n        elif block_type == 'sLSTM':\n            # Create multiple sLSTM layers\n            self.layers = nn.ModuleList([sLSTM(hidden_size, hidden_size) for _ in range(layers)])\n\n    # forward pass\n    def forward(self, x, initial_states):\n        # Initial hidden states\n        hidden_states = self.layers[0].init_hidden()\n        # Loop through the layers\n        for i in range(len(self.layers)):\n            # Forward pass through each layer\n            x, hidden_states = self.layers[i](x, hidden_states)\n            # Apply dropout\n            x = self.dropout(x)\n        # Return the hidden state and the new states\n        return x, hidden_states\n\n\n\n\n\n\n\nSource Code\n\n\n\nView xLSTM.py\n\n\n\nConfiguration: The complexity of the dataset and the depth of features required a configuration of 30 hidden units, 30 matrix memory dimension size for mLSTM layers, and one layer of either mLSTM or sLSTM depending on the corresponding xLSTM block. Through trial and error, this configuration was found to provide the most consistency in terms of accuracy (Listing 4).\n\n\nListing 4. xLSTM configuration.\n\nmain.py\n\n# xLSTM model parameters\nhidden_size = 30\nmem_dim = 30\nlayers = 1\nseq_len = 80\n\n\nDynamic Dropout Layers: Custom Dynamic dropout layers are integrated within the xLSTM blocks to prevent overfitting by randomly omitting subsets of features and hidden states during the training process. This custom layer class adjusts the rate in either direction by a factor of 0.02 for a minimum of 0 and a maximum of 0.1. These adjustments during training are meant to help the model to avoid local minima and ensure stable convergence to the optimal solution. These features are critical for maintaining the model’s performance stability across different datasets and training conditions.\n\n\n\n\n\n\n\nSource Code\n\n\n\nView dynamic_dropout.py\n\n\n\n\n1.2.2 Justifications\nThe architecture is carefully designed to enhance the LSTM’s ability to process and remember information over long sequences, crucial for time-series forecasting. Dropout layers add an essential regularization element, increasing the model’s generalization capabilities."
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#temporal-sequence-input-mechanism",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#temporal-sequence-input-mechanism",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "1.3 Temporal Sequence Input Mechanism",
    "text": "1.3 Temporal Sequence Input Mechanism\nThe train_model() method in xLSTM_model class feeds the training data to the model of the specified sequence length (Listing 5). As previously mentioned, a sequence length of 80 tends to produce the most consistently accurate results. Experimenting with various sequence lengths helps identify the most effective input configuration for maximizing the predictive performance of the model, ensuring that it is tuned to the specific temporal dynamics of the dataset.\n\nListing 5. Train Model method.\n\nxLSTM.py\n\n# Train the model\ndef train_model(self, data, val_data, epochs, seq_len):\n    # initialize the early_stopping object\n    early_stopping = EarlyStopping(patience=50, verbose=True)\n    # loop through the epochs\n    for epoch in range(epochs):\n        # zero the gradients\n        self.optimizer.zero_grad()\n        # initialize the loss\n        loss = 0\n        # loop through the sequence length\n        for t in range(seq_len - 1):\n            # get the input at time t\n            x = data[:, t]\n            # get the target at time t+1\n            y_true = data[:, t + 1, 0]\n            # get the prediction\n            y_pred = self(x)\n            # calculate the loss from the training data\n            loss += self.criterion(y_pred, y_true)\n\n        # validate the model on the validation data\n        val_loss = self.validate(val_data)\n        # print the validation loss\n        print(f'Epoch {epoch} Validation Loss {val_loss.item()}')\n        # call the early stopping object\n        early_stopping(val_loss, self)\n        # if early stopping is triggered\n        if early_stopping.early_stop:\n            # print message\n            print(\"Early stopping\")\n            # stop the training\n            break\n        # calculate the average loss\n        loss.backward()\n        # clip the gradients to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(self.optimizing_parameters, max_norm=1)\n        # update the weights\n        self.optimizer.step()\n        # print the training loss every 10 epochs\n        if epoch % 10 == 0:\n            print(f'Epoch {epoch} Loss {loss.item()}')\n\n    # load the best model before early stopping\n    self.load_state_dict(torch.load('checkpoint.pt'))"
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#training-process",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#training-process",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "1.4 Training Process",
    "text": "1.4 Training Process\n\n1.4.1 Step-by-Step Description\n\nTraining Execution: The model is trained on the preprocessed dataset, using a combination of the Adam optimizer and custom training enhancements like dynamic dropout and early stopping.\nOptimizer: The xLSTM uses an Adam optimizer with weight decay and gradient clipping to optimize training. This setup helps in mitigating the common issues of underfitting and overfitting by regulating the update magnitudes (See init() method in xLSTM_model class).\nEarly Stopping: Utilizes a custom class to halt training when improvements cease as observed by the loss produced by the validation data set. Each time there is an improvement in loss, evaluating on a validation data set during training, the model’s current state in training is saved. Whenever ever losses don’t improve from the recorded minimum the early stopping object’s patience counter is incremented and it triggers the dynamic dropout layer’s rate to increase. Every time the loss improves the patience counter is reset back to 0 and the dynamic dropout layer is triggered to decrease its rate.\n\n\n\n\n\n\n\nSource Code\n\n\n\nView early_stopping.py\n\n\n\nMonitoring Metrics: During training, Smooth L1 Loss is monitored to assess the model’s performance and make necessary adjustments. Smooth L1 Loss, also known as the Huber loss, is a combination of L1 and L2 loss functions. It behaves like L1 loss when the absolute error is large and like L2 loss when the error is small (See init() method in xLSTM_model class).\n\n\n\n1.4.2 Justifications\n\nAdam Optimizer with Weight Decay and Gradient Clipping: The Adam optimizer is chosen for its adaptability, as it customizes learning rates for each parameter, enhancing convergence efficiency across diverse data scales. Incorporating weight decay directly combats overfitting by penalizing larger weights, promoting a simpler, more generalizable model. Gradient clipping is essential to maintain numerical stability, preventing the exploding gradients phenomenon that can disrupt the training of deep networks, especially in architectures like LSTMs that are susceptible to large gradient values.\nEarly Stopping: Early stopping acts as a practical regularization technique, halting training when validation performance ceases to improve, thereby avoiding overfitting. This method not only ensures the model does not overlearn from the training data but also conserves computational resources. By saving the model with the best overall validation losses we can preserve the model at the exact point in the training that it was performing the best. We then load the model’s best saved state after training has finished.\nSmooth L1 Loss: This loss function transitions between L1 loss, which is robust against outliers, and L2 loss, which is sensitive to small error variations, depending on the magnitude of the error. This dual nature makes Smooth L1 Loss ideal for regression tasks in LSTM models, where maintaining the precision of predictions and handling outliers effectively are both crucial for achieving accurate and reliable forecasts."
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#evaluation",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#evaluation",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "1.5 Evaluation",
    "text": "1.5 Evaluation\n\n1.5.1 Step-by-Step Description\n\nTesting: The model’s performance is evaluated on a separate test set to ensure it generalizes well beyond the training data.\nMetrics: Performance metrics such as MAE, RMSE, correlation, and R2 are calculated, providing a comprehensive view of the model’s accuracy and reliability. Predictions are also plotted against the actual test values to allow the user to compare results visually\n\n\n\n1.5.2 Justifications\nUnivariate and multivariate versions of the model are created for performance comparison. The model’s effectiveness is evaluated using a comprehensive set of metrics such as Mean Average Error (MAE), Root Mean Squared Error (RMSE), correlation, and coefficient of determination (R2), offering a nuanced view of its predictive capabilities. Each metric of evaluation was carefully selected to capture a certain aspect of the model’s performance. MAE reveals how well adapted the model is to outliers, RMSE provides a more general view of the model’s performance, correlation tells how well the model responds to trends in the data, and R2 shows how well the model has the learned the variance between the independent variables and the dependent variables indicating how well the model will predict previously unseen samples. These are examined in comparison to the Markov Chain model to reveal that both univariate and multivariate models can be trained to a staggering precision of accuracy (Figure 1).\n\n\n\n\n\n\n\n(a) Plot\n\n\n\n\n\n\n\n\n\n(b) Metrics\n\n\n\n\nFigure 1. Evaluation Results"
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#order-selection",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#order-selection",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "2.1 Order Selection",
    "text": "2.1 Order Selection\n\n2.1.1 Step-by-Step Description\n\nModel Training and Prediction: For each order from a range of 2 to 30, instantiate and train a Markov Chain model. Extend the test data by appending the last ‘order’ elements of the training data to provide necessary context, then use this model to predict outcomes over the adjusted test set.\nPerformance Evaluation: Calculate key performance metrics for each order, including Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Pearson Correlation, and R-squared (R2).\nOptimal Order Determination: Compile the performance metrics into a DataFrame, rank each order based on these metrics, and compute an average ranking for each. The order with the lowest average rank, indicating the best overall performance, is selected as the optimal order for the Markov Chain model.\n\n\n\n2.1.2 Justifications\nThis methodical approach ensures a comprehensive evaluation of how well the Markov Chain model performs across a spectrum of complexity (different orders). By considering multiple metrics and their average ranks, the selection process is robust against the peculiarities of any single metric. This thorough testing and selection strategy ensures that the chosen order is well-suited for the dataset and prediction task, balancing model simplicity and predictive accuracy effectively (Listing 6).\n\nListing 6. Find Best Order method.\n\nmarkov.py\n\n# Define a function to find the best order for the Markov Chain model\ndef find_best_order(train_data, test_data, min_order, max_order):\n    # Initialize a list to store the results\n    results = []\n\n    # Loop through the range of orders\n    for order in range(min_order, max_order + 1):\n        # Create and train the Markov Chain model\n        model = MarkovChain(order)\n        model.fit(train_data)\n\n        # Add the last 'order' elements of the training data to the test data\n        # to ensure that the test data is long enough for the predictions\n        # for the test data in the LSTM models\n        test_data = np.concatenate((train_data[-order:], test_data))\n\n        # Make predictions on the test set, feeding the model order elements at a time\n        markov_pred = [model.predict(test_data[i - order:i]) for i in range(order, len(test_data))]\n\n        # Calculate the metrics\n        mse = mean_squared_error(test_data[order:], markov_pred)\n        mae = mean_absolute_error(test_data[order:], markov_pred)\n        corr, _ = pearsonr(test_data[order:], markov_pred)\n        r2 = r2_score(test_data[order:], markov_pred)\n\n        # Store the results\n        results.append((order, sqrt(mse), mae, corr, r2))\n\n    # Convert the results to a DataFrame\n    results_df = pd.DataFrame(results, columns=['order', 'rmse', 'mae', 'corr', 'r2'])\n\n    # Rank the results\n    results_df['rmse_rank'] = results_df['rmse'].rank()\n    results_df['mae_rank'] = results_df['mae'].rank()\n    results_df['corr_rank'] = results_df['corr'].rank(ascending=False)\n    results_df['r2_rank'] = results_df['r2'].rank(ascending=False)\n\n    # Calculate the average rank\n    results_df['avg_rank'] = results_df[['rmse_rank', 'mae_rank', 'corr_rank', 'r2_rank']].mean(axis=1)\n\n    # Return the order with the lowest average rank\n    best_order = results_df.loc[results_df['avg_rank'].idxmin(), 'order']\n\n    # Return the best order for the Markov Chain model\n    return best_order\n\n\n\n\n\n\n\nSource Code\n\n\n\nView markov.py"
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#transition-probabilities",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#transition-probabilities",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "2.2 Transition Probabilities",
    "text": "2.2 Transition Probabilities\n\n2.2.1 Step-by-Step Description\n\nCalculation of Transition Probabilities: The probabilities of transitioning from one state to another were calculated based on historical data, forming a transition matrix essential for the Markov Chain operations (Listing 7).\n\n\nListing 7. Calculate transition matrix.\n\nmarkov.py\n\n# Fit the Markov chain to the data\ndef fit(self, data):\n    # Get the unique states in the data, and convert them to strings\n    self.states = np.unique(data).astype(str)\n    # Initialize the transition matrix, and set all values to zero\n    # with the states as the row and column indices\n    self.transition_matrix = pd.DataFrame(np.zeros((len(self.states), len(self.states))), index=self.states, columns=self.states)\n\n    # Iterate through the data, and update the transition matrix\n    for i in range(len(data) - self.order):\n        # Get the current and next states\n        current_state = data[i].astype(str)\n        # Next state is order steps ahead\n        next_state = data[i + self.order].astype(str)\n        # Update the transition matrix at the current and next states\n        self.transition_matrix.loc[current_state, next_state] += 1\n\n    # Normalize the transition matrix to values between 0 and 1\n    # Sum up the rows of the transition matrix\n    # Add a small value to the denominator to avoid division by zero\n    row_sums = self.transition_matrix.sum(axis=1) + 1e-10\n    # Divide each row by the row sum to get the transition probabilities\n    self.transition_matrix = self.transition_matrix.div(row_sums, axis=0)\n\n\nState Space Simplification: Given the continuous nature of temperature values, the state space was large. Rounding these values to integers significantly reduced the state space, allowing for more manageable and accurate transition probability calculations.\n\n\n\n2.2.2 Justifications\nThe simplification of the state space by rounding continuous values to integers does blunt of model precision in predicting exact float values. However, this is an acceptable loss when taking into context that the overall model accuracy is greatly improved. By narrowing the state space, the model can calculate transition probabilities that lead to much closer predictions to the real-world data (Figure 2).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteger Values\n\n\n\n\n\n\n\nFloat Values\n\n\n\n\nFigure 2. Integer vs Float Values"
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#temporal-sequence-input",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#temporal-sequence-input",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "2.3 Temporal Sequence Input",
    "text": "2.3 Temporal Sequence Input\nThe sequence length for input into the Markov Chain model was adjusted during the experimental phase to identify the optimal configuration for capturing relevant temporal dependencies, aligning with the chosen order of 2. Optimizing sequence length ensures that the model has sufficient historical context to make accurate predictions while avoiding the overhead of unnecessary data processing."
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#prediction-mechanism",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#prediction-mechanism",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "2.4 Prediction Mechanism",
    "text": "2.4 Prediction Mechanism\nThe Markov Chain model uses the transition matrix to predict future values based on a given sequence of historical data. This method utilizes the established probabilities to forecast the next most likely state (Listing 8). This approach leverages the probabilistic nature of Markov models to provide a straightforward and effective means of time-series forecasting, particularly suitable for datasets with clear transitional patterns.\n\nListing 8. Predict method.\n\nmarkov.py\n\n# Predict the next state in the sequence\ndef predict(self, sequence):\n    # Get the current state\n    state = sequence[0].astype(str)\n    # If the state is not in the transition matrix\n    if state not in self.transition_matrix.index:\n        # Return the first state in the sequence as the prediction (fallback)\n        return sequence[0]\n    # Return the state with the highest probability in the transition matrix\n    return self.states[np.argmax(self.transition_matrix.loc[state])].astype(sequence.dtype)"
  },
  {
    "objectID": "posts/xlstm-vs-markov/xlstm-vs-markov.html#evaluation-1",
    "href": "posts/xlstm-vs-markov/xlstm-vs-markov.html#evaluation-1",
    "title": "Time Series Forecasting xLSTM vs Markov Chain",
    "section": "2.5 Evaluation",
    "text": "2.5 Evaluation\n\n2.5.1 Step-by-Step Description\n\nPerformance Comparison: The performance of the Markov Chain model was evaluated against the LSTM models using the same test set. Metrics like Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R2 were used for a comprehensive assessment.\nResults Analysis: While the xLSTM models generally performed better due to their ability to model complex dependencies, the Markov Chain model offered competitive accuracy with much lower computational requirements (Figure 1).\n\n\n\n2.5.2 Justifications\nThe best performing Markov model (trained on int float values) is compared with some of the best trained xLSTM models (univariate and multivariate) to reveal that on average the xLSTM models proved more accurate than the Markov chain model for this particular dataset. This isn’t entirely surprising given that the xLSTM model’s complexity allow it to capture the intricate relationships in the time series data better. However, the Markov chain model is much more light weight in time and space complexity. Also, the implementation of Markov chain is much simpler and straightforward requiring little experimentation to achieve reasonably accurate performance. While the xLSTM models required a large amount of trial-and-error experimentation with hyper parameters to find optimal and stable results. Overall if the accuracy of Markov chain is within acceptable ranges of error than the clear choice in model implementation would be Markov. Markov is a more cost-effective solution in terms of time, resources, and deployment. But if the solution is less tolerant of error, then xLSTM is a preferable implementation for accuracy in time series data. (Figure 1)."
  },
  {
    "objectID": "posts/open-stack/open-stack.html#keystone-identity-service",
    "href": "posts/open-stack/open-stack.html#keystone-identity-service",
    "title": "OpenStack Virtualization",
    "section": "1.1 Keystone Identity Service",
    "text": "1.1 Keystone Identity Service\n\n\n\nFigure 1. Keystone - OpenRC Script"
  },
  {
    "objectID": "posts/open-stack/open-stack.html#glance-image-service",
    "href": "posts/open-stack/open-stack.html#glance-image-service",
    "title": "OpenStack Virtualization",
    "section": "1.2 Glance Image Service",
    "text": "1.2 Glance Image Service\n\n\n\nFigure 2. Glance - Verification Operation"
  },
  {
    "objectID": "posts/open-stack/open-stack.html#nova-compute-service",
    "href": "posts/open-stack/open-stack.html#nova-compute-service",
    "title": "OpenStack Virtualization",
    "section": "1.3 Nova Compute Service",
    "text": "1.3 Nova Compute Service\n\n\n\nFigure 3. Nova - Verification Operation Part 1\n\n\n\n\n\nFigure 4. Nova - Verification Operation Part 2"
  },
  {
    "objectID": "posts/open-stack/open-stack.html#neutron-networking-service",
    "href": "posts/open-stack/open-stack.html#neutron-networking-service",
    "title": "OpenStack Virtualization",
    "section": "1.4 Neutron Networking Service",
    "text": "1.4 Neutron Networking Service\n\n\n\nFigure 5. Neutron - Verification Operation"
  },
  {
    "objectID": "posts/open-stack/open-stack.html#horizon-dashboard",
    "href": "posts/open-stack/open-stack.html#horizon-dashboard",
    "title": "OpenStack Virtualization",
    "section": "1.5 Horizon Dashboard",
    "text": "1.5 Horizon Dashboard\n\n\n\nFigure 6. Horizon - Dashboard"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#purpose-of-document",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#purpose-of-document",
    "title": "Linear Regression, Classification, Clustering",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide a comparison between different AI models for a given dataset to determine which models are most accurate. This document also explores what measures can be taken to improve accuracy in various AI models."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#scope",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#scope",
    "title": "Linear Regression, Classification, Clustering",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an exploratory examination of a dataset to determine how best to sample and clean the data for AI training and testing purposes. Various data visualizations are needed to properly understand the dataset and how best to proceed with training models. Training of various models and algorithms are required to produce sufficient comparisons with the ultimate goal of improving accuracy."
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#descriptive-analysis",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#descriptive-analysis",
    "title": "Linear Regression, Classification, Clustering",
    "section": "2.1 Descriptive Analysis",
    "text": "2.1 Descriptive Analysis\nThe dataset used in this project consists of available independent variables for a variety of cars to ascertain how they affect the price. The chosen dataset contains 26 columns and 205 rows of data with no null values. It is a sufficient dataset in terms of size and types of data for use in training univariate & multivariate linear regression, classification and clustering models.\nThe Columns\n\nCar_ID : Unique id of each observation (Integer)\nSymboling : Its assigned insurance risk rating, A value of +3 - Indicates that the auto is risky, -3 that it is probably pretty safe.\ncarCompany : Name of car company (Categorical)\nfueltype : Car fuel type i.e gas or diesel (Categorical)\naspiration : Aspiration used in a car (Categorical)\ndoornumber : Number of doors in a car (Categorical)\ncarbody : Body of car (Categorical)\ndrivewheel : Type of drive wheel (Categorical)\nenginelocation : Location of car engine (Categorical)\nwheelbase : Wheelbase of car (Numeric)\ncarlength : Length of car (Numeric)\ncarwidth : Width of car (Numeric)\ncarheight : Height of car (Numeric)\ncurbweight : The weight of a car without occupants or baggage. (Numeric)\nenginetype : Type of engine. (Categorical)\ncylindernumber : Cylinder placed in the car (Numeric)\nenginesize : Size of car (Numeric)\nfuelsystem : Fuel system of car (Categorical)\nboreratio : Boreratio of car (Numeric)\nstroke : Stroke or volume inside the engine (Numeric)\ncompressionratio : Compression ratio of car (Numeric)\nhorsepower : Horsepower (Numeric)\npeakrpm : Car peak rpm (Numeric)\ncitympg : Mileage in city (Numeric)\nhighwaympg : Mileage on highway (Numeric)\nprice(Dependent variable) : Price of car (Numeric)\n\n\n\nCode\n# Import libraries for analysis and plotting\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Save data in Pandas dataframe\ndataset = pd.read_csv(\"CarPrice_Assignment.csv\")\n\n# Print how many rows and columns are in dataset\nprint('Dataset Shape:',dataset.shape)\n\n# Turn of max columns so that head() displays all columns in dataset\npd.set_option('display.max_columns', None)\n\npd.set_option('display.max_rows', 5)\n\n# Display 1st five entries of dataset\ndataset.head()\n\n\nDataset Shape: (205, 26)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\nfour\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\nsix\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\nfour\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\nfive\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n\nCode\n# Print data types and how many null values are present\ndataset.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   car_ID            205 non-null    int64  \n 1   symboling         205 non-null    int64  \n 2   CarName           205 non-null    object \n 3   fueltype          205 non-null    object \n 4   aspiration        205 non-null    object \n 5   doornumber        205 non-null    object \n 6   carbody           205 non-null    object \n 7   drivewheel        205 non-null    object \n 8   enginelocation    205 non-null    object \n 9   wheelbase         205 non-null    float64\n 10  carlength         205 non-null    float64\n 11  carwidth          205 non-null    float64\n 12  carheight         205 non-null    float64\n 13  curbweight        205 non-null    int64  \n 14  enginetype        205 non-null    object \n 15  cylindernumber    205 non-null    object \n 16  enginesize        205 non-null    int64  \n 17  fuelsystem        205 non-null    object \n 18  boreratio         205 non-null    float64\n 19  stroke            205 non-null    float64\n 20  compressionratio  205 non-null    float64\n 21  horsepower        205 non-null    int64  \n 22  peakrpm           205 non-null    int64  \n 23  citympg           205 non-null    int64  \n 24  highwaympg        205 non-null    int64  \n 25  price             205 non-null    float64\ndtypes: float64(8), int64(8), object(10)\nmemory usage: 41.8+ KB\n\n\n\n\nCode\n# Display some descriptive statistics\ndataset.describe().round(2)\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\ncount\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n205.00\n\n\nmean\n103.00\n0.83\n98.76\n174.05\n65.91\n53.72\n2555.57\n126.91\n3.33\n3.26\n10.14\n104.12\n5125.12\n25.22\n30.75\n13276.71\n\n\nstd\n59.32\n1.25\n6.02\n12.34\n2.15\n2.44\n520.68\n41.64\n0.27\n0.31\n3.97\n39.54\n476.99\n6.54\n6.89\n7988.85\n\n\nmin\n1.00\n-2.00\n86.60\n141.10\n60.30\n47.80\n1488.00\n61.00\n2.54\n2.07\n7.00\n48.00\n4150.00\n13.00\n16.00\n5118.00\n\n\n25%\n52.00\n0.00\n94.50\n166.30\n64.10\n52.00\n2145.00\n97.00\n3.15\n3.11\n8.60\n70.00\n4800.00\n19.00\n25.00\n7788.00\n\n\n50%\n103.00\n1.00\n97.00\n173.20\n65.50\n54.10\n2414.00\n120.00\n3.31\n3.29\n9.00\n95.00\n5200.00\n24.00\n30.00\n10295.00\n\n\n75%\n154.00\n2.00\n102.40\n183.10\n66.90\n55.50\n2935.00\n141.00\n3.58\n3.41\n9.40\n116.00\n5500.00\n30.00\n34.00\n16503.00\n\n\nmax\n205.00\n3.00\n120.90\n208.10\n72.30\n59.80\n4066.00\n326.00\n3.94\n4.17\n23.00\n288.00\n6600.00\n49.00\n54.00\n45400.00"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#cleaning",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#cleaning",
    "title": "Linear Regression, Classification, Clustering",
    "section": "2.2 Cleaning",
    "text": "2.2 Cleaning\nMultiple columns are object data types but for classification and clustering purposes they were converted to category types. Column 16, “cylindernumber”, values were changed from strings to integers to assist in training some of the linear regression models.\n\n\nCode\n# Convert object data types to category types\ndataset['CarName'] = dataset['CarName'].astype('category')\ndataset['fueltype'] = dataset['fueltype'].astype('category')\ndataset['aspiration'] = dataset['aspiration'].astype('category')\ndataset['doornumber'] = dataset['doornumber'].astype('category')\ndataset['carbody'] = dataset['carbody'].astype('category')\ndataset['drivewheel'] = dataset['drivewheel'].astype('category')\ndataset['enginelocation'] = dataset['enginelocation'].astype('category')\ndataset['enginetype'] = dataset['enginetype'].astype('category')\ndataset['fuelsystem'] = dataset['fuelsystem'].astype('category')\ndataset['curbweight'] = dataset['curbweight'].astype('int')\n\n# Convert strings to integers in cylindernumber column to potentially use in the regression models\ndataset['cylindernumber'] = dataset['cylindernumber'].replace(['two'], 2).replace(['three'], 3)\\\n.replace(['four'], 4).replace(['five'], 5).replace(['six'], 6).replace(['eight'], 8).replace(['twelve'], 12)\n\ndataset.head()\n\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginetype\ncylindernumber\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n64.1\n48.8\n2548\ndohc\n4\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n65.5\n52.4\n2823\nohcv\n6\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n66.2\n54.3\n2337\nohc\n4\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n66.4\n54.3\n2824\nohc\n5\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n\n\n\n\n\nCode\n# Print new data types and how many null values are present\ndataset.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   car_ID            205 non-null    int64   \n 1   symboling         205 non-null    int64   \n 2   CarName           205 non-null    category\n 3   fueltype          205 non-null    category\n 4   aspiration        205 non-null    category\n 5   doornumber        205 non-null    category\n 6   carbody           205 non-null    category\n 7   drivewheel        205 non-null    category\n 8   enginelocation    205 non-null    category\n 9   wheelbase         205 non-null    float64 \n 10  carlength         205 non-null    float64 \n 11  carwidth          205 non-null    float64 \n 12  carheight         205 non-null    float64 \n 13  curbweight        205 non-null    int64   \n 14  enginetype        205 non-null    category\n 15  cylindernumber    205 non-null    int64   \n 16  enginesize        205 non-null    int64   \n 17  fuelsystem        205 non-null    category\n 18  boreratio         205 non-null    float64 \n 19  stroke            205 non-null    float64 \n 20  compressionratio  205 non-null    float64 \n 21  horsepower        205 non-null    int64   \n 22  peakrpm           205 non-null    int64   \n 23  citympg           205 non-null    int64   \n 24  highwaympg        205 non-null    int64   \n 25  price             205 non-null    float64 \ndtypes: category(9), float64(8), int64(9)\nmemory usage: 36.1 KB"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#visualizations",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#visualizations",
    "title": "Linear Regression, Classification, Clustering",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nA pairplot ( See Figure 1 ) provides a quick overview of how the variables relate, showing some possibilities for training models. The ‘carbody’ ( See Figure 2 ) and ‘fueltype’ ( See Figure 3 ) columns show promise for use with the classification and clustering models ( See Figure 4 (f) & Figure 4 (g) ). Clear linear relationships exist between ‘carlength’, ‘carwidth’, ‘curbweight’, ‘enginesize’, ‘cylindernumber’ and ‘horsepower’ independent variables and the dependent variable ‘price’ ( See Figure 4 (a) - (f) ).\n\n\nCode\n# Display a pairplot to quickly see how varaiables relate to one another with 'fueltype' hue\nsns.pairplot(dataset, kind= 'scatter', hue= 'fueltype')\nplt.show()\n\n\n\n\n\nFigure 1. Pairplot with Fuel Type Hue\n\n\n\n\n\n\nCode\n# display pie chart data for carbody\ndataset['carbody'].value_counts().plot.pie(autopct='%1.3f%%');\n\n# Display relationship between body style and price\ndataset.groupby('carbody')['price'].mean().round(2)\n\n\n\n\n\nFigure 2. Carbody Pie Plot\n\n\n\n\n\n\ncarbody\nconvertible    21890.50\nhardtop        22208.50\nhatchback      10376.65\nsedan          14344.27\nwagon          12371.96\nName: price, dtype: float64\n\n\n\n\nCode\n# display pie chart data for fueltype\ndataset['fueltype'].value_counts().plot.pie(autopct='%1.3f%%');\n\n# Display ralationship between body style and price\ndataset.groupby('fueltype')['price'].mean().round(2)\n\n\n\n\n\nFigure 3. Fuel Type Pie Plot\n\n\n\n\n\n\nfueltype\ndiesel    15838.15\ngas       12999.80\nName: price, dtype: float64\n\n\n\n\nCode\n# Carlength has moderate relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"carlength\", y=\"price\");\n\n# Carwidth has moderate relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"carwidth\", y=\"price\");\n\n# Carweight has moderate/strong relationship to price\nplt.figure(figsize=(6,6));\nsns.regplot(data=dataset, x=\"curbweight\", y=\"price\");\n\n# Engine size has strong relationship to price\nsns.lmplot(data=dataset, x=\"enginesize\", y=\"price\", hue='fueltype', height=6)\n\n# Horsepower has strong relationship to price for both fuel types\nsns.lmplot(data=dataset, x=\"horsepower\", y=\"price\", hue='fueltype', height=6)\n\n# Cylinder number has moderate/strong relationship to price\nsns.jointplot(data=dataset, x=\"cylindernumber\", y=\"price\", kind=\"reg\");\n\n# Clear classification relationship between compressionratio and fueltype\ng = sns.jointplot(data=dataset, x=\"compressionratio\", y=\"price\", hue='fueltype');\ng.plot_joint(sns.kdeplot, hue='fueltype');\n\n\n\n\n\n\n\n\n(a) Car Length vs Price\n\n\n\n\n\n\n\n(b) Car Width vs Price\n\n\n\n\n\n\n\n\n\n(c) Curb Weight vs Price\n\n\n\n\n\n\n\n(d) Engine Size vs Price\n\n\n\n\n\n\n\n\n\n(e) Horsepower vs Price\n\n\n\n\n\n\n\n(f) Cylinder Number vs Price\n\n\n\n\n\n\n\n\n\n(g) Compression Ratio vs Price\n\n\n\n\nFigure 4. Plots for visualizing"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#univariate-models",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#univariate-models",
    "title": "Linear Regression, Classification, Clustering",
    "section": "3.1 Univariate Models",
    "text": "3.1 Univariate Models\nMultiple univariate models were trained for comparison using a custom Linear Regression training function. Models were trained with ‘carlength’, ‘carwidth’, ‘curbweight’, ‘cylindernumber’, ‘enginesize’ and ‘horsepower’ independent variables. In most cases the model accuracy was the best with a 70% training and 30% testing split ( See Items 3.1.1 - 3.1.4 ). However, with engine size and horsepower models more accuracy was achieved with an 80% training and 20% testing split ( See Items 3.1.5 - 3.1.6 ).\n\n\nCode\n# Import regression training libraries and packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\n# Linear Regression training function that takes in X and Y arguments and displays results\ndef myLinRegModel(x, y, testSize):\n    \n    # While loop to iterate every 10% from given test size\n    while testSize&gt;0:\n        \n        # Splitting data into training and testing variables using the values passed into function\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=(testSize/100), random_state=0)\n\n        # Training model with LinearRegression function and training data\n        regressor = LinearRegression()\n        regressor.fit(x_train, y_train)\n    \n        # Print test size of current iteration\n        print('Test Size:', testSize, '%\\n')\n\n        # Print intercept and CoEfficient values of model\n        print(\"a =\", regressor.intercept_)\n        print(\"b =\", regressor.coef_)\n\n        # Test the trained model with test data and store in variable\n        y_pred = regressor.predict(x_test)\n\n        # Display predicted values next to actual values for comparison\n        df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n        print(df)\n    \n        # Display accuracy of model predictions in the form of Mean Absolute Error, Mean Squared Error,\n        # Root Mean Squared Error using the difference between actual and predicted values\n        print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n        print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n        print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n        print('R2 Score: ', metrics.r2_score(y_test,y_pred)*100, '%\\n', sep='')\n        \n        # Decrease test size by 10\n        testSize -= 10\n\n\n\n3.1.1 Car Length vs Price\n\n\nCode\n# Carlength Column\ncarLength = dataset.iloc[:, 10:-15].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carLength, price, 30)\n\n\nTest Size: 30 %\n\na = -63541.11342037626\nb = [440.33603117]\n     Actual     Predicted\n0    6795.0   6516.349139\n1   15750.0  19153.993234\n..      ...           ...\n60   6479.0    131.476687\n61  15510.0  18625.589997\n\n[62 rows x 2 columns]\nMean Absolute Error: 3981.584437549869\nMean Squared Error: 32715085.38508641\nRoot Mean Squared Error: 5719.71025359558\nR2 Score: 50.45973947401606%\n\nTest Size: 20 %\n\na = -63738.09854118214\nb = [441.42013341]\n     Actual     Predicted\n0    6795.0   6491.844685\n1   15750.0  19160.602514\n..      ...           ...\n39  45400.0  24192.792035\n40   8916.5   5079.300258\n\n[41 rows x 2 columns]\nMean Absolute Error: 4528.295484564718\nMean Squared Error: 43469954.12056989\nRoot Mean Squared Error: 6593.174813439266\nR2 Score: 43.84913586892223%\n\nTest Size: 10 %\n\na = -66742.8631493445\nb = [459.19742348]\n     Actual     Predicted\n0    6795.0   6315.446927\n1   15750.0  19494.412981\n..      ...           ...\n19   6488.0   6131.767957\n20   9959.0  12698.291113\n\n[21 rows x 2 columns]\nMean Absolute Error: 3945.9317457788898\nMean Squared Error: 29396652.85426334\nRoot Mean Squared Error: 5421.868022578873\nR2 Score: 26.410928631260454%\n\n\n\n\n\n3.1.2 Car Width vs Price\n\n\nCode\n# Carwidth Column\ncarWidth = dataset.iloc[:, 11:-14].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWidth, price, 30)\n\n\nTest Size: 30 %\n\na = -172630.60948546475\nb = [2822.14912394]\n     Actual     Predicted\n0    6795.0   8551.364271\n1   15750.0  15042.307256\n..      ...           ...\n60   6479.0   7704.719534\n61  15510.0  15042.307256\n\n[62 rows x 2 columns]\nMean Absolute Error: 3036.57768015824\nMean Squared Error: 22710512.087679498\nRoot Mean Squared Error: 4765.554751304354\nR2 Score: 65.60960571372881%\n\nTest Size: 20 %\n\na = -172526.22359994025\nb = [2819.03318321]\n     Actual     Predicted\n0    6795.0   8455.706762\n1   15750.0  14939.483084\n..      ...           ...\n39  45400.0  30444.165591\n40   8916.5   6764.286852\n\n[41 rows x 2 columns]\nMean Absolute Error: 3674.9155902799166\nMean Squared Error: 31370813.470780104\nRoot Mean Squared Error: 5600.965405247573\nR2 Score: 59.47779746918066%\n\nTest Size: 10 %\n\na = -181627.87173597398\nb = [2957.89666431]\n     Actual     Predicted\n0    6795.0   8269.094113\n1   15750.0  15072.256441\n..      ...           ...\n19   6488.0   6494.356114\n20   9959.0  11818.570110\n\n[21 rows x 2 columns]\nMean Absolute Error: 3197.214272696799\nMean Squared Error: 19783555.22362383\nRoot Mean Squared Error: 4447.87086409035\nR2 Score: 50.47553663690271%\n\n\n\n\n\n3.1.3 Curb Weight vs Price\n\n\nCode\n# Curbweight Column\ncarWeight = dataset.iloc[:, 13:-12].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(carWeight, price, 30)\n\n\nTest Size: 30 %\n\na = -18679.037713196016\nb = [12.40359272]\n     Actual     Predicted\n0    6795.0   4949.806413\n1   15750.0  20404.682939\n..      ...           ...\n60   6479.0   2568.316611\n61  15510.0  15530.071001\n\n[62 rows x 2 columns]\nMean Absolute Error: 2670.404540077829\nMean Squared Error: 18443910.151758883\nRoot Mean Squared Error: 4294.637371392244\nR2 Score: 72.0704958192619%\n\nTest Size: 20 %\n\na = -18833.605447325583\nb = [12.47623193]\n     Actual     Predicted\n0    6795.0   4933.616372\n1   15750.0  20479.001353\n..      ...           ...\n39  45400.0  27515.596159\n40   8916.5   4546.853183\n\n[41 rows x 2 columns]\nMean Absolute Error: 3256.3206631106873\nMean Squared Error: 25249391.034916148\nRoot Mean Squared Error: 5024.877215904499\nR2 Score: 67.3849408384091%\n\nTest Size: 10 %\n\na = -19880.405624111718\nb = [12.9537027]\n     Actual     Predicted\n0    6795.0   4796.398026\n1   15750.0  20936.711595\n..      ...           ...\n19   6488.0   6221.305324\n20   9959.0  10819.869783\n\n[21 rows x 2 columns]\nMean Absolute Error: 2695.197926817389\nMean Squared Error: 11737364.677960433\nRoot Mean Squared Error: 3425.9837533123873\nR2 Score: 70.61768320191304%\n\n\n\n\n\n3.1.4 Cylinder Number vs Price\n\n\nCode\n# Cylinder Number Column\ncylinderNumber = dataset.iloc[:, 15:-10].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(cylinderNumber, price, 30)\n\n\nTest Size: 30 %\n\na = -8750.74345729567\nb = [5045.13677503]\n     Actual     Predicted\n0    6795.0  11429.803643\n1   15750.0  21520.077193\n..      ...           ...\n60   6479.0  11429.803643\n61  15510.0  11429.803643\n\n[62 rows x 2 columns]\nMean Absolute Error: 3944.3868255082953\nMean Squared Error: 26684225.038138304\nRoot Mean Squared Error: 5165.677597192676\nR2 Score: 59.59223566856468%\n\nTest Size: 20 %\n\na = -9046.162097201766\nb = [5112.35112126]\n     Actual     Predicted\n0    6795.0  11403.242388\n1   15750.0  21627.944630\n..      ...           ...\n39  45400.0  31852.646873\n40   8916.5  11403.242388\n\n[41 rows x 2 columns]\nMean Absolute Error: 4280.5628888250285\nMean Squared Error: 32605207.611888204\nRoot Mean Squared Error: 5710.096987958103\nR2 Score: 57.88330998687709%\n\nTest Size: 10 %\n\na = -10564.254121382277\nb = [5479.84028365]\n     Actual     Predicted\n0    6795.0  11355.107013\n1   15750.0  22314.787580\n..      ...           ...\n19   6488.0  11355.107013\n20   9959.0  11355.107013\n\n[21 rows x 2 columns]\nMean Absolute Error: 3464.088015146232\nMean Squared Error: 18346836.560473613\nRoot Mean Squared Error: 4283.32073985519\nR2 Score: 54.072095495610604%\n\n\n\n\n\n3.1.5 Engine Size vs Price\n\n\nCode\n# Engine Size Column\nengineSize = dataset['enginesize'].values.reshape(-1, 1)\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(engineSize, price, 30)\n\n\nTest Size: 30 %\n\na = -7574.131488222356\nb = [163.29075344]\n     Actual     Predicted\n0    6795.0   7285.327074\n1   15750.0  18715.679815\n..      ...           ...\n60   6479.0   7448.617828\n61  15510.0  12184.049678\n\n[62 rows x 2 columns]\nMean Absolute Error: 2898.9726929694702\nMean Squared Error: 14541824.65222288\nRoot Mean Squared Error: 3813.374444271488\nR2 Score: 77.97940083865093%\n\nTest Size: 20 %\n\na = -7613.370926304753\nb = [164.31545176]\n     Actual     Predicted\n0    6795.0   7339.335184\n1   15750.0  18841.416808\n..      ...           ...\n39  45400.0  42338.526410\n40   8916.5   7175.019732\n\n[41 rows x 2 columns]\nMean Absolute Error: 3195.031241401546\nMean Squared Error: 16835544.028987687\nRoot Mean Squared Error: 4103.113942969131\nR2 Score: 78.25324722629195%\n\nTest Size: 10 %\n\na = -8207.420855494747\nb = [169.490971]\n     Actual     Predicted\n0    6795.0   7216.257505\n1   15750.0  19080.625475\n..      ...           ...\n19   6488.0   7385.748476\n20   9959.0  10436.585954\n\n[21 rows x 2 columns]\nMean Absolute Error: 2877.111549011615\nMean Squared Error: 12997474.409783443\nRoot Mean Squared Error: 3605.2010221045157\nR2 Score: 67.46323206602058%\n\n\n\n\n\n3.1.6 Horsepower vs Price\n\n\nCode\n# Horsepower Column\nhorsepower = dataset.iloc[:, 21:-4].values\n\n# Price Column\nprice = dataset.iloc[:, 25].values.round(2)\n\n# Call custom regression model function with 30% test size\nmyLinRegModel(horsepower, price, 30)\n\n\nTest Size: 30 %\n\na = -4438.686268723588\nb = [170.53827527]\n     Actual     Predicted\n0    6795.0   7157.916450\n1   15750.0  22165.284674\n..      ...           ...\n60   6479.0   5452.533697\n61  15510.0  14320.524011\n\n[62 rows x 2 columns]\nMean Absolute Error: 3518.2488303322393\nMean Squared Error: 25821021.51495541\nRoot Mean Squared Error: 5081.438921698795\nR2 Score: 60.89937966412712%\n\nTest Size: 20 %\n\na = -4053.153036276188\nb = [166.64923709]\n     Actual     Predicted\n0    6795.0   7278.995086\n1   15750.0  21944.127950\n..      ...           ...\n39  45400.0  26610.306588\n40   8916.5   7612.293560\n\n[41 rows x 2 columns]\nMean Absolute Error: 3733.6933754512147\nMean Squared Error: 29626244.692692798\nRoot Mean Squared Error: 5442.999604325983\nR2 Score: 61.73128603174041%\n\nTest Size: 10 %\n\na = -4796.241165629246\nb = [174.95075436]\n     Actual     Predicted\n0    6795.0   7100.410131\n1   15750.0  22496.076514\n..      ...           ...\n19   6488.0   6050.705605\n20   9959.0  15498.046340\n\n[21 rows x 2 columns]\nMean Absolute Error: 3839.1982159225827\nMean Squared Error: 26172943.363739382\nRoot Mean Squared Error: 5115.949898478227\nR2 Score: 34.48088778430891%"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#multivariate-models",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#multivariate-models",
    "title": "Linear Regression, Classification, Clustering",
    "section": "3.2 Multivariate Models",
    "text": "3.2 Multivariate Models\nMultiple univariate models were trained for comparison using a custom Linear Regression training function. Accuracy varies in each model with changes in training/test splits and would most likely would be benefitted with more rows of data ( See Items 3.2.1 - 3.2.3 ). The highest accurracy is seen with the model that takes in the most columns for the independent variables ( See Items 3.2.3 ).\n\n3.2.1 Carlength, Carwidth, Curbweight vs Price\n\n\nCode\n# Create copy of dataset and drop all columns not used for multivariate regression models\ndatasetCopy = dataset\ndatasetCopy.drop(['carheight', 'enginetype', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio'],\\\ninplace=True, axis=1)\n\n# Store carlength, carwidth & curbweight columns in X\nX1 = datasetCopy.iloc[:, 10:-7].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X1, price, 30)\n\n\nTest Size: 30 %\n\na = -36739.21951780665\nb = [-208.48890057  764.98885537   13.97180015]\n     Actual     Predicted\n0    6795.0   5818.760203\n1   15750.0  19003.466111\n..      ...           ...\n60   6479.0   5929.766976\n61  15510.0  13762.735333\n\n[62 rows x 2 columns]\nMean Absolute Error: 2458.5442776902337\nMean Squared Error: 16492573.815910544\nRoot Mean Squared Error: 4061.1049993703123\nR2 Score: 75.02539290462342%\n\nTest Size: 20 %\n\na = -44634.67127094674\nb = [-188.42001434  856.204069     13.34802623]\n     Actual     Predicted\n0    6795.0   5783.995638\n1   15750.0  18977.251262\n..      ...           ...\n39  45400.0  29066.672270\n40   8916.5   5459.428429\n\n[41 rows x 2 columns]\nMean Absolute Error: 2943.0381053387778\nMean Squared Error: 22423198.502769\nRoot Mean Squared Error: 4735.313981434494\nR2 Score: 71.03558082852051%\n\nTest Size: 10 %\n\na = -51021.53652492876\nb = [-194.19115484  961.28023332   13.59661598]\n     Actual     Predicted\n0    6795.0   5698.395155\n1   15750.0  19277.437054\n..      ...           ...\n19   6488.0   6694.931234\n20   9959.0  10475.100811\n\n[21 rows x 2 columns]\nMean Absolute Error: 2357.566870487648\nMean Squared Error: 9101964.422986511\nRoot Mean Squared Error: 3016.9462081691995\nR2 Score: 77.21491923452972%\n\n\n\n\n\n3.2.2 Cylinder Number, Engine Size, Horsepower vs Price\n\n\nCode\n# Store cylindernumber, enginesize & horsepower columns in X\nX2 = datasetCopy.iloc[:, 13:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X2, price, 30)\n\n\nTest Size: 30 %\n\na = -6717.131795698624\nb = [-875.82889691  133.38667558   65.31455209]\n     Actual     Predicted\n0    6795.0   6359.129636\n1   15750.0  19692.219717\n..      ...           ...\n60   6479.0   5839.370791\n61  15510.0  13103.941091\n\n[62 rows x 2 columns]\nMean Absolute Error: 2681.2430726638395\nMean Squared Error: 13246002.119140355\nRoot Mean Squared Error: 3639.505752041114\nR2 Score: 79.941657245098%\n\nTest Size: 20 %\n\na = -7307.824968281864\nb = [-522.31275964  128.16628088   63.17240963]\n     Actual     Predicted\n0    6795.0   6561.779408\n1   15750.0  20047.965597\n..      ...           ...\n39  45400.0  39099.945713\n40   8916.5   6559.957946\n\n[41 rows x 2 columns]\nMean Absolute Error: 3028.44528450474\nMean Squared Error: 15255724.671464592\nRoot Mean Squared Error: 3905.8577382522003\nR2 Score: 80.29392621688581%\n\nTest Size: 10 %\n\na = -7824.384102535303\nb = [-613.42877141  135.45474355   63.82356299]\n     Actual     Predicted\n0    6795.0   6388.284759\n1   15750.0  20259.732808\n..      ...           ...\n19   6488.0   6140.798124\n20   9959.0  12025.455910\n\n[21 rows x 2 columns]\nMean Absolute Error: 2944.3040595022885\nMean Squared Error: 12712894.325743863\nRoot Mean Squared Error: 3565.5145948016907\nR2 Score: 68.17562555579416%\n\n\n\n\n\n3.2.3 Carlength, Carwidth, Curbweight, Cylinder Number, Engine Size, Horsepower vs Price\n\n\nCode\n# Store carlength, carwidth, curbweight, cylindernumber,\n# enginesize & horsepower columns in X\nX3 = datasetCopy.iloc[:, 10:-4].values\n\n# Call Regression Model Function with multiple x values & 30% test size\nmyLinRegModel(X3, price, 30)\n\n\nTest Size: 30 %\n\na = -50133.27454870472\nb = [-62.20404054 772.47835707   3.18527494  18.99449375  65.77507898\n  64.33402142]\n     Actual     Predicted\n0    6795.0   6067.345502\n1   15750.0  20331.280734\n..      ...           ...\n60   6479.0   5548.422659\n61  15510.0  13525.755400\n\n[62 rows x 2 columns]\nMean Absolute Error: 2536.7293535638096\nMean Squared Error: 12555624.008739235\nRoot Mean Squared Error: 3543.39159686581\nR2 Score: 80.98709273909488%\n\nTest Size: 20 %\n\na = -54793.72590677004\nb = [-38.92156396 789.71920542   2.95208156 366.09875078  59.3369646\n  58.3182745 ]\n     Actual     Predicted\n0    6795.0   6167.243070\n1   15750.0  20562.635157\n..      ...           ...\n39  45400.0  36977.654082\n40   8916.5   5783.745608\n\n[41 rows x 2 columns]\nMean Absolute Error: 2873.7239149228203\nMean Squared Error: 16025434.859390952\nRoot Mean Squared Error: 4003.178094887979\nR2 Score: 79.29967874050975%\n\nTest Size: 10 %\n\na = -55531.82635570387\nb = [-30.01857827 784.53201154   2.29960226 213.59515415  74.83936968\n  58.2469381 ]\n     Actual     Predicted\n0    6795.0   6065.470340\n1   15750.0  20665.341927\n..      ...           ...\n19   6488.0   5585.072554\n20   9959.0  11876.766620\n\n[21 rows x 2 columns]\nMean Absolute Error: 3020.6881171033187\nMean Squared Error: 13605511.765351668\nRoot Mean Squared Error: 3688.5650008304947\nR2 Score: 65.94112325398689%"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#no-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#no-scaling",
    "title": "Linear Regression, Classification, Clustering",
    "section": "4.1 No Scaling",
    "text": "4.1 No Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'None')\n\n\n\n\n\nKNeigbors Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       0.00      0.00      0.00         6\n         gas       0.85      0.94      0.89        35\n\n    accuracy                           0.80        41\n   macro avg       0.42      0.47      0.45        41\nweighted avg       0.72      0.80      0.76        41\n\nDecision Tree Classifier - Scaling: None\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         6\n         gas       1.00      1.00      1.00        35\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure 5. Classifier Models Confusion Matrices - No Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#standardized-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#standardized-scaling",
    "title": "Linear Regression, Classification, Clustering",
    "section": "4.2 Standardized Scaling",
    "text": "4.2 Standardized Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Standardize')\n\n\n\n\n\nKNeigbors Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Standardize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure 6. Classifier Models Confusion Matrices - Standard Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#normalized-scaling",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#normalized-scaling",
    "title": "Linear Regression, Classification, Clustering",
    "section": "4.3 Normalized Scaling",
    "text": "4.3 Normalized Scaling\n\n\nCode\n# Store all numeric values in X\nX = dataset[['wheelbase', 'carlength', 'carwidth', 'carheight',\\\n             'curbweight', 'cylindernumber', 'enginesize', 'boreratio',\\\n             'stroke', 'compressionratio', 'horsepower', 'peakrpm',\\\n             'citympg', 'highwaympg', 'price']].values\n\n# Classify according to fuel type\ny = dataset['fueltype']\n\n# Call Classification Model Function with no scalar\nmyClassModel(X, y, 'Normalize')\n\n\n\n\n\nKNeigbors Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\nDecision Tree Classifier - Scaling: Normalize\n              precision    recall  f1-score   support\n\n      diesel       1.00      1.00      1.00         3\n         gas       1.00      1.00      1.00        38\n\n    accuracy                           1.00        41\n   macro avg       1.00      1.00      1.00        41\nweighted avg       1.00      1.00      1.00        41\n\n\n\n\n\n\n\n\n(a) KNeigbors Classifier\n\n\n\n\n\n\n\n(b) Decision Tree Classifier\n\n\n\n\nFigure 7. Classifier Models Confusion Matrices - Normalized Scaling"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#kmeans-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#kmeans-model",
    "title": "Linear Regression, Classification, Clustering",
    "section": "5.1 KMeans Model",
    "text": "5.1 KMeans Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# KMeans Cluster Model to be used\nmodel = 'KM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Scatter Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - No Scaling\n\n\n\n\n\n\n\n\n\n(c) Joint Plot - Normalized Scaling\n\n\n\n\nFigure 8. KMeans Cluster Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#gaussian-mixture-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#gaussian-mixture-model",
    "title": "Linear Regression, Classification, Clustering",
    "section": "5.2 Gaussian Mixture Model",
    "text": "5.2 Gaussian Mixture Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Gaussian Mixture Model to be used\nmodel = 'GMM'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Joint Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - Normalized Scaling\n\n\n\n\nFigure 9. Gaussian Mixture Cluster Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#spectral-clustering-model",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#spectral-clustering-model",
    "title": "Linear Regression, Classification, Clustering",
    "section": "5.3 Spectral Clustering Model",
    "text": "5.3 Spectral Clustering Model\n\n\nCode\n# Store all features in X\nX = dataset[['compressionratio', 'price']]\n\n# Spectral Clustering Model to be used\nmodel = 'SC'\n\n# Number of clusters to be created\nn_clusters = 3\n\n# Call Clustering Model Function and pass in features\n# model & number of clusters\nmyClusterModel(X, model, n_clusters)\n\n\n\n\n\n\n\n\n(a) Joint Plot - No Scaling\n\n\n\n\n\n\n\n(b) Joint Plot - Normalized Scaling\n\n\n\n\nFigure 10. Spectral Clustering Model Visualizations"
  },
  {
    "objectID": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#references",
    "href": "posts/intro-to-ai-assignment1/Assignment1BrandonToews.html#references",
    "title": "Linear Regression, Classification, Clustering",
    "section": "References",
    "text": "References\nDataset\nhttps://www.kaggle.com/datasets/hellbuoy/car-price-prediction?select=CarPrice_Assignment.csv\nData Cleaning\nhttps://datatofish.com/category/python/\nData Scaling\nhttps://dataakkadian.medium.com/standardization-vs-normalization-da7a3a308c64\nhttps://medium.datadriveninvestor.com/data-pre-processing-with-scikit-learn-9896c561ef2f\nMeasuring Accuracy\nhttps://www.bmc.com/blogs/mean-squared-error-r2-and-variance-in-regression-analysis/\nVisualizations\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\nhttps://seaborn.pydata.org/api.html\nhttps://matplotlib.org/stable/api/pyplot_summary.html\nClassification & Clustering Models\nhttps://www.activestate.com/resources/quick-reads/how-to-classify-data-in-python/\nhttps://builtin.com/data-science/data-clustering-python\nhttps://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203\nGeneral Python\nhttps://stackoverflow.com"
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#purpose-of-document",
    "href": "posts/wireless-networking/wireless-networking.html#purpose-of-document",
    "title": "Wireless Technology Networking",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide a secure, reliable, and cost effective solution for the Acme Corp Infrastructure Project. It details the physical and logical requirements and how to address those in the best manner possible."
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#scope",
    "href": "posts/wireless-networking/wireless-networking.html#scope",
    "title": "Wireless Technology Networking",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nScope can be split into two parts; one being the establishment of a wireless point-to-point link between Landmark 3 and Landmark 2 buildings and the other creating a WLAN network at the CAT facility in Landmark 3 on the ground floor. However, the proposed solution was designed in such a way to address potential contingencies as they arise."
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#equipment-pricing",
    "href": "posts/wireless-networking/wireless-networking.html#equipment-pricing",
    "title": "Wireless Technology Networking",
    "section": "2.1 Equipment / Pricing",
    "text": "2.1 Equipment / Pricing\n\n\n\n2.1.1\nTwo (2) MikroTik mANTBox 52 15s Units ( Antenna mounts included )\n$456.85\n\n\n2.1.2\nTwo (2) WC-44 Outdoor Enclosures\n$211.38\n\n\n2.1.3\nTwo (2) Pole Mounting Assemblies\n$100.93\n\n\n2.1.4\nTwo (2) Heavy Duty, High-speed Cat8 Ethernet Cables ( 150ft outdoor cables )\n$165.98\n\n\n2.1.5\nOne (1) UniFi EdgeSwitch 16XP\n$425.00\n\n\n2.1.6\nOne (1) Netgate 3100 MAX pfSense+ Firewall\n$442.00\n\n\n2.1.7\nNine (9) UniFi WiFi 6 Long-Range Access Points ( See Cabling )\n$1,611.00\n\n\n\nTOTAL\n$3,413.14\n\n\n\n\n\n\n\n\n\nPrices are estimated based on current market costs as of Dec 2021 and are subject to change. Only includes hardware costs.\n\n\n\n\n\n\n\n\n\n\n\n\nCabling\n\n\n\nAdditional subcontractor Cat5 cabling charges may added.\n\n\n\n\n\n\n\n\nNote\n\n\n\nURLs and Data Sheets for equipment are referenced in Appendix."
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#installation",
    "href": "posts/wireless-networking/wireless-networking.html#installation",
    "title": "Wireless Technology Networking",
    "section": "2.2 Installation",
    "text": "2.2 Installation\n\n2.2.1 Point-to-point Connection – Antenna Mounting Locations\nThere are two buildings that sit between the target locations. The Ecco supply building is one of them but poses no threat of interference with the point-to-point link as it is only 2 storeys tall, which sits well below the height of the antenna installations. The Landmark 6 building is taller than both Landmark 2 & 3 buildings but doesn’t block the line of sight between them. However, Landmark 6 might encroach on the fresnel zone of the link if not properly placed. Therefore the antenna on Landmark 3 should be installed on the roof at the SE corner and the antenna on Landmark 2 should be installed on the roof at the NE corner. ( See Figure 1 )\n\n\n\n\n\n\n\n(a) Surrounding Building Heights\n\n\n\n\n\n\n\n(b) Antenna Mounts\n\n\n\n\nFigure 1. Antenna Mounting Locations\n\n\n\n\n2.2.2 Point-to-point Connection – Landmark 2\nTo connect services from the electrical room on the 3rd floor to the roof will require a minimum of 37m of Cat8 ethernet cable. A 150ft cable is suggested for this solution to give some extra leway. The pole mount will need to be installed on the NE corner of the roof. The enclosure that is included with the antenna/base station does not have a sufficient rating for the weather conditions experienced year round in Kelowna. Therefore, they should be held in a WC-44 enclosure that is included in this solution. Included with the antenna/base station is one (1) Gigabit PoE injector cable with shielded connector. If there is no PoE device in the electrical room to connect to the base station then the PoE injector cable can be used in conjuction with a 50ft extension cord to reach a power supply located on the roof. Attach grounding wire to the grounding screw, then attach the other end of the grounding wire to the grounded mast. The antenna must be installed at an downtilt angle of 7.7244° degrees based on calculations below. ( See Figure 2 )\nDowntilt Ang = tan-1 [(Landmark 2 Hgt + Ant. Hgt) - (Landmark 3 Hgt + Ant. Hgt)]/Distance\n7.7244° deg = tan-1 [(50m + 2m) - (30m + 2m) / 145.45m]\n\n\n2.2.3 Point-to-point Connection – Landmark 3\nTo connect services from the meet-me room on the basement floor to the roof will require a minimum of 35m of Cat8 ethernet cable. A 150ft cable is suggested for this solution to give some extra leway. The pole mount will need to be installed on the SE corner of the roof. It is suggested that one (1) PoE UniFi EdgeSwitch 16XP be installed in the wiring closet of the basement and this will serve as both a power source and data connection for the base station. During installation the base station should be connected to ethernet port 1 on the switch. Attach grounding wire to the grounding screw, then attach the other end of the grounding wire to the grounded mast. The antenna must be installed at an uptilt angle of -7.7244° degrees based on calculations below. ( See Figure 2 )\nUptilt Ang = tan-1 [(Landmark 3 Hgt + Ant. Hgt) - (Landmark 2 Hgt + Ant. Hgt)]/Distance\n-7.7244° deg = tan-1 [(30m + 2m) - (50m + 2m) / 145.45m]\n\n\n\nFigure 2. Antenna Angles\n\n\n\n\n2.2.4 WLAN Network\nWiFi coverage is needed on the ground floor of CAT facilities in the Landmark 3 building. Nine (9) UniFi WiFi 6 Long-Range Access Points mounted on the ceiling at the locations depicted in Figure 3 will provide superior coverage of the space. Each access point should be connected to the PoE switch located in the wiring closet with Cat5 cables. The access points should be connected to the switch starting from ethernet port 2 to ethernet port 10. The Netgate firewall device should be located in the wiring closet with the switch and connected to port 16 on the switch, leaving five (5) ports available for future additions if need be. For further discussion of access point configurations and coverage reference Section 3, items 3.2.2 and 3.2.3 of this document.\n\n\n\nFigure 3. Access Point Locations"
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#network-topology",
    "href": "posts/wireless-networking/wireless-networking.html#network-topology",
    "title": "Wireless Technology Networking",
    "section": "3.1 Network Topology",
    "text": "3.1 Network Topology\n\n\n\nFigure 4. Proposed Network Design"
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#proposed-network-configuration",
    "href": "posts/wireless-networking/wireless-networking.html#proposed-network-configuration",
    "title": "Wireless Technology Networking",
    "section": "3.2 Proposed Network Configuration",
    "text": "3.2 Proposed Network Configuration\nProposed layer 2 and layer 3 network topology depicted in Figure 4 demonstrates room for possible future contingencies. In the suggested solution, essential elements to be configured are three (3) VLANs with VLAN 10 being used for management, VLAN 20 for ACME staff access, and VLAN 100 for the point-to-point connection. The subnets assigned to these VLANs are detailed in Figure 4. All ports should be configured as trunk ports to allow management and staff VLANs the ability to reach ACME’s ISP in Landmark 2 directly. VLAN 100 network addresses should be assigned to an interface on the firewall in Landmark 3 and whatever device is used to manage traffic in Landmark 2. The Firewall device should be configured to manage/route VLAN traffic. If, in the future, ACME comes into an agreement with CAT to allow CAT staff to use the WLAN network then VLAN 30 can be activated and configured with the subnet provided in Figure 4. If this contigency is used then the Firewall device should be configured to route VLAN 30 traffic through the ACME point-to-point subnet.\n\n3.2.1 Point-To-Point Configuration\nThe antenna/base station units should be set to auto-modulate to ensure that the highest possible data rates are achieved. In addition, ensuring that the signal is transmitted on 5 Ghz band will result in the fastest, most reliable connection. As per Figure 6, as long as the signal received is higher than -72dBm the connection will run at the highest data rates supported by the base units. As referenced in the link budget in Figure 5, the transmit power should be set for 23dBm resulting in a receiving signal of -43.79dBm which is significantly higher than the minimum -72dBm needed. Finally the units should be configured to a dynamic power transmit setting so as to compensate for weather fluctuations that may interfere with the connection.\n\n\n\nFigure 5. Link Budget\n\n\n\n\n\nFigure 6. Rate to dBm\n\n\n\n\n3.2.2 Access Point Configuration\nAll access points should be configured so as not to broadcast VLAN 10’s network SSID “Management”. However, it is advisable for all access points to broadcast VLAN 20’s network SSID “Staff” to allow easier access for users trying to connect. All access points should enable automatic channel selection and be set to transmit in both 2.4 Ghz and 5 Ghz bands. Each access point should be configured to transmit at a power of 12dBm for the 2.4Ghz band and 23dBm for the 5Ghz band as shown in Figure 7. Transmitting at these proposed levels will ensure all devices on premises will receive no lower than a -50dBm signal. The heat maps in @heat-maps illustrate signal strength at a -50dBm threshold. ( See Figure 8 )\n\n\n\nFigure 7. Access Point Information\n\n\n\n\n3.2.3 Heat Maps\n\n\n\n\n\n\n\n(a) 2.4 Ghz Band\n\n\n\n\n\n\n\n(b) 5 Ghz Band\n\n\n\n\nFigure 8. Heat maps\n\n\n\n\n3.2.4 Firewall Configuration\nA DHCP server should be configured to assign IP addresses to clients connecting to the access points. It would be recommend to enable the adblocking and website blocking features to protect the users and the network. If the VLAN 30 network is employed then the firewall should be configured to route traffic from VLAN 30 through VLAN 100 to reach services at ACME in Landmark 2. VPN features are available and may be taken advantage. Finally, it is highly recommended to enable/configure the intrusion prevension system (IPS) as this will provide an invaluable layer of security to the network.\n\n\n3.2.5 Management IP Address Distribution\nIt is suggested to assign the management devices in this solution the IP addresses shown in Figure 9. Figure 9 can easily be referenced when there is a need to access a specific device from a web browser.\n\n\n\nFigure 9. IP Address Subnets"
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#proposed-security-configuration",
    "href": "posts/wireless-networking/wireless-networking.html#proposed-security-configuration",
    "title": "Wireless Technology Networking",
    "section": "4.1 Proposed Security Configuration",
    "text": "4.1 Proposed Security Configuration\n\nUse longer more complex passwords for wireless networks\nSeparate management access from general user access by creating a management VLAN\nDon’t broadcast the SSID for the management VLAN\nEnable WPA2 and WPA3 protocols on the network and disable all other security protocols\nOnly allow the specific MAC addresses of installed access points to connect to network to deter rogue AP attacks\nConfigure the RF signal strength of access points in such a way that the network can only be reached and connected to inside the immediate premises. ( See Figure 6 )\nSet access points to isolate clients\nEnable automatic security updates for all devices\nEnable IPS on the firewall"
  },
  {
    "objectID": "posts/wireless-networking/wireless-networking.html#references",
    "href": "posts/wireless-networking/wireless-networking.html#references",
    "title": "Wireless Technology Networking",
    "section": "References",
    "text": "References\n\nMikroTik “mANTBox 52 15s\nhttps://mikrotik.com/product/mantbox_52_15s\n\n\nWC-44 Outdoor Enclosure with Clear Cover\nhttps://www.polycase.com/wc-44\n\n\nPole Mounting Assembly\nhttps://wilsonamplifiers.ca/pole-mounting-assembly-for-outdoor-antennas-10-inch-901117/\n\n\n150ft Cat8 Heavy Duty High-speed Cable\nhttps://www.amazon.ca/Ethernet-Shielded-Lastest-2000Mhz-Weatherproof/dp/B087N2BBF6\n\n\nUniFi EdgeSwitch 16XP\nhttps://store.ui.com/collections/operator-edgemax-switches/products/es-16xp\n\n\nNetgate 3100 MAX pfSense+ Security Gateway\nhttps://shop.netgate.com/products/3100-max-pfsense?variant=32156745531507\n\n\nUniFi Access Point WiFi 6 Long-Range\nhttps://store.ui.com/products/unifi-6-long-range-access-point_pos=20&_sid=883e3e553&_ss=r"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#purpose-of-document",
    "href": "posts/linux-admin/linux-admin.html#purpose-of-document",
    "title": "Linux Administration Project",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide suggestions, solutions, and configurations for a major upgrade that will improve the current network."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#scope",
    "href": "posts/linux-admin/linux-admin.html#scope",
    "title": "Linux Administration Project",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an examination of the current network to ascertain where it can be improved. In addition, design and implemention of a prototype network to demonstrate what it could look like after the proposed upgrades. The proposed solutions must include detailed configurations and address potential growth of the business in the future."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#existing-network-design",
    "href": "posts/linux-admin/linux-admin.html#existing-network-design",
    "title": "Linux Administration Project",
    "section": "2.1 Existing Network Design",
    "text": "2.1 Existing Network Design\nThe existing network utilizes 11 servers comprised of one (1) NIS authentication server, two (2) NFS file share servers, one (1) Telnet server, two (2) database servers, two (2) app servers, two (2) web servers, and one (1) backup server. ( See Figure 1 )\n\n\n\nFigure 1. Existing Network Topology"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#proposed-network-topology",
    "href": "posts/linux-admin/linux-admin.html#proposed-network-topology",
    "title": "Linux Administration Project",
    "section": "2.2 Proposed Network Topology",
    "text": "2.2 Proposed Network Topology\nThe proposed network utilizes 11 servers comprised of one (1) LDAP authentication server, one (1) Samba file share server, one (1) NFS file share server, two (2) database servers, two (2) app servers, two (2) web servers, one (1) backup server, and one (1) Firewall. ( See Figure 2 )\n\n\n\nFigure 2. Proposed Network Topology"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#server-os-upgrade",
    "href": "posts/linux-admin/linux-admin.html#server-os-upgrade",
    "title": "Linux Administration Project",
    "section": "3.1 Server OS Upgrade",
    "text": "3.1 Server OS Upgrade\nThe existing servers are all running CentOS 5.0 currently which is starting to become old and losing community support. All servers should be upgraded to CentOS 7.0 as this will provide the most support while maintaining stronger stability that newer releases.3.2 LDAP Centralized Authentication Current network athentication is managed by a NIS server which only functions for Linux operating systems and passes sensitive information in plain text. The prototype network runs an LDAP server instead for many reasons. Eventually the Windows clients can be looped into the system to authenticate making it so that the total Linux and Windows structures would all be managed from the same place. In addition LDAP touts improved security as it utilizes TLS certifications to establish connections and pass information. The LDAP server can be used to authenticate against for all of the other services that the network contains."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#firewall-addition",
    "href": "posts/linux-admin/linux-admin.html#firewall-addition",
    "title": "Linux Administration Project",
    "section": "3.2 Firewall Addition",
    "text": "3.2 Firewall Addition\nNo firewall is present with the existing network which places the system at significantly more risk. With the addition of a pfSense firewall in place all internal traffic is protected and blocked from the external. All traffic that accesses anything outside of the LAN, including and especially the internet, is routed through firewall so as to keep as small of an attack surface as possible."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#samba-share-server",
    "href": "posts/linux-admin/linux-admin.html#samba-share-server",
    "title": "Linux Administration Project",
    "section": "3.3 Samba Share Server",
    "text": "3.3 Samba Share Server\nWhile the two (2) NFS share servers are useful they are overkill and could be downsized to make room for something that may grow better with the company in the future. One (1) of the NFS servers has been replaced in the prototype network for a Samba server. At the moment the Samba server is just acting as a share but the possibilities for it to be so much more are endless. In the future it could be utilized alongside the LDAP server to provide authentication for both Windows and Linux clients. All users could sign in on whatever device they prefer while the Samba server acts as an NT4 primary domain server or even an Active Directory domain controller, all with an LDAP backend. This would all for a possibility to add a secondary domain server to add redundancy to the system and provide stability as the business grows."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#telnet-ftp",
    "href": "posts/linux-admin/linux-admin.html#telnet-ftp",
    "title": "Linux Administration Project",
    "section": "3.4 Telnet & FTP",
    "text": "3.4 Telnet & FTP\nThe current system utilizes Telnet for communications among servers which has been long recognized as very insecure and should not be used in production. Hence all Telnet services have been replaced with SSH servers so as to secure and encrypted communications. Having SSH servers allows us to take advantage of Secure Copy Protocol which will also replace the very insecure File Transfer Protocol servers the occupy the system now. Neither Telnet nor FTP servers should be used as long as we have better options like SSH and SCP available to us."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#ldap-configuration",
    "href": "posts/linux-admin/linux-admin.html#ldap-configuration",
    "title": "Linux Administration Project",
    "section": "4.1 LDAP Configuration",
    "text": "4.1 LDAP Configuration\nThe LDAP server has an NFS server running on it to export the home directories to the client servers so that they can automount them when they login to various users. The LDAP structure has been generated on the LDAP server using the following ldif files and ens33 file… db.ldif, monitor.ldif, base.ldif, users.ldif, groups.ldif, ens33.\n\n\ndb.ldif\n\ndn: olcDatabase={2}hdb,cn=config\nchangetype: modify\nreplace: olcSuffix\nolcSuffix: dc=azuretech,dc=local\n\ndn: olcDatabase={2}hdb,cn=config\nchangetype: modify\nreplace: olcRootDN\nolcRootDN: cn=ldapadm,dc=azuretech,dc=local\n\ndn: olcDatabase={2}hdb,cn=config\nchangetype: modify\nreplace: olcRootPW\nolcRootPW: {SSHA}GBlGYcFck5dRl6+FwVlArdlJywiCsCfK\n\n\n\nmonitor.ldif\n\ndn: olcDatabase={1}monitor,cn=config\nchangetype: modify\nreplace: olcAccess\nolcAccess: {0}to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external,\ncn=auth\" read by dn.base=\"cn=ldapadm,dc=azuretech,dc=local\" read by * none\n\n\n\nbase.ldif\n\ndn: dc=azuretech,dc=local\ndc: azuretech\nobjectClass: top\nobjectClass: domain\n\ndn: cn=ldapadm,dc=azuretech,dc=local\nobjectClass: organizationalRole\ncn: ldapadm\ndescription: LDAP Manager\n\ndn: ou=People,dc=azuretech,dc=local\nobjectClass: organizationalUnit\nou: People\n\ndn: ou=Group,dc=azuretech,dc=local\nobjectClass: organizationalUnit\nou: Group\n\n\n\nusers.ldif\n\ndn: uid=ldapuser1,ou=People,dc=azuretech,dc=local\nuid: ldapuser1\ncn: ldapuser1\nsn: ldapuser1\nmail: ldapuser1@azuretech.local\nobjectClass: person\nobjectClass: organizationalPerson\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: topobjectClass: shadowAccount\nuserPassword:\n{crypt}\n$6$SCUZRM.D$B4l.n2hipD4IvghGaaJiQtLkiOF62YS7PhDy30EpVd81noq4KkDU2EqORUW.\n8Dq4k5GhPkChklxZIYiKMvcFx1\nshadowLastChange: 19159\nshadowMin: 0\nshadowMax: 99999\nshadowWarning: 7\nloginShell: /bin/bash\nuidNumber: 1000\ngidNumber: 1000\nhomeDirectory: /home/ldapuser1\n\ndn: uid=ldapuser2,ou=People,dc=azuretech,dc=local\nuid: ldapuser2\ncn: ldapuser2\nsn: ldapuser2\nmail: ldapuser2@azuretech.local\nobjectClass: person\nobjectClass: organizationalPerson\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nobjectClass: shadowAccount\nuserPassword:\n{crypt}\n$6$CbncYy5y$JlNL4899lmlZAgqnLYersesFTLkFJNY6DQdNvwOFU8LyHNXPMMrfCaUo7Br\nSdYw/KFlDOubpyn2b242re1Zea/\nshadowLastChange: 19159\nshadowMin: 0\nshadowMax: 99999\nshadowWarning: 7\nloginShell: /bin/bash\nuidNumber: 1001gidNumber: 1001\nhomeDirectory: /home/ldapuser2\n\ndn: uid=ldapuser3,ou=People,dc=azuretech,dc=local\nuid: ldapuser3\ncn: ldapuser3\nsn: ldapuser3\nmail: ldapuser3@azuretech.local\nobjectClass: person\nobjectClass: organizationalPerson\nobjectClass: inetOrgPerson\nobjectClass: posixAccount\nobjectClass: top\nobjectClass: shadowAccount\nuserPassword:\n{crypt}\n$6$J5mrmTYw$bKruVOsucaiHJFS3S6MKG4jyC.o/P62DeLW/R.YVpktFaYlpHzjjjxIGW5HpA\nmsf5OCN9dId5s/F33HptMfvG0\nshadowLastChange: 19159\nshadowMin: 0\nshadowMax: 99999\nshadowWarning: 7\nloginShell: /bin/bash\nuidNumber: 1002\ngidNumber: 1002\nhomeDirectory: /home/ldapuser3\n\n\n\ngroups.ldif\n\ndn: cn=ldapuser1,ou=Group,dc=azuretech,dc=local\nobjectClass: posixGroup\nobjectClass: top\ncn: ldapuser1\nuserPassword: {crypt}x\ngidNumber: 1000\n\ndn: cn=ldapuser2,ou=Group,dc=azuretech,dc=local\nobjectClass: posixGroup\nobjectClass: top\ncn: ldapuser2\nuserPassword: {crypt}x\ngidNumber: 1001\n\ndn: cn=ldapuser3,ou=Group,dc=azuretech,dc=local\nobjectClass: posixGroup\nobjectClass: top\ncn: ldapuser3\nuserPassword: {crypt}x\ngidNumber: 1002\n\n\n\n/etc/sysconfig/network-scripts/ifcfg-ens33\n\nTYPE=\"Ethernet\"\nPROXY_METHOD=\"none\"\nBROWSER_ONLY=\"no\"\nBOOTPROTO=\"none\"\nDEFROUTE=\"yes\"\nIPV4_FAILURE_FATAL=\"no\"\nIPV6INIT=\"no\"\n#IPV6_AUTOCONF=\"yes\"\n#IPV6_DEFROUTE=\"yes\"\n#IPV6_FAILURE_FATAL=\"no\"\n#IPV6_ADDR_GEN_MODE=\"stable-privacy\"\nNAME=\"ens33\"\nUUID=\"93f56d12-8057-4ca0-af7f-6c8e6fcffccd\"\nDEVICE=\"ens33\"\nONBOOT=\"yes\"\nIPADDR=\"192.168.65.10\"\nPREFIX=\"24\"\nGATEWAY=\"192.168.65.20\"\nDNS1=\"192.168.65.10\"\nDNS2=\"8.8.8.8\"\nDNS3=\"8.8.4.4\"\nDOMAIN=\"azuretech.local\""
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#ldap-client-configuration",
    "href": "posts/linux-admin/linux-admin.html#ldap-client-configuration",
    "title": "Linux Administration Project",
    "section": "4.2 LDAP Client Configuration",
    "text": "4.2 LDAP Client Configuration\nThe LDAP Clients need to install some packing and create some automap files for autofs to automount LDAP user home directories. I used the follow script to accomplish this…\n\n\nLDAP_Client_Install.sh\n\n#!/bin/bash\nyum install -y openldap-clients nss-pam-ldapd nfs-utils autofs\nauthconfig --enableldap --enableldapauth --ldapserver=192.168.65.10 --ldapbasedn=\"dc=azuretech,dc=local\" --update\nsystemctl restart nslcd\nsetsebool -P use_nfs_home_dirs=1\necho \"/home /etc/home.map\" &gt;&gt; /etc/auto.master\necho \"* -fstype=nfs,rw,nosuid,soft 192.168.65.10:/home/&\" &gt;&gt; /etc/home.map\nsystemctl enable rpcbind\nsystemctl start rpcbind\nsystemctl enable autofs\nsystemctl start autofs\nexit"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#firewall-configuration",
    "href": "posts/linux-admin/linux-admin.html#firewall-configuration",
    "title": "Linux Administration Project",
    "section": "4.3 Firewall Configuration",
    "text": "4.3 Firewall Configuration\nConfigured as seen in figures below. ( See Figure 3, Figure 4, & Figure 5)\n\n\n\nFigure 3. Firewall Configurations\n\n\n\n\n\nFigure 4. Firewall Configurations\n\n\n\n\n\nFigure 5. Firewall Configurations"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#dns-master-server-configuration",
    "href": "posts/linux-admin/linux-admin.html#dns-master-server-configuration",
    "title": "Linux Administration Project",
    "section": "4.4 DNS Master Server Configuration",
    "text": "4.4 DNS Master Server Configuration\nThe DNS master server was installed on the same server as the LDAP so it is referred to as LDAP in the configuration files. The main config, the forward zone, and the reverse zone files shown below…\n\n\n/etc/named.conf\n\n//\n// named.conf\n//\n// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS\n// server as a caching only nameserver (as a localhost DNS resolver only).\n//\n// See /usr/share/doc/bind*/sample/ for example named configuration files.\n//\n// See the BIND Administrator's Reference Manual (ARM) for details about the\n// configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html\n\noptions {\n  listen-on port 53 { 127.0.0.1; 192.168.65.10; };\n  // listen-on-v6 port 53 { ::1; };\n  directory\"/var/named\";\n  dump-file\"/var/named/data/cache_dump.db\";\n  statistics-file \"/var/named/data/named_stats.txt\";\n  memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n  recursing-file \"/var/named/data/named.recursing\";\n  secroots-file \"/var/named/data/named.secroots\";\n  allow-query    { localhost; 192.168.65.0/24; };\n  allow-transfer { localhost; 192.168.65.11; };\n  \n  /*\n    - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.\n    - If you are building a RECURSIVE (caching) DNS server, you need to enable\n      recursion.\n    - If your recursive DNS server has a public IP address, you MUST enable access\n      control to limit queries to your legitimate users. Failing to do so will\n      cause your server to become part of large scale DNS amplification\n      attacks. Implementing BCP38 within your network would greatly\n      reduce such attack surface\n  */\n  recursion no;\n  \n  dnssec-enable yes;\n  dnssec-validation yes;\n  // dnssec-lookaside auto;\n  \n  /* Path to ISC DLV key */\n  bindkeys-file \"/etc/named.root.key\";managed-keys-directory \"/var/named/dynamic\";\n  \n  pid-file \"/run/named/named.pid\";\n  session-keyfile \"/run/named/session.key\";\n};\n\nlogging {\n  channel default_debug {\n    file \"data/named.run\";\n    severity dynamic;\n  };\n};\n\nzone \".\" IN {\n  type hint;\n  file \"named.ca\";\n};\n\nzone \"azuretech.local\" IN {\n  type master;\n  file \"forward.azuretech\";\n  allow-update { none; };\n};\n\nzone \"65.168.192.in-addr.arpa\" IN {\n  type master;\n  file \"reverse.azuretech\";\n  allow-update { none; };\n};\n\ninclude \"/etc/named.rfc1912.zones\";\ninclude \"/etc/named.root.key\";\n\n\n\n/var/named/forward.azuretech\n\n$TTL 86400\n@   IN SOA ldap.azuretech.local. root.azuretech.local. (\n                    2022061101 ; serial\n                    3600; refresh\n                    1800; retry\n                    604800 ; expire\n                    86400 ) ; minimum TTL\n                    \n;Name Servers\n    IN    NS    ldap.azuretech.local.     ; Master\n    IN    NS    samba.azuretech.local.    ; Slave\n  \n    IN    A     192.168.65.10 ; Name Server to IP resolve\n    IN    A     192.168.65.11\n  \nldap        IN    A   192.168.65.10 ; Host\nsamba       IN    A   192.168.65.11 ; Host\nnfs         IN    A   192.168.65.12 ; Client\nbackup      IN    A   192.168.65.13 ; Client\ndb1         IN    A   192.168.65.14 ; Client\ndb2         IN    A   192.168.65.15 ; Client\napp1        IN    A   192.168.65.16 ; Client\napp2        IN    A   192.168.65.17 ; Client\nweb1        IN    A   192.168.65.18 ; Client\nweb2        IN    A   192.168.65.19 ; Client\nfirewall    IN    A   192.168.65.20 ; Firewall\n\n\n\n/var/named/reverse.azuretech\n\n$TTL 86400\n@   IN SOA ldap.azuretech.local. root.azuretech.local. (\n                    2022061101 ; serial\n                    3600; refresh\n                    1800; retry\n                    604800 ; expire\n                    86400 ) ; minimum TTL\n                    \n;Name Servers\n    IN    NS    ldap.azuretech.local.   ; Master\n    IN    NS    samba.azuretech.local.  ; Slave\n    IN    PTR   azuretech.local.\n    \n;Record (IP) points to hostname\nldap        IN    A   192.168.65.10 ; Master Nameserver\nsamba       IN    A   192.168.65.11 ; Slave Nameserver\nnfs         IN    A   192.168.65.12 ; Client\nbackup      IN    A   192.168.65.13 ; Client\ndb1         IN    A   192.168.65.14 ; Client\ndb2         IN    A   192.168.65.15 ; Client\napp1        IN    A   192.168.65.16 ; Client\napp2        IN    A   192.168.65.17 ; Client\nweb1        IN    A   192.168.65.18 ; Client\nweb2        IN    A   192.168.65.19 ; Client\nfirewall    IN    A   192.168.65.20 ; Firewall\n\n10    IN    PTR   ldap.azuretech.local.\n11    IN    PTR   samba.azuretech.local.\n12    IN    PTR   nfs.azuretech.local.\n13    IN    PTR   backup.azuretech.local.\n14    IN    PTR   db1.azuretech.local.\n15    IN    PTR   db2.azuretech.local.\n16    IN    PTR   app1.azuretech.local.\n17    IN    PTR   app2.azuretech.local.\n18    IN    PTR   web1.azuretech.local.\n19    IN    PTR   web2.azuretech.local.\n20    IN    PTR   firewall.azuretech.local."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#dns-slave-server-configuration",
    "href": "posts/linux-admin/linux-admin.html#dns-slave-server-configuration",
    "title": "Linux Administration Project",
    "section": "4.5 DNS Slave Server Configuration",
    "text": "4.5 DNS Slave Server Configuration\nThe DNS slave server was installed on the same server as the Samba, so it is referred to as samba in the configuration files. The main config file is shown below…\n\n\n/etc/name.conf\n\n//\n// named.conf\n//\n// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS\n// server as a caching only nameserver (as a localhost DNS resolver only).\n//\n// See /usr/share/doc/bind*/sample/ for example named configuration files.\n//\n// See the BIND Administrator's Reference Manual (ARM) for details about the\n// configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html\n\noptions {\n  listen-on port 53 { 127.0.0.1; 192.168.65.11; };\n  // listen-on-v6 port 53 { ::1; };\n  directory\"/var/named\";\n  dump-file\"/var/named/data/cache_dump.db\";\n  statistics-file \"/var/named/data/named_stats.txt\";\n  memstatistics-file \"/var/named/data/named_mem_stats.txt\";\n  recursing-file \"/var/named/data/named.recursing\";\n  secroots-file \"/var/named/data/named.secroots\";\n  allow-query { localhost; 192.168.65.0/24; };\n  \n  /*\n    - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.\n    - If you are building a RECURSIVE (caching) DNS server, you need to enable\n      recursion.\n    - If your recursive DNS server has a public IP address, you MUST enable access\n      control to limit queries to your legitimate users. Failing to do so will\n      cause your server to become part of large scale DNS amplification\n      attacks. Implementing BCP38 within your network would greatly\n      reduce such attack surface\n  */\n  recursion no;\n\n  dnssec-enable yes;\n  dnssec-validation yes;\n\n  /* Path to ISC DLV key */\n  bindkeys-file \"/etc/named.root.key\";\n\n  managed-keys-directory \"/var/named/dynamic\";\n\n  pid-file \"/run/named/named.pid\";\n  session-keyfile \"/run/named/session.key\";\n};\n\nlogging {\n  channel default_debug {\n    file \"data/named.run\";\n    severity dynamic;\n  };\n};\n\nzone \".\" IN {\n  type hint;\n  file \"named.ca\";\n};\n\nzone \"azuretech.local\" IN {\n  type slave;\n  file \"slaves/azuretech.fwd.zone\";\n  masters { 192.168.65.10; };\n};\n\nzone \"65.168.192.in-addr.arpa\" IN {\n  type slave;\n  file \"slaves/azuretech.rev.zone\";\n  masters { 192.168.65.10; };\n};\n\ninclude \"/etc/named.rfc1912.zones\";\ninclude \"/etc/named.root.key\";"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#dhcp-server-configuration",
    "href": "posts/linux-admin/linux-admin.html#dhcp-server-configuration",
    "title": "Linux Administration Project",
    "section": "4.6 DHCP Server Configuration",
    "text": "4.6 DHCP Server Configuration\nThe DHCP server was installed on the same server as the LDAP. The main config file is shown below…\n\n\n/etc/dhcp/dhcpd.conf\n\n#\n# DHCP Server Configuration file.# see /usr/share/doc/dhcp*/dhcpd.conf.example\n# see dhcpd.conf(5) man page\n#\noption domain-name \"azuretech.local\";\noption domain-name-servers ldap.azuretech.local, samba.azuretech.local, 8.8.8.8, 8.8.4.4;\n\ndefault-lease-time 86400; # 24 hours\nmax-lease-time 604800;    # One week\n\n# Use this to enble / disable dynamic dns updates globally.\n#ddns-update-style none;\n\n# If this DHCP server is the official DHCP server for the local\n# network, the authoritative directive should be uncommented.\nauthoritative;\n\nsubnet 192.168.65.0 netmask 255.255.255.0 {\n  range 192.168.65.21 192.168.65.100;\n  option domain-name-servers 192.168.65.10, 192.168.65.11, 8.8.8.8, 8.8.4.4;\n  option domain-name \"azuretech.local\";\n  option routers 192.168.65.2;\n  option subnet-mask 255.255.255.0;\n  option broadcast-address 192.168.65.255;\n}\n\nhost nfs {\n  hardware ethernet 00:0c:29:08:dc:1c;\n  fixed-address 192.168.65.12;\n}\n\nhost backup {\n  hardware ethernet 00:0c:29:0e:59:b4;\n  fixed-address 192.168.65.13;\n}\n\nhost db1 {\n  hardware ethernet 00:0c:29:d1:f5:0c;\n  fixed-address 192.168.65.14;\n}\n\nhost db2 {\n  hardware ethernet 00:0c:29:95:46:a9;\n  fixed-address 192.168.65.15;\n}\nhost app1 {\n  hardware ethernet 00:0c:29:83:8c:60;\n  fixed-address 192.168.65.16;\n}\n\nhost app2 {\n  hardware ethernet 00:0c:29:f2:40:5c;\n  fixed-address 192.168.65.17;\n}\n\nhost web1 {\n  hardware ethernet 00:0c:29:a7:d9:38;\n  fixed-address 192.168.65.18;\n}\n\nhost web2 {\n  hardware ethernet 00:0c:29:e9:fa:2b;\n  fixed-address 192.168.65.19;\n}"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#nfs-server-configuration",
    "href": "posts/linux-admin/linux-admin.html#nfs-server-configuration",
    "title": "Linux Administration Project",
    "section": "4.7 NFS Server Configuration",
    "text": "4.7 NFS Server Configuration\nThe NFS server’s export file and client fstab config file are shown below…\n\n\n/etc/exports\n\n/nfsfileshare 192.168.65.0/24(rw,sync,no_root_squash)\n\n\n\n/etc/fstab\n\n#\n# /etc/fstab\n# Created by anaconda on Wed May 11 18:06:18 2022\n#\n# Accessible filesystems, by reference, are maintained under '/dev/disk'\n# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n#\n/dev/mapper/centos-root /       xfs   defaults    00\nUUID=747f3e8d-9501-43e8-9ed3-3fae6fea9f23 /boot       xfs   defaults    00\n/dev/mapper/centos-swap swap        swap  defaults    00\nnfs:/nfsfileshare /mnt/nfsfileshare       nfs   nosuid,rw,sync,hard,intr    00"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#samba-server-configuration",
    "href": "posts/linux-admin/linux-admin.html#samba-server-configuration",
    "title": "Linux Administration Project",
    "section": "4.8 Samba Server Configuration",
    "text": "4.8 Samba Server Configuration\nThe Samba server’s interface config and smb.conf files are shown below…\n\n\n/etc/sysconfig/network-scripts/ifcfg-ens33\n\nTYPE=\"Ethernet\"\nPROXY_METHOD=\"none\"\nBROWSER_ONLY=\"no\"\nBOOTPROTO=\"none\"\nDEFROUTE=\"yes\"\nIPV4_FAILURE_FATAL=\"no\"\nIPV6INIT=\"no\"\n#IPV6_AUTOCONF=\"yes\"\n#IPV6_DEFROUTE=\"yes\"\n#IPV6_FAILURE_FATAL=\"no\"\n#IPV6_ADDR_GEN_MODE=\"stable-privacy\"\nNAME=\"ens33\"\nUUID=\"eb0d3098-d1cd-4d11-9e74-e7478d56b0c5\"\nDEVICE=\"ens33\"\nONBOOT=\"yes\"\nIPADDR=\"192.168.65.11\"\nPREFIX=\"24\"\nGATEWAY=\"192.168.65.20\"\nDNS1=\"192.168.65.10\"\nDNS2=\"192.168.65.11\"\nDNS3=\"8.8.8.8\"\nDNS4=\"8.8.4.4\"\nDOMAIN=\"azuretech.local\"\n\n\n\n/etc/samba/smb.conf\n\n# See smb.conf.example for a more detailed config file or\n# read the smb.conf manpage.\n# Run 'testparm' to verify the config is correct after\n# you modified it.\n\n[global]\n  workgroup = SAMBA\n  security = user\n  \n  passdb backend = tdbsam\n  printing = cups\n  printcap name = cups\n  load printers = yes\n  cups options = raw\n\n[homes]\n  comment = Home Directories\n  valid users = %S, %D%w%S\n  browseable = No\n  read only = No\n  inherit acls = Yes\n\n[printers]\n  comment = All Printers\n  path = /var/tmp\n  printable = Yes\n  create mask = 0600\n  browseable = No\n\n[print$]\n  comment = Printer Drivers\n  path = /var/lib/samba/drivers\n  write list = @printadmin root\n  force group = @printadmin\n  create mask = 0664\n  directory mask = 0775\n\n[smb]\n  path = /samba\n  browseable = yes\n  read only = no\n  force create mode = 0660\n  force directory mode = 2770\n  valid users = smb @sadmin"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#email-server-configuration",
    "href": "posts/linux-admin/linux-admin.html#email-server-configuration",
    "title": "Linux Administration Project",
    "section": "4.9 Email Server Configuration",
    "text": "4.9 Email Server Configuration\nThe email servers’ were installed on the samba server machine and that is why it is labelled as samba in the config files. The Postfix and Dovecot config file alterations are shown below…\n\n\n/etc/postfix/main.cf\n\n# line 75: uncomment and specify hostname\nmyhostname = samba.azuretech.local\n\n# line 83: uncomment and specify domain name\nmydomain = azuretech.local\n\n# line 99: uncomment\nmyorigin = $mydomain\n\n# line 116: change\ninet_interfaces = all\n\n# line 164: add\nmydestination = $myhostname, localhost.$mydomain, localhost, $mydomain\n\n# line 264: uncomment and specify your local network\nmynetworks = 192.168.65.0/24\n\n# line 419: uncomment (use Maildir)\nhome_mailbox = Maildir/\n\n# line 574: add\nsmtpd_banner = $myhostname ESMTP\n\n# add follows to the end\n# limit an email size with 10M\nmessage_size_limit = 10485760\n\n\n\n/etc/dovecot/dovecot.conf\n\n# line 24: uncomment\nprotocols = imap pop3 lmtp\n\n# line 30: uncomment and change ( if not use IPv6 )\nlisten = *\n\n\n\n/etc/dovecot/conf.d/10-auth.conf\n\n# line 10: uncomment and change ( allow plain text auth )\ndisable_plaintext_auth = no\n\n# line 100: add\nauth_mechanisms = plain login\n\n\n\n/etc/dovecot/conf.d/10-mail.conf\n\n# line 30: uncomment and add\nmail_location = maildir:~/Maildir\n\n\n\n/etc/dovecot/conf.d/10-master.conf\n\n# line 96-98: uncomment and add like follows\n# Postfix smtp-auth\nunix_listener /var/spool/postfix/private/auth {\n  mode = 0666\n  user = postfix\n  group = postfix\n}\n\n\n\n/etc/dovecot/conf.d/10-ssl.conf\n\n# line 8: change (not require SSL)\nssl = no"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#password-generator-script",
    "href": "posts/linux-admin/linux-admin.html#password-generator-script",
    "title": "Linux Administration Project",
    "section": "5.1 Password Generator Script",
    "text": "5.1 Password Generator Script\nAs an administration tool I created a random password generator script that asks how long you want the password to be. After it generates the password is asks if the user wants to output it into a file and if so, what should the file name be. The bash script code for the file is below…\n\n\npasswdgen_script.sh\n\n#!/bin/bash\n#This is a script for generating secure random password suggestions for users\necho -e \"\\n****** Secure Random Password Generator ******\\n\\nLet's generate a random password...\\n\\nHow long would you like your password to be?\\n\\nCharacter Length: \"\nread length\npasswd=`echo $RANDOM | md5sum | head -c ${length}`\necho -e \"\\nPassword Suggestion: ${passwd}\\n\\nWould you like to store this password in a file? (y/n): \"\nanswer=null\nwhile [ ${answer} != y ] || [ ${answer} != n ]; do\nread answer\nif [ ${answer} = \"y\" ]; then\n  echo -e \"\\nWhat would you like you filename to be?: \"\n  read filenm\n  echo ${passwd} &gt; ${filenm}\n  echo -e \"\\nFile has been saved in the current working directory... Goodbye!\\n\"\n  return\nelif [ ${answer} = \"n\" ]; then\n  echo -e \"\\nOk... Gooodbye!\\n\"\n  return\nelse\n  echo -e \"\\nNot a valid answer, try again.. You must choose either (y/n): \\r\"\nfi\ndone\nreturn"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#ip-address-script",
    "href": "posts/linux-admin/linux-admin.html#ip-address-script",
    "title": "Linux Administration Project",
    "section": "5.2 IP Address Script",
    "text": "5.2 IP Address Script\nAs an administration tool I created an IP address script that asks you for an IP address. Then it asks the user for the subnet mask in bits and after which it prints the given IP address’ classification along with the subnet mask in decimal format. Thoughout the script if the user gives the wrong output it tells the user what they did wrong and lets them try the input again. The bash script code for the file is below…\n\n\nipaddr_script.sh\n\n#!/bin/bash\n#This script informs users about IP address class\n\necho -e \"\\n\\n****** IP Address Classification ******\\n\"\n\nipaddr=null\n\nwhile ! [[ ${ipaddr} =~ .[0-9] ]] || [ ${#ipaddr} -gt 15 ] || [ ${#ipaddr} -lt 7 ] || [ ${firstoct} -lt 1 ] || [ ${firstoct} -gt 255 ] || [ ${secoct} -lt 1 ] || [ ${secoct} -gt 255 ] || [ ${thirdoct} -lt 1 ] || [ ${thirdoct} -gt 255 ] || [ ${fourthoct} -lt 1 ] || [ ${fourthoct} -gt 255 ]; do\n\n  echo -e \"\\nProvide IP Address: \"\n  \n  read ipaddr\n  \n  firstoct=`echo ${ipaddr} | awk -F'.' '{print $1}'`\n  secoct=`echo ${ipaddr} | awk -F'.' '{print $2}'`\n  thirdoct=`echo ${ipaddr} | awk -F'.' '{print $3}'`\n  fourthoct=`echo ${ipaddr} | awk -F'.' '{print $4}'`\n  \n  if [ ${#ipaddr} -gt 15 ]; then\n    echo -e \"\\nIP address is too long, try again...\"\n    \n  elif [ ${#ipaddr} -lt 7 ]; then\n    echo -e \"\\nIP address is too short, try again...\"\n    \n  elif ! [[ ${ipaddr} =~ .[0-9] ]]; then\n    echo -e \"\\nMust be a number, try again...\"\n    \n  elif [ ${firstoct} -lt 1 ] || [ ${firstoct} -gt 255 ] || [ ${secoct} -lt 1 ] || [ ${secoct} -gt 255 ] || [ ${thirdoct} -lt 1 ] || [ ${thirdoct} -gt 255 ] || [ ${fourthoct} -lt 1 ] || [ ${fourthoct} -gt 255 ]; then\n    echo -e \"\\nEach octect must be a number between 1-255, try again...\"\n    \n  fi\ndone\n\nsbmask=null\n\nwhile ! [[ ${sbmask} =~ [0-9] ]] || [ ${sbmask} -gt 32 ] || [ ${sbmask} -lt 1 ]; do\n  echo -e \"\\nHow many bits is the subnet mask?:\"\n\n  read sbmask\n  \n  if [ ${sbmask} -gt 32 ]; then\n    echo -e \"\\nThere are only 32 bits in a subnet mask, try again...\"\n    \n  elif [ ${sbmask} -lt 1 ]; then\n    echo -e \"\\nMust have at least 1 bit in the subnet mask, try again...\"\n\n  elif ! [[ ${sbmask} =~ [0-9] ]]; then\n    echo -e \"\\nMust be a number, try again...\"\n\n  fi\ndone\n\nif [ ${firstoct} -gt 0 ] && [ ${firstoct} -lt 128 ]; then\n  echo -e \"\\n** CLASS A ADDRESS **\"\n\nelif [ ${firstoct} -gt 127 ] && [ ${firstoct} -lt 192 ]; then\n  echo -e \"\\n** CLASS B ADDRESS **\"\n    \nelif [ ${firstoct} -gt 191 ] && [ ${firstoct} -lt 224 ]; then\n  echo -e \"\\n** CLASS C ADDRESS **\"\n\nelif [ ${firstoct} -gt 223 ] && [ ${firstoct} -lt 240 ]; then\n  echo -e \"\\n** CLASS D ADDRESS **\"\n    \nelif [ ${firstoct} -gt 239 ] && [ ${firstoct} -lt 256 ]; then\n  echo -e \"\\n** CLASS E ADDRESS **\"\n    \nelse\n  return\nfi\necho -e \"\\nIP Address: ${ipaddr}\"\n\nM=$(( 0xffffffff ^ ((1 &lt;&lt; (32-sbmask)) -1) ))\necho -e \"\\nSubnet Mask: $(( (M&gt;&gt;24) & 0xff )).$(( (M&gt;&gt;16) & 0xff )).$(( (M&gt;&gt;8) & 0xff )).$(( M & 0xff ))\"\n\necho -e \"\\nGoodbye!\\n\"\nreturn"
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#firewalld-selinux",
    "href": "posts/linux-admin/linux-admin.html#firewalld-selinux",
    "title": "Linux Administration Project",
    "section": "6.1 Firewalld & SELinux",
    "text": "6.1 Firewalld & SELinux\nMany turn firewalld off to make it easier to configure their services but it is far better to add the port rules that you need to each server for their respective services. In addition, it is a better practice to keep SELinux enabled and to change the security contexts where needed. Throughout the server configuration process these systems were constantly adjusted in the prototype network to allow the services to run but were never disabled. This ensures that the network is as hardened as it can be to intruders."
  },
  {
    "objectID": "posts/linux-admin/linux-admin.html#references",
    "href": "posts/linux-admin/linux-admin.html#references",
    "title": "Linux Administration Project",
    "section": "References",
    "text": "References\n\nNFS\nhttps://www.itzgeek.com/how-tos/linux/centos-how-tos/how-to-setup-nfs-server-on-centos-7-rhel-7-fedora-22.html\n\n\nNFS - Windows\nhttps://it.umn.edu/services-technologies/how-tos/network-file-system-nfs-mount-nfs-share\n\n\nLDAP\nhttps://www.itzgeek.com/how-tos/linux/centos-how-tos/step-step-openldap-server-configuration-centos-7-rhel-7.html\n\n\nLDAP Local User Migration\nhttps://www.itzgeek.com/how-tos/linux/centos-how-tos/migrate-local-users-ldap-accounts.html\n\n\nLDAP Automount Home Directories\nhttps://www.myfaqbase.com/q0001874-Software-OS-Unix-Linux-RHEL-7-RHCSA-How-to-configure-AutoFS-to-mount-user-home-directories-from-NFS-server.html\n\n\nLDAP TLS\nhttps://www.golinuxcloud.com/configure-openldap-with-tls-certificates/\n\n\nSamba\nhttps://linuxize.com/post/how-to-install-and-configure-samba-on-centos-7/\n\n\nPostfix Server\nhttps://www.server-world.info/en/note?os=CentOS_7&p=mail\n\n\nDovcote Server\nhttps://www.server-world.info/en/note?os=CentOS_7&p=mail&f=2"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#purpose-of-document",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#purpose-of-document",
    "title": "Deep Learning FastAI Model",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to detail the building of deep learning models using a convolutional neural network architecture. The different techniques, models and methods used to improve performance will be discussed."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#bee-vs-wasp",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#bee-vs-wasp",
    "title": "Deep Learning FastAI Model",
    "section": "2.1 Bee vs Wasp",
    "text": "2.1 Bee vs Wasp\nFor this project I chose a Bee vs Wasp dataset found on Kaggle. I imported the dataset and created a new folder called “images” to which I added subfolders: “bee1”, “bee2”, “wasp1”, “wasp2”, “other_insect” and “other_noinsect”. The data loader in my custom train_models function then creates classes based on the folder structure and feeds that to the model. The data set itself isn’t the cleanest as it seems that some images have not been placed in the correct folder which will sometimes give the model wrong information. No doubt this will affect the accuracy that can be attained with this dataset.\n\n\n\n\n\n\nDataset\n\n\n\nView Bee vs Wasp dataset on Kaggle."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#trial-and-error",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#trial-and-error",
    "title": "Deep Learning FastAI Model",
    "section": "3.1 Trial and Error",
    "text": "3.1 Trial and Error\nI created a custom function named train_models that I could use to conduct my tests a little faster. With a trial and error approach, I began manually trying different learning rates, model types and image sizes, along with training models with unfrozen weights ( See Figure 1, Table 1, Figure 2 & Table 2 ). Eventually I thought I should start trying to automate some of these tuning methods and, by doing so, hopefully optimize the outcomes.\n\n\n\n\n\n\nNote\n\n\n\nView full details of the trial and error testing in the Experimenting section on the Google Colab Notebook.\n\n\n\n\n#Function to train models more easily\ndef train_models(image_size, batch_size, images_path, test_size, model_type):\n    #instructions for preparing data batches, size of images\n    #and normalize data\n    batch_tfms = [*aug_transforms(size=image_size),Normalize.from_stats(*imagenet_stats)]\n\n    #function for creating batches with specified parameters\n    data = ImageDataLoaders.from_folder(images_path,\n                                        valid_pct=test_size,\n                                        ds_tfms=batch_tfms,\n                                        item_tfms=Resize(460),\n                                        bs=batch_size)\n    \n    # test whether batch function is working with parameters\n    data.show_batch(max_n=9, figsize=(20,10))\n\n    #return the trained model\n    return vision_learner(data, model_type, metrics=error_rate).to_fp16()\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#image size\nimage_size = 224\n\n#batch size, number of images to transfer to GPU to train at one time\nbatch_size = 64\n\n#Image path\nimages_path = \"kaggle_bee_vs_wasp/images\"\n\n#test size\ntest_size = 0.2\n\n#CNN model\nmodel_type = resnet34\n\n#Create model with dataset and parameters\nlearn_resnet34 = train_models(image_size, batch_size, images_path, test_size, model_type)\n\n\n\n\n\nFigure 1. First resnet34 model test\n\n\n\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#train with discovered learning\n#rates and train two more epochs... may improve accuracy\nlearn_resnet34.fit_one_cycle(2, lr_max=slice(1e-6,1e-3))\n\n\n\n\n\n\n\n\nTable 1. First resnet34 best training results\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.179072\n0.169389\n0.055166\n02:12\n\n\n1\n0.105617\n0.161333\n0.046848\n02:08\n\n\n\n\n\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#image size\nimage_size = 224\n\n#batch size, number of images to transfer to GPU to train at one time\nbatch_size = 64\n\n#Image path\nimages_path = \"kaggle_bee_vs_wasp/images\"\n\n#test size\ntest_size = 0.2\n\n#CNN model\nmodel_type = resnet50\n\n#Try resnet50 with same image size as first resnet34 tests\nlearn_resnet50 = train_models(image_size, batch_size, images_path, test_size, model_type)\n\n\n\n\n\nFigure 2. First resnet50 model test\n\n\n\n\nSource: Google Colab Notebook\n\n\n\n\n\nCode\n#save where the model is currently at\nlearn_resnet50.save('stage_2')\n# freeze most of the weights again and train two more epochs\nlearn_resnet50.freeze()\nlearn_resnet50.fit_one_cycle(2)\n\n\n\n\n\n\n\n\nTable 2. First resnet50 best training results\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.152754\n0.199921\n0.056918\n04:58\n\n\n1\n0.082319\n0.159553\n0.042907\n04:59\n\n\n\n\n\n\nSource: Google Colab Notebook"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automating-hyperparameter-tuning",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automating-hyperparameter-tuning",
    "title": "Deep Learning FastAI Model",
    "section": "3.2 Automating Hyperparameter Tuning",
    "text": "3.2 Automating Hyperparameter Tuning\nIn research, I found a Python library called Optuna that could be used to automate hyperparameter tuning. Optuna does this by creating a “study” that runs a user specified amount of trials and uses an objective function to suggest user specified parameters to optimize for a certain metric. So in this case, I created a custom objective function named tune_hyperparameters that takes in learning rate, batch size, and weight decay parameters and returns the error rate of the model trained with those parameters. The Optuna optimize function then suggests hyperparameters that should start lowering the error rate of successive trials. I then wrote another custom function called optimization_study that ran the Optuna study using the tune_hyperparameters function. The optimization_study function also selects the trial that did the best and proceeds to unfreeze all of the weights and train the model again with the best found hyperparameters. Some of my initial tests with this automated hyperparameter tuning proved promising as I was able to get the error rate lower than I had previously gotten it.\n\n\n\n\n\n\nNote\n\n\n\nView full details of the automation testing in the Automate Hyperparameter Tuning section on the Google Colab Notebook.\n\n\n\n\n\n\n# Custom function to choose the best trial and unfreeze weights to train further\ndef optimization_study(selected_model):\n\n    # Create an Optuna study and optimize the objective function\n    study = optuna.create_study(direction=\"minimize\") # Minimize the error rate\n    study.optimize(lambda trial: tune_hyperparameters(trial, image_size, images_path, test_size, selected_model), n_trials=3)\n\n    # Print the best hyperparameters and the corresponding accuracy\n    best_params = study.best_params\n    best_error_rate = study.best_value\n    print(\"Best Hyperparameters:\", best_params)\n    print(\"Best Accuracy:\", best_error_rate)\n\n    #retrieve best model's state dict file\n    best_state_dict_file = f\"{selected_model}_state_dict_trial_{study.best_trial.number}.pth\"\n\n    # Get the best trial from the study\n    best_trial = study.trials[study.best_trial.number]\n\n    # Retrieve the hyperparameters of the best trial\n    hyperparameters = best_trial.params\n\n    # Access individual hyperparameters\n    learning_rate = hyperparameters[\"learning_rate\"]\n    batch_size = hyperparameters[\"batch_size\"]\n    weight_decay = hyperparameters[\"weight_decay\"]\n\n    #Create model with the same architecture as the best trial and load dict file into it\n    best_trial_learn = train_models(image_size, batch_size, images_path, test_size, selected_model)\n    best_trial_learn.model.load_state_dict(torch.load(best_state_dict_file))\n\n    #unfreeze all weights to train with optimal hyperparameters\n    best_trial_learn.unfreeze()\n    # Fit the model with the best trial's hyperparameters\n    best_trial_learn.fine_tune(4, base_lr=learning_rate, wd=weight_decay)\n\n    #close it back up\n    best_trial_learn.freeze()\n\n    #return the model with all of the best results\n    return best_trial_learn\n\nSource: Google Colab Notebook"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automate-testing-different-models",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#automate-testing-different-models",
    "title": "Deep Learning FastAI Model",
    "section": "3.3 Automate Testing Different Models",
    "text": "3.3 Automate Testing Different Models\nAs I started to achieve some good results with my automations I decided to go even further. I wrote another custom function called try_models that loops through a list of different models, runs an Optuna study on it and saves the model state from the best trial from that particular study on that particular model. Once the try_models function has finished looping through the list of models it selects the model that achieved the lowest error rate, creates a learner from that model and loads the model state of the best trial from that model. It then proceeds to unfreeze all of the weights and train the model again with hyperparameters from that particular model’s best trail. After training is complete the function freezes the weights again, displays the results, and returns the model. I found some success using this new function as long as I kept the trial size relatively low as when I increased the trial size it exponentially increases compute time and quickly reaches the limits of free tier kernels.\n\n\n\n\n\n\nNote\n\n\n\nView full details of the try_models function automation testing in the Automate testing different models section on the Kaggle Notebook.\n\n\n\n\n# Custom function to try different models with the other automation functions\ndef try_models(image_size, images_path, test_size, models, trial_size):\n    best_trials = {}\n    for model in models:\n        best_trials[model.__name__] = optimization_study(image_size, images_path, test_size, model, trial_size)\n        \n    best_overall = min(best_trials,  key=lambda x: best_trials[x].value)\n    lowest_model = best_trials[best_overall]\n    print(\"\\n\\nBest Overall Model:\", lowest_model.user_attrs['model'].__name__)\n    print(\"Error Rate:\", lowest_model.value)\n    print(\"Load Model's state and retrain with best hyperparameters\")\n    \n    \n    #retrieve best model's state dict file\n    best_state_dict_file = f\"/kaggle/working/{lowest_model.user_attrs['model'].__name__}_state_dict_trial_{lowest_model.number}.pth\"\n\n    \n    # Retrieve the hyperparameters of the best trial\n    hyperparameters = lowest_model.params\n\n    # Access individual hyperparameters\n    learning_rate = hyperparameters[\"learning_rate\"]\n    batch_size = hyperparameters[\"batch_size\"]\n    weight_decay = hyperparameters[\"weight_decay\"]\n\n    #Create model with the same architecture as the best trial and load dict file into it\n    best_model_learn = train_models(image_size, batch_size, images_path, test_size, lowest_model.user_attrs[\"model\"])\n    best_model_learn.model.load_state_dict(torch.load(best_state_dict_file))\n\n    #unfreeze all weights to train with optimal hyperparameters\n    best_model_learn.unfreeze()\n    # Fit the model with the best trial's hyperparameters\n    best_model_learn.fine_tune(1, base_lr=learning_rate, wd=weight_decay)\n\n    #close it back up\n    best_model_learn.freeze()\n    \n    # Evaluate the model on the validation set\n    best_error_rate = best_model_learn.validate()[1]\n    \n    print(f\"\\n\\nFinal result after training model \"+lowest_model.user_attrs['model'].__name__+\" with:\")\n    print(\"Hyperparameters:\", hyperparameters)\n    print(\"Error Rate:\", best_error_rate)\n\n    \n    #return the model with all of the best results\n    return best_model_learn\n\nSource: Kaggle Notebook"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#data-augmentation",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#data-augmentation",
    "title": "Deep Learning FastAI Model",
    "section": "3.4 Data Augmentation",
    "text": "3.4 Data Augmentation\nI also briefly experimented with some data augmentation, namely randomly cropping to a 224x224 image size and introducing a random horizontal flip to the images. Tests with this didn’t seem to yield any improved results, in fact it seems it may have adversely affected model performance in training. I theorize that this didn’t have much effect because the dataset already possesses a great deal of randomness so injecting more isn’t advantageous."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#usage-limits",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#usage-limits",
    "title": "Deep Learning FastAI Model",
    "section": "4.1 Usage Limits",
    "text": "4.1 Usage Limits\nVery early on it was clear that usage limits of free tier kernels would significantly limit the ability to experiment, test and iterate. For this reason, the approach was taken to use more than one kernel so that when one reached its limit the other could be used to continue with the project. Google Colab and Kaggle were both used to complete this project and in the following two items ( 4.2 Google Colab & 4.3 Kaggle ) in this section I detail what each kernel was primarily used for. A notebook from each kernel is provided in this project submission, with Part 1 and Part 3 being included in the Google Colab notebook and Part 2 being included in the Kaggle notebook."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#google-colab",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#google-colab",
    "title": "Deep Learning FastAI Model",
    "section": "4.2 Google Colab",
    "text": "4.2 Google Colab\nI started my initial experimentation in Google Colab and that is why it starts with the heading Part 1. Part way through the refinement of my custom automation functions I reached my limit with Google Colab so Part 2 of my code is found in the Kaggle notebook. The final part of my testing and code can be found under Part 3 of the Google Colab notebook. In Part 3, I decided to purchase some Pay-As-You-Go compute so that I could continue the rest of my project without further delays."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#kaggle",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#kaggle",
    "title": "Deep Learning FastAI Model",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\nThe Kaggle notebook starts with the heading of Part 2 as it is the point where I switched from Google Colab. The Kaggle notebook only includes one part and it is where most of the refinements on my custom functions can be found. I was able to make some fairly large tests at the end of the Kaggle notebook but then reached my limit. At this point I switched back to finish things off in my Google Colab notebook under Part 3."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#worst-performance",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#worst-performance",
    "title": "Deep Learning FastAI Model",
    "section": "5.1 Worst Performance",
    "text": "5.1 Worst Performance\nI wasn’t able to test VGG16 to long before I ran into limit restrictions on the kernel but it wasn’t performing all that well from what was seen. Further investigation would be required to confirm that VGG16 is not a good model for this dataset. SqueezeNet models did not perform as well as the other models which is not surprising giving the size and architecture of SqueezeNet models ( See Figure 3 ). The 896x896 image size did not seem to yield better results and neither did batch sizes 16 and 64. Learning rate range 1e-5 - 1e-1 did not yield good results as well as weight decay range 1e-5 – 1e-3.\n\n\n\nFigure 3. Best SqueezeNet model results after optimization automations were the worst results."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#best-performance",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#best-performance",
    "title": "Deep Learning FastAI Model",
    "section": "5.2 Best Performance",
    "text": "5.2 Best Performance\nAfter studying some of the tests I started to isolate that a batch size of 32 did consistently well. Along with training only with a 32 batch size I narrowed the learning rate range to 1e-3 – 1e-2 and the weight decay range to 1e-5 -1e-4 as these ranges seems to provide the best results. In the end of all my testing the best performance I achieved was from a Resnet32 model trained with a 224 image size, 32 batch size, a learning rate of 3.102551277095900e-3 and a weight decay of 7.49113519525403e-05. This yielded a model with a training loss of 0.022758, valid loss of 0.065226, and error rate of 0.015762. These results show that the model is slightly overfitted but performing quite well. ( See Figure 4 )\n\n\n\nFigure 4. Best Resnet model results after optimization automations were the best performance."
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#model-training-video-walkthrough",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#model-training-video-walkthrough",
    "title": "Deep Learning FastAI Model",
    "section": "Model Training Video Walkthrough",
    "text": "Model Training Video Walkthrough\nVideo"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#references",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#references",
    "title": "Deep Learning FastAI Model",
    "section": "References",
    "text": "References\nOptimization Library\nhttps://optuna.org/"
  },
  {
    "objectID": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#deployment",
    "href": "posts/intro-to-ai-fastai/intro-to-ai-fastai.html#deployment",
    "title": "Deep Learning FastAI Model",
    "section": "Deployment",
    "text": "Deployment\n\nDeployment Source Code\nView the source code for the model deployment on HuggingFace repository.\n\n\nGradio App\nView Gradio app for model deployment hosted on HuggingFace.\n\n\nDeployment Resources\nHugging Face - AI community & place to host ML deployments\nhttps://huggingface.co/\nGradio - Open-source Python library used to build machine learning/data science demos & web applications\nhttps://www.gradio.app/"
  },
  {
    "objectID": "posts/networking-cisco/networking-cisco.html",
    "href": "posts/networking-cisco/networking-cisco.html",
    "title": "Networking with Cisco Technologies",
    "section": "",
    "text": "Designed and constructed a fictional network with five (5) departments in two (2) locations using GNS3 simulation software. All departments were represented by Windows 10 VMs run on VMware and were configured to communicate with each other thru GNS3. Variable length subnetting from an address space of 172.16.16.0/20 was used to create department subnets. Routing protocol OSPF along with ACLs were used to allow departments to communicate with each other while at the same time preventing certain ones from reaching others. See figures below for full documentation of the project.\n\n\n\n\n\n\nExpand To Learn About Project Scenario\n\n\n\n\n\nThe head office is located in Vancouver and has three departments. These departments are Accounting, Administration, and Human Resources. The Accounting department uses 10 computers, Administration uses 19 computers, and Human Resources uses 3 computers.\nThe branch office is located in Kelowna and has two departments. These departments are Marketing and Sales. The Marketing department uses 16 computers, and the Sales department uses 45 computers.\n\n\n\n\n\n\n\n\nFigure 1. Network Diagram\n\n\n\n\n\n\n\n\nFigure 2. IP Address Scheme\n\n\n\n\n\n\n\n\nFigure 3. Access Control Lists\n\n\n\n\n\n\n\n\nFigure 4. Router Access Credentials\n\n\n\n\n\n\nListing 1. Vancouver Router Configuration\n!\n!\nversion 12.4\nservice timestamps debug datetime msec\nservice timestamps log datetime msec\nno service password-encryption\n!\nhostname Vancouver\n!\nboot-start-marker\nboot-end-marker\n!\nenable secret 5 $1$rCPC$Q34x6NTdMojTQHgQQwDe7/\n!\nno aaa new-model\nmemory-size iomem 5\nno ip icmp rate-limit unreachable\n!\n!\nip cef\nno ip domain lookup\n!\n!\n!\n!\n!\nip tcp synwait-time 5\n!\n!\ninterface Ethernet0/0\n  description Router Network\n  ip address 172.16.16.153 255.255.255.252\n  half-duplex\n!\ninterface Ethernet0/1\n  description HR Network\n  ip address 172.16.16.148 255.255.255.248\n  ip access-group 3 out\n  half-duplex\n!\ninterface Ethernet0/2\n  description Admin & SMTP/WebServer Network\n  ip address 172.16.16.84 255.255.255.224\n  ip access-group 110 out\n  half-duplex\n!\ninterface Ethernet0/3\n  description Accounting Network\n  ip address 172.16.16.139 255.255.255.240\n  ip access-group 2 out\n  half-duplex\n!\nrouter ospf 1\n  log-adjacency-changes\n  network 172.16.16.64 0.0.0.31 area 0\n  network 172.16.16.128 0.0.0.15 area 0\n  network 172.16.16.144 0.0.0.7 area 0\n  network 172.16.16.152 0.0.0.3 area 0\n!\nno ip http server\n!\nip forward-protocol nd\n!\n!\naccess-list 2 deny   172.16.16.96 0.0.0.31\naccess-list 2 deny   172.16.16.0 0.0.0.63\naccess-list 2 permit any\naccess-list 3 deny   172.16.16.0 0.0.0.63\naccess-list 3 permit any\naccess-list 110 deny   tcp 172.16.16.128 0.0.0.15 host 172.16.16.66 eq\nwww\naccess-list 110 deny   tcp 172.16.16.144 0.0.0.7 host 172.16.16.66 eq\nwww\naccess-list 110 deny   tcp 172.16.16.0 0.0.0.63 host 172.16.16.66 eq\nsmtp\naccess-list 110 deny   tcp 172.16.16.96 0.0.0.31 host 172.16.16.66 eq\nsmtp\naccess-list 110 permit ip any any\nno cdp log mismatch duplex\n!\ncontrol-plane\n!\nbanner motd\n\nWelcome to Vancouver\n\n!\nline con 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline aux 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline vty 0 4\n  password 1234\n  login\n!\n!\nend\n\n\nListing 2. Kelowna Router Configuration\n!\n!\nversion 12.4\nservice timestamps debug datetime msec\nservice timestamps log datetime msec\nno service password-encryption\n!\nhostname Kelowna\n!\nboot-start-marker\nboot-end-marker\n!\nenable secret 5 $1$dGub$MNu5a6gEp0IcO1T14Qlg4/\n!\nno aaa new-model\nmemory-size iomem 5\nno ip icmp rate-limit unreachable\n!\n!\nip cef\nno ip domain lookup\n!\n!\n!\n!\n!\nip tcp synwait-time 5\n!\n!\ninterface Ethernet0/0\n  description Router Network\n  ip address 172.16.16.154 255.255.255.252\n  half-duplex\n!\ninterface Ethernet0/1\n  description Sales Network\n  ip address 172.16.16.46 255.255.255.192\n  ip access-group 1 out\n  half-duplex\n!\ninterface Ethernet0/2\n  description Marketing Network\n  ip address 172.16.16.113 255.255.255.224\n  ip access-group 4 out\n  half-duplex\n!\ninterface Ethernet0/3\n  description Nothing\n  no ip address\n  shutdown\n  half-duplex\n!\nrouter ospf 1\n  log-adjacency-changes\n  network 172.16.16.0 0.0.0.63 area 0\n  network 172.16.16.96 0.0.0.31 area 0\n  network 172.16.16.152 0.0.0.3 area 0\n!\nno ip http server\n!\nip forward-protocol nd\n!\n!\naccess-list 1 deny   172.16.16.128 0.0.0.15\naccess-list 1 deny   172.16.16.96 0.0.0.31\naccess-list 1 permit any\naccess-list 4 deny   172.16.16.0 0.0.0.63\naccess-list 4 permit any\nno cdp log mismatch duplex\n!\ncontrol-plane\n!\nbanner motd\n\n\nWelcome to Kelowna\n\n\n!\nline con 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline aux 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline vty 0 4\n  password 1234\n  login\n!\n!\nend"
  },
  {
    "objectID": "posts/networking-cisco/networking-cisco.html#network-diagram",
    "href": "posts/networking-cisco/networking-cisco.html#network-diagram",
    "title": "Networking with Cisco Technologies",
    "section": "",
    "text": "Figure 1. Network Diagram"
  },
  {
    "objectID": "posts/networking-cisco/networking-cisco.html#ip-address-scheme",
    "href": "posts/networking-cisco/networking-cisco.html#ip-address-scheme",
    "title": "Networking with Cisco Technologies",
    "section": "",
    "text": "Figure 2. IP Address Scheme"
  },
  {
    "objectID": "posts/networking-cisco/networking-cisco.html#access-control-lists",
    "href": "posts/networking-cisco/networking-cisco.html#access-control-lists",
    "title": "Networking with Cisco Technologies",
    "section": "",
    "text": "Figure 3. Access Control Lists"
  },
  {
    "objectID": "posts/networking-cisco/networking-cisco.html#router-access-credentials",
    "href": "posts/networking-cisco/networking-cisco.html#router-access-credentials",
    "title": "Networking with Cisco Technologies",
    "section": "",
    "text": "Figure 4. Router Access Credentials"
  },
  {
    "objectID": "posts/networking-cisco/networking-cisco.html#router-configurations",
    "href": "posts/networking-cisco/networking-cisco.html#router-configurations",
    "title": "Networking with Cisco Technologies",
    "section": "",
    "text": "Listing 1. Vancouver Router Configuration\n!\n!\nversion 12.4\nservice timestamps debug datetime msec\nservice timestamps log datetime msec\nno service password-encryption\n!\nhostname Vancouver\n!\nboot-start-marker\nboot-end-marker\n!\nenable secret 5 $1$rCPC$Q34x6NTdMojTQHgQQwDe7/\n!\nno aaa new-model\nmemory-size iomem 5\nno ip icmp rate-limit unreachable\n!\n!\nip cef\nno ip domain lookup\n!\n!\n!\n!\n!\nip tcp synwait-time 5\n!\n!\ninterface Ethernet0/0\n  description Router Network\n  ip address 172.16.16.153 255.255.255.252\n  half-duplex\n!\ninterface Ethernet0/1\n  description HR Network\n  ip address 172.16.16.148 255.255.255.248\n  ip access-group 3 out\n  half-duplex\n!\ninterface Ethernet0/2\n  description Admin & SMTP/WebServer Network\n  ip address 172.16.16.84 255.255.255.224\n  ip access-group 110 out\n  half-duplex\n!\ninterface Ethernet0/3\n  description Accounting Network\n  ip address 172.16.16.139 255.255.255.240\n  ip access-group 2 out\n  half-duplex\n!\nrouter ospf 1\n  log-adjacency-changes\n  network 172.16.16.64 0.0.0.31 area 0\n  network 172.16.16.128 0.0.0.15 area 0\n  network 172.16.16.144 0.0.0.7 area 0\n  network 172.16.16.152 0.0.0.3 area 0\n!\nno ip http server\n!\nip forward-protocol nd\n!\n!\naccess-list 2 deny   172.16.16.96 0.0.0.31\naccess-list 2 deny   172.16.16.0 0.0.0.63\naccess-list 2 permit any\naccess-list 3 deny   172.16.16.0 0.0.0.63\naccess-list 3 permit any\naccess-list 110 deny   tcp 172.16.16.128 0.0.0.15 host 172.16.16.66 eq\nwww\naccess-list 110 deny   tcp 172.16.16.144 0.0.0.7 host 172.16.16.66 eq\nwww\naccess-list 110 deny   tcp 172.16.16.0 0.0.0.63 host 172.16.16.66 eq\nsmtp\naccess-list 110 deny   tcp 172.16.16.96 0.0.0.31 host 172.16.16.66 eq\nsmtp\naccess-list 110 permit ip any any\nno cdp log mismatch duplex\n!\ncontrol-plane\n!\nbanner motd\n\nWelcome to Vancouver\n\n!\nline con 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline aux 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline vty 0 4\n  password 1234\n  login\n!\n!\nend\n\n\nListing 2. Kelowna Router Configuration\n!\n!\nversion 12.4\nservice timestamps debug datetime msec\nservice timestamps log datetime msec\nno service password-encryption\n!\nhostname Kelowna\n!\nboot-start-marker\nboot-end-marker\n!\nenable secret 5 $1$dGub$MNu5a6gEp0IcO1T14Qlg4/\n!\nno aaa new-model\nmemory-size iomem 5\nno ip icmp rate-limit unreachable\n!\n!\nip cef\nno ip domain lookup\n!\n!\n!\n!\n!\nip tcp synwait-time 5\n!\n!\ninterface Ethernet0/0\n  description Router Network\n  ip address 172.16.16.154 255.255.255.252\n  half-duplex\n!\ninterface Ethernet0/1\n  description Sales Network\n  ip address 172.16.16.46 255.255.255.192\n  ip access-group 1 out\n  half-duplex\n!\ninterface Ethernet0/2\n  description Marketing Network\n  ip address 172.16.16.113 255.255.255.224\n  ip access-group 4 out\n  half-duplex\n!\ninterface Ethernet0/3\n  description Nothing\n  no ip address\n  shutdown\n  half-duplex\n!\nrouter ospf 1\n  log-adjacency-changes\n  network 172.16.16.0 0.0.0.63 area 0\n  network 172.16.16.96 0.0.0.31 area 0\n  network 172.16.16.152 0.0.0.3 area 0\n!\nno ip http server\n!\nip forward-protocol nd\n!\n!\naccess-list 1 deny   172.16.16.128 0.0.0.15\naccess-list 1 deny   172.16.16.96 0.0.0.31\naccess-list 1 permit any\naccess-list 4 deny   172.16.16.0 0.0.0.63\naccess-list 4 permit any\nno cdp log mismatch duplex\n!\ncontrol-plane\n!\nbanner motd\n\n\nWelcome to Kelowna\n\n\n!\nline con 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline aux 0\n  exec-timeout 0 0\n  privilege level 15\n  logging synchronous\nline vty 0 4\n  password 1234\n  login\n!\n!\nend"
  },
  {
    "objectID": "posts/emcs/emcs.html",
    "href": "posts/emcs/emcs.html",
    "title": "Microsoft Exchange Server Network",
    "section": "",
    "text": "Deployed an Exchange server environment using Microsoft Exchange Server 2016. Created three (3) domain controllers, two (2) mailbox servers, and one (1) edge transport server on VMware. One (1) of the domain controllers was running Server Core. Formatted disks with different file systems and created simple, spanned, stripped, mirrored, and RAID-5 volumes. Installed and configured Active Directory including creating and managing OU structure, users, and groups. Created an Active Directory backup and restored from backup. Installed and upgraded Exchange Server 2016. Created mailbox databases in a DAG. Configured namespaces, SSL certificates, and mail flow. Spent extensive time using PowerShell and Exchange Management Shell for installs and configuration. See project screenshots below.\n\n\n\n\n\nFigure 1. Domain Controller 1 - IP Configurations\n\n\n\n\n\nFigure 2. Domain Controller 3 (Server Core) - IP Configurations\n\n\n\n\n\nFigure 3. Domain Controller 3 (Server Core) - Firewall Configurations\n\n\n\n\n\n\n\n\nFigure 4. Domain Controller 2 - Raid Volumes\n\n\n\n\n\n\n\n\nFigure 5. Active Directory Installation\n\n\n\n\n\nFigure 6. Active Directory - OUs & Users\n\n\n\n\n\n\n\n\nFigure 7. Exchange Server Install\n\n\n\n\n\nFigure 8. Mailbox Databases\n\n\n\n\n\nFigure 9. Recipients\n\n\n\n\n\nFigure 10. Resource Mailboxes & Distribution Groups"
  },
  {
    "objectID": "posts/emcs/emcs.html#domain-controller-configurations",
    "href": "posts/emcs/emcs.html#domain-controller-configurations",
    "title": "Microsoft Exchange Server Network",
    "section": "",
    "text": "Figure 1. Domain Controller 1 - IP Configurations\n\n\n\n\n\nFigure 2. Domain Controller 3 (Server Core) - IP Configurations\n\n\n\n\n\nFigure 3. Domain Controller 3 (Server Core) - Firewall Configurations"
  },
  {
    "objectID": "posts/emcs/emcs.html#file-systems",
    "href": "posts/emcs/emcs.html#file-systems",
    "title": "Microsoft Exchange Server Network",
    "section": "",
    "text": "Figure 4. Domain Controller 2 - Raid Volumes"
  },
  {
    "objectID": "posts/emcs/emcs.html#active-directory",
    "href": "posts/emcs/emcs.html#active-directory",
    "title": "Microsoft Exchange Server Network",
    "section": "",
    "text": "Figure 5. Active Directory Installation\n\n\n\n\n\nFigure 6. Active Directory - OUs & Users"
  },
  {
    "objectID": "posts/emcs/emcs.html#exhange-server",
    "href": "posts/emcs/emcs.html#exhange-server",
    "title": "Microsoft Exchange Server Network",
    "section": "",
    "text": "Figure 7. Exchange Server Install\n\n\n\n\n\nFigure 8. Mailbox Databases\n\n\n\n\n\nFigure 9. Recipients\n\n\n\n\n\nFigure 10. Resource Mailboxes & Distribution Groups"
  },
  {
    "objectID": "posts/laserman/laserman.html",
    "href": "posts/laserman/laserman.html",
    "title": "Retro C++ Game LaserMan",
    "section": "",
    "text": "Play Game\n\n\n\n\n\n\nPlayer Keyboard Controls\n\n\n\n\n\n\n\n\nMove\nArrow Keys\nSelect\nEnter\n\n\nEscape Program\nEsc\n\n\n\n\n\n\n🚀 Play Game\nDownload, unzip and open LaserMan.exe (Only runs on Windows)\n\n\n\n\n\nRetro Game\nProject to create a retro game using a beginner’s game-development framework called Playbuffer all written in C++ strictly for Windows. Programming algorithms and techniques such as state machines and object oriented programming were developed in the game loop.\n\n\n\n\n\n\nSource Code\n\n\n\nView source code on Github repository.\n\n\n\n\nVideo Walkthrough\nVideo"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#purpose-of-document",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#purpose-of-document",
    "title": "Hotel Booking System Mock Design",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide system designs to use in the development of the hotel booking system for City and Resort hotels."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#scope",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#scope",
    "title": "Hotel Booking System Mock Design",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an examination of the business scenario to ascertain how best to design, develop, and improve upon the proposed hotel booking system. In addition, creation of system designs and implementation documentation must be produced to drive the development of the project. The proposed solutions must include detailed logical & physical Entity-Relationship Diagrams, a Context Diagram, and up to Level 2 Data Flow Diagrams."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#waterfall",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#waterfall",
    "title": "Hotel Booking System Mock Design",
    "section": "2.1 Waterfall",
    "text": "2.1 Waterfall\nThe Waterfall methodology contains five phases that are as follows: Requirements, Design, Implementation, Verification and Maintenance. Each of these phases are present in most methodologies however with the Waterfall approach each phase is followed by the next in strict fashion. The team can only progress to the next phase when the current phase has been completed and you cannot revert back to a previous phase. This approach is useful when a high level of reliability is needed for a project."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#agile",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#agile",
    "title": "Hotel Booking System Mock Design",
    "section": "2.2 Agile",
    "text": "2.2 Agile\nThe Agile method is an iterative approach that allows for similar phases as Waterfall to be engaged simultaneously and in no specific order. The team is able to work in small increments allowing for customer input regularly. This methodology is far more flexible in terms of adapting to changes in customer wants and needs as they see the project develop."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#preferred-software-development-methodology",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#preferred-software-development-methodology",
    "title": "Hotel Booking System Mock Design",
    "section": "2.3 Preferred Software Development Methodology",
    "text": "2.3 Preferred Software Development Methodology\nThe fact that there are two hotels that will be using this system in essence means that there two clients to consider in the development of this project. Each client has unique requirements that differ from one another and are subject to change. As the hospitality industry is fast paced and endeavours to adapt to ever changing customer opinions, the Agile methodology is best suited for this project. As stated above, Waterfall is great for certain projects but is highly inflexible while Agile allows for development teams to incorporate client input in small, digestible increments. This approach will translate to a product that provides more utility to the customer which in turn will result in higher customer satisfaction. ( See Appendix – Waterfall Methodology & Agile Methodology )"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#logical-entity-relationship-diagram",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#logical-entity-relationship-diagram",
    "title": "Hotel Booking System Mock Design",
    "section": "3.1 Logical Entity-Relationship Diagram",
    "text": "3.1 Logical Entity-Relationship Diagram\nThe four entities in this diagram are as follows: Customer, City Reservation, Resort Reservation and Invoice. Each entity has a primary key that consists of a uniquely generated number. The system is purposefully designed with no weak entities and foreign keys as this is not advisable to create a robust system. The Customer can either book a reservation at the City or Resort hotel. There is a need to delineate the City and Resort hotel reservations because of their individual discount policies. Each hotel will only reward their customers with a discount when they have previously stayed at their specific location before. In addition, the Resort hotel will only provide a discount to customers who haven’t had any cancellations at their location in the past year. Therefore the Resort Reservation entity has an addition attribute labelled “Cancelled” that records whether any of the reservations have been cancelled. With the information supplied from the reservation entities the Invoice is populated. ( See Figure 1 )\n\n\n\nFigure 1. Logical Entity-Relationship Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#physical-entity-relationship-diagram",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#physical-entity-relationship-diagram",
    "title": "Hotel Booking System Mock Design",
    "section": "3.2 Physical Entity-Relationship Diagram",
    "text": "3.2 Physical Entity-Relationship Diagram\nThe invoice entity contains a composite and multivalued attribute labelled “Payment” that allows for storage of the date and amount for payments made towards the total. When payments are made the “Amount Owing” attribute on the entry is updated accordingly. ( See Figure 2 )\n\n\n\nFigure 2. Physical Entity-Relationship Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#data-fields-with-sample-data",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#data-fields-with-sample-data",
    "title": "Hotel Booking System Mock Design",
    "section": "3.3 Data Fields with Sample Data",
    "text": "3.3 Data Fields with Sample Data\n\n\n\nFigure 3. Data Fields with Sample Data"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#context-diagram",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#context-diagram",
    "title": "Hotel Booking System Mock Design",
    "section": "4.1 Context Diagram",
    "text": "4.1 Context Diagram\nSix (6) entities: Customer, City Hotel, Resort Hotel, Car Parking Agency, Restaurant and ABC Bank interact with the system from a data flow perspective. A hotel only interacts with the system if the customer has chosen the specified hotel. Each entity’s interactions with the system will be described in further depth in the items to follow. ( See Figure 4 )\n\n\n\nFigure 4. Context Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-1-dfd",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-1-dfd",
    "title": "Hotel Booking System Mock Design",
    "section": "4.2 Level 1 DFD",
    "text": "4.2 Level 1 DFD\nThere are three (3) processes depicted at level 1 that handle the flow of data for the system. Process 1, “Booking”, involves making a reservation, obtaining missing information for the reservation and cancelling the reservation. Process 2, “Check-In”, is where the customer checks into their room and makes an initial payment. Process 3, “Check-Out”, ends the customer’s interaction with the system by settling the remaining balance on the invoice and paying the parking invoice if the customer has opted for parking. ( See Figure 5 )\n\n\n\nFigure 5. Level 1 Data Flow Diagram"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-booking-process-1",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-booking-process-1",
    "title": "Hotel Booking System Mock Design",
    "section": "4.3 Level 2 DFD – Booking Process 1",
    "text": "4.3 Level 2 DFD – Booking Process 1\nThere are five (5) sub-processes depicted at level 2 of “Booking” Process 1. In Process 1.1 the customer submits their customer details which are then stored. Process 1.2 generates room and date availability information by examining current reservations which is then used by the customer to submit their reservation preferences. In Process 1.3 the reservation details are stored and the hotel sends the customer a reservation code and request, in the form of an email, to submit missing information. The only none mandatory fields that may be missing are “Food Requirements” and “Parking”. ( See Figure 1, Figure 2 and Figure 3 ) Process 1.4 gives the customer an opportunity to submit the missing information by clicking a link provided in the information request email and filling in the missing fields. This new information is then sent to the hotel, restaurant and parking agency along with being saved in the customer’s reservation entry. Process 1.5 allows the customer to cancel the reservation if desired. When a cancellation is made the “Cancellation” field is updated if it is a Resort reservation entry or it deletes the entry in the case of a City reservation. All relevant parties are then informed of the cancellation. ( See Figure 6 )\n\n\n\nFigure 6. Level 2 Data Flow Diagram Booking Process 1"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-in-process-2",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-in-process-2",
    "title": "Hotel Booking System Mock Design",
    "section": "4.4 Level 2 DFD – Check-In Process 2",
    "text": "4.4 Level 2 DFD – Check-In Process 2\nThere are four (4) sub-processes depicted at level 2 of “Check-In” Process 2. In Process 2.1 the customer submits their reservation code which is then used to extract the customer’s details and reservation. This information is used by the hotel to generate an invoice which is then stored and a copy is sent to the customer in Process 2.2. In Process 2.3 the customer submits payment information which is then received by ABC bank along with the invoice. In Process 2.4 the bank handles the payment, sends payment confirmation to all relevant parties and the invoice entry is updated with payment details. ( See Figure 7 )\n\n\n\nFigure 7. Level 2 Data Flow Diagram Check-In Process 2"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-out-process-3",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#level-2-dfd-check-out-process-3",
    "title": "Hotel Booking System Mock Design",
    "section": "4.5 Level 2 DFD – Check-Out Process 3",
    "text": "4.5 Level 2 DFD – Check-Out Process 3\nThere are three (3) sub-processes depicted at level 2 of “Check-Out” Process 3. In Process 3.1 the customer submits their reservation code which is then used by the hotel to bring up the customer’s details, reservation and invoice. The customer details and reservation are also sent to the Car Parking Agency which uses Process 3.2 to send the customer a parking invoice along with their hotel invoice. In Process 3.3 the customer submits payment information for both invoices, which is then received by ABC bank along with both invoices. In Process 3.4 the bank processes the payment and sends payment confirmation to all relevant parties and the hotel invoice entry is updated with payment details. ( See Figure 8 )\n\n\n\nFigure 8. Level 2 Data Flow Diagram Check-Out Process 3"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#additional-review-process",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#additional-review-process",
    "title": "Hotel Booking System Mock Design",
    "section": "5.1 Additional Review Process",
    "text": "5.1 Additional Review Process\nAn improvement upon the system could be achieved by adding a fourth process at level 1 to prompt the customer to provide a review. The review information could then be stored in an additional field in the reservation data stores. This review information could then be analyzed by AI technology to produce actionable suggestions on how to improve the customer’s experience."
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#sentiment-intent-analysis",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#sentiment-intent-analysis",
    "title": "Hotel Booking System Mock Design",
    "section": "5.2 Sentiment & Intent Analysis",
    "text": "5.2 Sentiment & Intent Analysis\nNatural language processing could be employed to analyze thousands of reviews to identify recurring themes and topics reported on by customers. These can then be rated so that hotel staff can see how customers feel and why they feel that way. This would allow for staff to see where best to make improvements to their services thereby increasing customer satisfaction. ( See Appendix – Sentiment & Intent Analysis )"
  },
  {
    "objectID": "posts/dev-design-assignment1/design-dev-assignment1.html#references",
    "href": "posts/dev-design-assignment1/design-dev-assignment1.html#references",
    "title": "Hotel Booking System Mock Design",
    "section": "References",
    "text": "References\n\nWaterfall Methodology\nhttps://www.projectmanager.com/guides/waterfall-methodology/\n\n\nAgile Methodology\nhttps://www.atlassian.com/agile\n\n\n\n\nSentiment & Intent Analysis\nhttps://www.lexalytics.com/technology/sentiment-analysis/\nhttps://www.lexalytics.com/technology/intentions/"
  },
  {
    "objectID": "posts/unreal-design-patterns/unreal-design-patterns.html#command-design-pattern-c",
    "href": "posts/unreal-design-patterns/unreal-design-patterns.html#command-design-pattern-c",
    "title": "Programming Design Patterns",
    "section": "2.1 Command Design Pattern (C++)",
    "text": "2.1 Command Design Pattern (C++)\nCreated C++ classes that inherit from UObject class. Three command child classes(Jump, Run and Fire) inherit from a base command class. Command objects are created by the level blueprint based on keyboard and mouse inputs. These command objects then go on to execute the specific command they were designed for."
  },
  {
    "objectID": "posts/unreal-design-patterns/unreal-design-patterns.html#state-machine-design-pattern-c",
    "href": "posts/unreal-design-patterns/unreal-design-patterns.html#state-machine-design-pattern-c",
    "title": "Programming Design Patterns",
    "section": "2.2 State Machine Design Pattern (C++)",
    "text": "2.2 State Machine Design Pattern (C++)\nCreated C++ state classes are that inherit from UObject class and are managed by a StateManager class that inherits from Actor component class. The StateManager actor component object is created and attached to the enemy character class. The StateManager component then creates the three different state objects, switches the current state based on certain conditions and runs the current state."
  },
  {
    "objectID": "posts/unreal-design-patterns/unreal-design-patterns.html#object-pool-design-pattern-blueprints",
    "href": "posts/unreal-design-patterns/unreal-design-patterns.html#object-pool-design-pattern-blueprints",
    "title": "Programming Design Patterns",
    "section": "2.3 Object Pool Design Pattern (Blueprints)",
    "text": "2.3 Object Pool Design Pattern (Blueprints)\nCreated Blueprint BulletPool class actor component that attaches to actors and created a set number of bullets from the blueprint bullet class. The bulletpool than attaches the created bullets to the character and when the chsaracter’s fire method is invoked it uses the bullets from the bullet pool. If the bullet pool becomes empty then the player must wait for it to refill before firing more bullets."
  },
  {
    "objectID": "posts/unreal-design-patterns/unreal-design-patterns.html#prototype-design-pattern-blueprints",
    "href": "posts/unreal-design-patterns/unreal-design-patterns.html#prototype-design-pattern-blueprints",
    "title": "Programming Design Patterns",
    "section": "2.4 Prototype Design Pattern (Blueprints)",
    "text": "2.4 Prototype Design Pattern (Blueprints)\nCreated Blueprint spawn classes that spawn different enemy characters from blueprint child enemy character classes.\nOne type of spawn class has a public variable that can be set in the unreal inspector for each instance that stores a specific enemy character class. It then spawns a character based on the saved enemy character class and saves it and continues to invoke that intial spawned character’s Clone method to spawn more characters of that class. This spawner creates and stores only one prototype of one type of enemy to spawn further enemies from.\nThe other type of spawn class stores all of the different enemy classes. It randomly selects an enemy class from the list of enemy classes, spawns an instance of this class, runs the Clone method of that instance to spawn an enemy, and finally destroys the initial instance of the enemy. So this spawner type can spawn any type of enemy by creating and using a prototype of that enemy."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#initial-data-overview",
    "href": "posts/data-analytics/data-analytics.html#initial-data-overview",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "1.1 Initial Data Overview",
    "text": "1.1 Initial Data Overview\nUpon loading the dataset using the pandas library in Python, we observed that it contained 20,000 entries with 7 columns. An initial assessment revealed the presence of missing values in the avg_lat_down_ms and avg_lat_up_ms columns. (Fig. 1)\n\n\n\nCode\n# Create a deep copy of the dataframe for cleaning\ncleaning_df = df.copy()\n\n# Not many rows with missing values as shown in plot\ncleaning_df.isna().sum().plot(kind='bar', ylim=(0, cleaning_df.shape[0]))\n\n# Drop rows with missing values\ncleaning_df.dropna(inplace = True)\n\n\n\n\n\nFigure 1. Bar plot of missing values in each column\n\n\n\n\nSource: Description.ipynb"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#data-cleaning-steps",
    "href": "posts/data-analytics/data-analytics.html#data-cleaning-steps",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "1.2 Data Cleaning Steps",
    "text": "1.2 Data Cleaning Steps\n\nDropping Rows with Missing Values: Rows containing missing values were dropped to ensure the reliability of our subsequent analyses. (Fig. 1)\nColumn Removal: We removed the unnecessary Unnamed: 0 column, as it served as an unnamed index and did not contribute to the analysis.\nSpelling Corrections and Categorization: We addressed spelling errors in the net_type column, changing ‘moblie’ to ‘Mobile’ and capitalizing ‘fixed’. The net_type column was then converted to a categorical data type.\nDuplicate Entry Removal: Duplicate entries were identified and subsequently dropped to ensure the uniqueness of our data.\nConversion of Float Columns to Int: We verified the avg_lat_down_ms and avg_lat_up_ms columns for floating-point values and converted them to integers if necessary."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#column-renaming-and-unit-conversion",
    "href": "posts/data-analytics/data-analytics.html#column-renaming-and-unit-conversion",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "1.3 Column Renaming and Unit Conversion",
    "text": "1.3 Column Renaming and Unit Conversion\nTo enhance clarity, we renamed columns related to average download and upload speeds and converted the corresponding values from kilobits per second to megabits per second."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#resulting-dataset",
    "href": "posts/data-analytics/data-analytics.html#resulting-dataset",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "1.4 Resulting Dataset",
    "text": "1.4 Resulting Dataset\nThe resulting cleaned dataset, now saved as ‘cleaned_dataset.parquet’, comprises 19,030 entries and 6 columns, each with non-null values. The net_type column is categorized into ‘Mobile’ and ‘Fixed’. The dataset is now ready for further analysis and modeling."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#understanding-initial-distributions",
    "href": "posts/data-analytics/data-analytics.html#understanding-initial-distributions",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "2.1 Understanding Initial Distributions",
    "text": "2.1 Understanding Initial Distributions\nThe initial step involved an examination of the distributions of both fixed and mobile network data. Histograms, box plots (Fig. 2), and summary statistics (Figs. 3 & 4) were employed to gain insights into the central tendencies, dispersions, and skewness of the datasets. Notably, the distributions were observed to be heavily positively skewed, prompting the need for transformation to meet the assumptions of parametric statistical tests.\n\n\n\n\n\n\n\n(a) Box/Hist plots of avg_d_mbps\n\n\n\n\n\n\n\n(b) Box/Hist plots of avg_u_mbps\n\n\n\n\n\n\n\n\n\n(c) Box/Hist plots of avg_lat_ms\n\n\n\n\n\n\n\n(d) Box/Hist plots of avg_lat_down_ms\n\n\n\n\n\n\n\n\n\n(e) Box/Hist plots of avg_lat_up_ms\n\n\n\n\nFigure 2. Box and histogram plots\n\n\n\n\n\nFigure 3. Summary statistics of each network type\n\n\n\n\n\n\n\n\n\n(a) avg_d_mbps column\n\n\n\n\n\n\n\n(b) avg_u_mbps column\n\n\n\n\n\n\n\n\n\n(c) avg_lat_ms column\n\n\n\n\n\n\n\n(d) avg_lat_up_ms column\n\n\n\n\n\n\n\n\n\n(e) avg_lat_down_ms column\n\n\n\n\nFigure 4. Skew and kurtosis values for everything and each network type"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#comparative-analysis",
    "href": "posts/data-analytics/data-analytics.html#comparative-analysis",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "2.2 Comparative Analysis",
    "text": "2.2 Comparative Analysis\nTo assess the disparities between fixed and mobile networks, we conducted thorough comparative analyses. Kernel density plots (Fig. 5) and statistical tests (Fig. 3) were leveraged to highlight variations in central tendencies. These comparisons served as a foundation for subsequent transformations and allowed us to pinpoint differences between the two networks.\n\n\n\n\n\n\n\n(a) avg_d_mbps for fixed network\n\n\n\n\n\n\n\navg_d_mbps for mobile network\n\n\n\n\n\n\n\n\n\navg_u_mbps for fixed network\n\n\n\n\n\n\n\navg_u_mbps for mobile network\n\n\n\n\n\n\n\n\n\n(b) avg_lat_ms for fixed network\n\n\n\n\n\n\n\n(c) avg_lat_ms for mobile network\n\n\n\n\n\n\n\n\n\n(d) avg_lat_down_ms for fixed network\n\n\n\n\n\n\n\n(e) avg_lat_down_ms for mobile network\n\n\n\n\n\n\n\n\n\n(f) avg_lat_up_ms for fixed network\n\n\n\n\n\n\n\n(g) avg_lat_up_ms for mobile network\n\n\n\n\nFigure 5. KDE plots"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#data-transformations",
    "href": "posts/data-analytics/data-analytics.html#data-transformations",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "2.3 Data Transformations",
    "text": "2.3 Data Transformations\nSeveral data transformations were applied, including but not limited to logarithmic, Box-Cox, and Yeo-Johnson transformations. Each transformation was carefully chosen based on its appropriateness for the given context and the nature of the initial distributions. Log transformations, for instance, are effective in addressing exponential growth patterns, while Box-Cox transformations are versatile in handling skewed data. (Lee, S. X. and McLachlan, G. J., 2022) (West, R. M., 2022)"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#comparative-assessment-of-transformations",
    "href": "posts/data-analytics/data-analytics.html#comparative-assessment-of-transformations",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "2.4 Comparative Assessment of Transformations",
    "text": "2.4 Comparative Assessment of Transformations\nA meticulous examination of the transformed datasets ensued, involving comparative analyses with the original data. Visualizations (Figs. 6) and statistical measures, including skewness and kurtosis tests (Fig. 8), were employed to quantify the improvements brought about by each transformation. The Yeo-Johnson transformation consistently demonstrated superior results in terms of bringing the data closer to a normal distribution. (Fig. 7)\n\n\n\n\nCode\n# Define a list of colors for the transformations\ntrans_colors = ['y', 'g', 'c', 'm']\n\n# Define a dictionary that maps transformation names to transformation functions\ntrans_funcs = {\n    'Sqrt Skew': np.sqrt,  # Square root transformation\n    'Log Skew': np.log1p,  # Logarithmic transformation\n    'Box-Cox Skew': stats.boxcox,  # Box-Cox transformation\n    'Yeo-J Skew': stats.yeojohnson  # Yeo-Johnson transformation\n}\n\n# Loop over each transformation in the 'trans_funcs' dictionary\nfor transform in trans_funcs:\n    # Loop over each key in the 'dfs' dictionary\n    for key in dfs:\n        # Get the number of columns in the dataframe corresponding to the current key\n        num_cols = len(dfs[key][0].columns)\n        # Create a subplot with 'num_cols' rows and 2 columns, and set the figure size\n        fig, ax = plt.subplots(num_cols, 2, figsize=(13, 5*num_cols))\n        \n        # Loop over each column in the dataframe corresponding to the current key\n        for i, col in enumerate(dfs[key][0]):\n            # Apply the transformation to the column\n            if transform in ['Box-Cox Skew', 'Yeo-J Skew']:\n                # For Box-Cox and Yeo-Johnson transformations, the function returns two values\n                target, _ = trans_funcs[transform](dfs[key][0][col])\n            else:\n                # For other transformations, the function returns one value\n                target = trans_funcs[transform](dfs[key][0][col])\n\n            # Calculate the skewness of the transformed data\n            transformed_skew = np.round(stats.skew(target),5)\n\n            # Store the skewness in the 'trans_skews' dictionary\n            trans_skews[col][key][transform] = transformed_skew\n\n            # Plot a histogram of the original data\n            sns.histplot(dfs[key][0][col], label='Orginal Skew: {0}'.format(trans_skews[col][key]['Org Skew']), color=\"r\", ax=ax[i][0], kde=True, edgecolor=None)\n            ax[i][0].legend()\n            ax[i][0].set_xlabel('ORGINAL')\n            ax[i][0].set_title(key+' - '+col)\n\n            # Plot a histogram of the transformed data\n            sns.histplot(target, label='Transformed Skew: {0}'.format(transformed_skew), color=trans_colors[trans_list.index(transform)-1], ax=ax[i][1], kde=True, edgecolor=None)\n            ax[i][1].legend()\n            ax[i][1].set_xlabel(transform + ' TRANSFORMED')\n            ax[i][1].set_title(key+' - '+col)\n        \n        # Adjust the padding between and around the subplots\n        fig.tight_layout()\n        # Display the figure\n        plt.show()\n\n\n\n\n\n(a) Original vs Sqrt transformed on fixed network\n\n\n\n\n\n\n\n(b) Original vs Sqrt transformed on mobile network\n\n\n\n\n\n\n\n(c) Original vs Log transformed on fixed network\n\n\n\n\n\n\n\n(d) Original vs Log transformed on mobile network\n\n\n\n\n\n\n\n(e) Original vs Box-Cox transformed on fixed network\n\n\n\n\n\n\n\n(f) Original vs Box-Cox transformed on mobile network\n\n\n\n\n\n\n\n(g) Original vs Yeo-Johnson transformed on fixed network\n\n\n\n\n\n\n\n(h) Original vs Yeo-Johnson transformed on mobile network\n\n\n\nFigure 6. Comparisons of data transformations on distributions\n\n\nSource: Analysis.ipynb\n\n\n\n\n\n\n\n\n\n(a) avg_d_mbps for fixed network\n\n\n\n\n\n\n\n(b) avg_d_mbps for mobile network\n\n\n\n\n\n\n\n\n\n(c) avg_u_mbps for fixed network\n\n\n\n\n\n\n\n(d) avg_u_mbps for mobile network\n\n\n\n\n\n\n\n\n\n(e) avg_lat_ms for fixed network\n\n\n\n\n\n\n\n(f) avg_lat_ms for mobile network\n\n\n\n\n\n\n\n\n\n(g) avg_lat_down_ms for fixed network\n\n\n\n\n\n\n\n(h) avg_lat_down_ms for mobile network\n\n\n\n\n\n\n\n\n\n(i) avg_lat_up_ms for fixed network\n\n\n\n\n\n\n\n(j) avg_lat_up_ms for mobile network\n\n\n\n\nFigure 7. Yeo-Johnson transformed KDE plots\n\n\n\n\n\n\n\n(a) avg_d_mbps column\n\n\n\n\n\n(b) avg_u_mbps column\n\n\n\n\n\n(c) avg_lat_ms column\n\n\n\n\n\n(d) avg_lat_up_ms column\n\n\n\n\n\n(e) avg_lat_down_ms column\n\n\nFigure 8. Comparison of original skew with data transformation skews on both networks"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#correlation-analysis",
    "href": "posts/data-analytics/data-analytics.html#correlation-analysis",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "2.5 Correlation Analysis",
    "text": "2.5 Correlation Analysis\nIn addition to distribution improvements, we investigated the impact of transformations on correlation structures within the data. Scatter plots (Figs. 9 & 10) and correlation matrices (Figs. 11 & 12) were employed to evaluate changes in relationships between variables. This step aimed to ensure that the transformations not only enhanced distributions but also preserved or revealed meaningful associations.\n\n\n\nCode\n# Create a pairplot of the dataframe 'df' sorted by 'net_type'\ng = sns.pairplot(df, hue='net_type', corner=True)\n# Set the title of the plot and adjust its position\ng.fig.suptitle('Everything', y=1.02)\n# Display the plot\nplt.show()\n\n# Loop over each key in the dictionary of dataframes 'dfs'\nfor key in dfs:\n    # Create a pairplot each dataframe in the dictionary\n    g = sns.pairplot(dfs[key][0], corner=True)\n    # Set the title of the plot as the key and adjust its position\n    g.fig.suptitle(key, y=1.02)\n    # Display the plot\n    plt.show()\n\n\n\n\n\n\n\n\n(a) Both networks\n\n\n\n\n\n\n\n\n\n(b) Fixed network\n\n\n\n\n\n\n\n(c) Mobile network\n\n\n\n\nFigure 9. Pairplots on untransformed data\n\n\n\nSource: Analysis.ipynb\n\n\n\n\n\nCode\n# Loop over each key in the 'yeoj_dfs' dictionary\nfor key in yeoj_dfs:\n    # Create a pairplot of the dataframe corresponding to the current key\n    # 'corner=True' means that only the lower triangle of the plot will be shown\n    g = sns.pairplot(yeoj_dfs[key], corner=True)\n    \n    # Set the title of the figure, adding a small space above the title\n    g.fig.suptitle(key+': Yeo-J Transformed', y=1.02)\n    \n    # Display the figure\n    plt.show()\n\n\n\n\n\n\n\n\n(a) Fixed network\n\n\n\n\n\n\n\n(b) Mobile network\n\n\n\n\nFigure 10. Pairplots on Yeo-Johnson transformed data\n\n\n\nSource: Analysis.ipynb\n\n\n\n\n\nCode\n# Create a subplot with 1 row and 3 columns, sharing the y-axis, and set the figure size\nfig, axes_mat = plt.subplots(1, 3, sharey=True, figsize=(10, 5))\n\n# Create a correlation matrix of all the numerical columns\ncorr = df.drop(columns='net_type').corr()\n\n# Visualize the correlation matrix with a heatmap\nsns.heatmap(corr, cmap='RdBu', vmin=-1, vmax=1, annot=True, square=True, ax=axes_mat[0], cbar=False)\n\n# Rotate and align labels on the x-axis\naxes_mat[0].set_xticklabels(axes_mat[0].get_xticklabels(), rotation=45, horizontalalignment='right')\naxes_mat[0].set_title('Correlation Matrix for Everything')\n\n# Loop over each key in the dictionary 'dfs' and \n# the axes in 'axes_mat' starting from the second one\nfor key, ax in zip(dfs, axes_mat[1:]):\n    # Create a correlation matrix of all the numerical columns\n    corr = dfs[key][0].corr()\n\n    # Visualize the correlation matrix with a heatmap\n    sns.heatmap(corr, cmap='RdBu', vmin=-1, vmax=1, annot=True, square=True, ax=ax, cbar=False)\n\n    # Rotate and align labels on the x-axis\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n    ax.set_title('Correlation Matrix for '+key)\n\n# Create a colorbar for the whole figure\nnorm = Normalize(vmin=-1, vmax=1)\n# Create a ScalarMappable object with the 'RdBu' colormap and the normalization\nsm = plt.cm.ScalarMappable(cmap='RdBu', norm=norm)\n# Set the array for the ScalarMappable to an empty array\nsm.set_array([])\n\n# Add an axes to the figure for the colorbar at position [left, top, width, height]\ncbar_ax = fig.add_axes([0.15, 0.95, 0.7, 0.05])  # [left, top, width, height]\n# Add the colorbar to the figure with the ScalarMappable,\n# with horizontal orientation, and in the colorbar axes\nfig.colorbar(sm, orientation='horizontal', cax=cbar_ax)\n\n# Adjust the padding between and around the subplots\nplt.tight_layout()\n# Display the figure\nplt.show()\n\n\n\n\n\nFigure 11. Correlation heatmap matrices for both networks together and separately\n\n\n\n\nSource: Analysis.ipynb\n\n\n\n\n\nCode\n# Create a subplot with 1 row and 2 columns, \n# sharing the y-axis, and set the figure size\nfig, axes_mat = plt.subplots(1, 2, sharey=True, figsize=(10, 7))\n\n# Loop over each key in the 'yeoj_dfs' dictionary and the axes in 'axes_mat'\nfor key, ax in zip(yeoj_dfs, axes_mat):\n    # Create a correlation matrix of all the numerical columns\n    corr = yeoj_dfs[key].corr()\n\n    # Visualize the correlation matrix with a heatmap\n    sns.heatmap(corr, cmap='RdBu', vmin=-1, vmax=1, annot=True, square=True, ax=ax, cbar=False)\n\n    # Rotate and align labels on the x-axis\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n    ax.set_title('Correlation Matrix for '+key)\n\n# Create a colorbar for the whole figure\nnorm = Normalize(vmin=-1, vmax=1)\nsm = plt.cm.ScalarMappable(cmap='RdBu', norm=norm)\nsm.set_array([])\n\n# Add an axes to the figure for the colorbar at position [left, top, width, height]\ncbar_ax = fig.add_axes([0.15, 0.95, 0.7, 0.05])  # [left, top, width, height]\nfig.colorbar(sm, orientation='horizontal', cax=cbar_ax)\n\n# Adjust the padding between and around the subplots\nplt.tight_layout()\n# Display the figure\nplt.show()\n\n\n\n\n\nFigure 12. Correlation heatmap matrices for both networks after Yeo-Johnson transformation\n\n\n\n\nSource: Analysis.ipynb"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#conclusion",
    "href": "posts/data-analytics/data-analytics.html#conclusion",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "2.6 Conclusion",
    "text": "2.6 Conclusion\nThe described EDA and distribution transformations constitute a critical phase in preparing the data for hypothesis testing. The chosen transformations were justified through a systematic exploration of initial distributions, comparative analyses, and a thorough assessment of the impact on correlations. The Yeo-Johnson transformation demonstrated a remarkable ability to normalize skewed data, effectively mitigating the positive skewness observed in the initial distributions. This methodical approach ensures that subsequent analyses are conducted on data that aligns more closely with parametric assumptions, enhancing the robustness and reliability of the findings."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#methodology",
    "href": "posts/data-analytics/data-analytics.html#methodology",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "3.1 Methodology",
    "text": "3.1 Methodology\nWe employed a comprehensive set of statistical tests, considering the positively skewed nature of the original avg_d_mbps dataset.\n\n3.1.1 Levene’s Test: Untransformed Data\nLevene’s test was conducted on the untransformed avg_d_mbps data to assess whether the standard deviation of download speeds differs significantly between fixed and mobile networks.\n\nDecision Justification: Levene’s test is robust for assessing equality of variances, and its non-parametric nature aligns well with the skewed distribution of the original data. (Yuhang Zhou, Yiyang Zhu and Weng Kee Wong, 2023) (Hosken, D. J., Buss, D. L. and Hodgson, D. J., 2018)\n\n\n\n3.1.2 F-Test: Yeo-Johnson Transformed Data\nAn F-test was performed on Yeo-Johnson transformed data to compare variances between fixed and mobile networks after addressing the skewness.\n\nDecision Justification: F-test is suitable for comparing variances, and using the transformed data allows us to make robust comparisons while accounting for skewness.\n\n\n\n3.1.3 T-Tests: Untransformed and Yeo-Johnson Transformed Data\nIndependent sample t-tests were conducted on both untransformed and transformed avg_d_mbps data to assess whether one network has significantly higher average download speeds than the other.\n\nDecision Justification: T-tests are appropriate for comparing means, and conducting them on both datasets ensures a comprehensive evaluation of average download speeds.\n\n\n\n3.1.4 Mann-Whitney U Test: Untransformed Data\nA non-parametric Mann-Whitney U test was performed on the untransformed data to corroborate findings from the t-tests and provide additional robustness.\n\nDecision Justification: The non-parametric nature of the Mann-Whitney U test suits skewed data, offering an alternative perspective on average download speed differences. (Mori, M. et al., 2024) (María Teresa Politi, Juliana Carvalho Ferreira and Cecilia María Patino, 2021)"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#results-and-interpretation",
    "href": "posts/data-analytics/data-analytics.html#results-and-interpretation",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "3.2 Results and Interpretation",
    "text": "3.2 Results and Interpretation\n\n3.2.1 Levene’s Test: Untransformed Data\n\nF statistic: 1046.03, p-value: 0.0\nConclusion: The standard deviation of avg_d_mbps significantly differs between fixed and mobile networks.\n\n\n\n3.2.2 F-Test: Yeo-Johnson Transformed Data\n\nF statistic: 6.07, p-value: 0.0\nConclusion: The F-test on transformed data reinforces the conclusion that the standard deviation of avg_d_mbps varies significantly between networks. Also, it indicates that the fixed network has significantly higher average download speeds and a higher standard deviation than the mobile network.\n\n\n\n3.2.3 T-Tests: Untransformed and Transformed Data\n\n3.2.3.1 Untransformed Data:\n\nt statistic: 40.16, p-value: 0.0\nConclusion: The fixed network has significantly higher average download speeds than the mobile network, and it also exhibits a higher standard deviation.\n\n\n\n3.2.3.2 Yeo-Johnson Transformed Data:\n\nt statistic: 120.57, p-value: 0.0\nConclusion: The transformed data supports the initial conclusion of the fixed network outperforming the mobile network in both average download speeds and standard deviation.\n\n\n\n\n3.2.4 Mann-Whitney U Test: Untransformed Data\n\nU statistic: 63199341.5, p-value: 0.0\nConclusion: The Mann-Whitney U test aligns with t-test results, indicating that the fixed network tends to have significantly higher average download speeds and a higher standard deviation."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#summary",
    "href": "posts/data-analytics/data-analytics.html#summary",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "3.3 Summary",
    "text": "3.3 Summary\nOur multifaceted analysis, incorporating Levene’s test, F-test, t-tests on both original and transformed data, and the Mann-Whitney U test, consistently suggests that the fixed network exhibits significantly higher average download speeds compared to the mobile network. However, it’s important to note that this superior performance is accompanied by a higher standard deviation, indicating a greater degree of variability in download speeds. While the fixed network showcases higher speeds on average, the increased standard deviation suggests a higher level of variability, implying that the consistency of download speeds in the fixed network may be more variable than that of the mobile network. This thorough approach provides a nuanced understanding of the network performance, acknowledging the strengths and potential areas of variability."
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#regression-models-for-average-download-speed",
    "href": "posts/data-analytics/data-analytics.html#regression-models-for-average-download-speed",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "4.1 Regression Models for Average Download Speed",
    "text": "4.1 Regression Models for Average Download Speed\n\n4.1.1 Linear Regression\nUni-variate and Multivariate linear regression models were employed to predict average download speed (avg_d_mbps). The initial models were trained on the original data, and the others were trained on Yeo-Johnson transformed data. The Yeo-Johnson transformed data exhibited a marginal improvement in performance, suggesting that addressing skewness contributed to better predictions (Pan, P., Li, R. and Zhang, Y., 2023). The mean absolute error (MAE), mean squared error (MSE), root mean squared error (RMSE), and R-squared (R2) were used to evaluate model performance. (Figs. 13 & 14) (Subasi, A. et al., 2020)\n\n\n\nFigure 13. Comparison of uni-variate linear regression models trained on original and transformed data\n\n\n\n\n\nFigure 14. Comparison of multivariate linear regression models trained on original and transformed data\n\n\n\n\n4.1.2 Gradient Boosting Regression\nA multivariate Gradient Boosting Regressor was employed as a more sophisticated regression model (Subasi, A. et al., 2020). The model was trained on the original data, and its performance was evaluated using the same metrics (Fig. 15). The Gradient Boosting model outperformed the linear regression models, achieving an R2 of 0.54. Gradient Boosting Regression demonstrated superior predictive power compared to linear regression.\n\n\n\nFigure 15. Gradient boosting results"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#classification-models-for-network-type",
    "href": "posts/data-analytics/data-analytics.html#classification-models-for-network-type",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "4.2 Classification Models for Network Type",
    "text": "4.2 Classification Models for Network Type\n\n4.2.1 Support Vector Machine (SVM)\nAn SVM classification model was trained on original, and Yeo-Johnson transformed data to predict the network type (Fixed or Mobile). Again, the transformed data trained model performed better than the other, achieving an accuracy of approximately 87%. The confusion matrix (Figs. 16 & 17) and classification report provided insights into precision, recall, and F1-score for each class.\n\n\n\nCode\n#Support Vector Machine model\nmodel = svm.SVC()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot()\nplt.show()\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\n\n\n\n\n\nFigure 16. SVM original data confusion matrix\n\n\n\n\n              precision    recall  f1-score   support\n\n       Fixed       0.84      0.80      0.82      1968\n      Mobile       0.79      0.83      0.81      1838\n\n    accuracy                           0.82      3806\n   macro avg       0.82      0.82      0.82      3806\nweighted avg       0.82      0.82      0.82      3806\n\n\n\nSource: ML_Models.ipynb\n\n\n\n\n\nCode\n# Support Vector Machine model with yeo-j transform\n\n# Create a pipeline for the transformed data\npipe_trans = Pipeline([\n    # Apply Yeo-Johnson transformation\n    ('power_transform', PowerTransformer(method='yeo-johnson')), \n    # Create a SVM model\n    ('model', svm.SVC())  \n])\n\n# Train the model\npipe_trans.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipe_trans.predict(X_test)\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=pipe_trans.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipe_trans.classes_)\ndisp.plot()\nplt.show()\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\n\n\n\n\n\nFigure 17. SVM transformed data confusion matrix\n\n\n\n\n              precision    recall  f1-score   support\n\n       Fixed       0.92      0.83      0.87      1968\n      Mobile       0.84      0.92      0.88      1838\n\n    accuracy                           0.87      3806\n   macro avg       0.88      0.88      0.87      3806\nweighted avg       0.88      0.87      0.87      3806\n\n\n\nSource: ML_Models.ipynb\n\n\n4.2.2 Random Forest Classifier\nA Random Forest Classifier was also employed for classification, achieving an accuracy of approximately 87%. (Figs. 18 & 19) A grid search was conducted to fine-tune hyperparameters, resulting in optimal values for max_depth, max_leaf_nodes, min_samples_leaf, and min_samples_split. (Behera, G. and Nain, N., 2022)\n\n\n\nCode\n# Random Forest Classifier Model\n\n# Create a pipeline for the transformed data\npipe_trans = Pipeline([\n    # Apply Random Forest\n    ('model', RandomForestClassifier(n_estimators=300))\n])\n\n# Train the model\npipe_trans.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipe_trans.predict(X_test)\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=pipe_trans.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipe_trans.classes_)\ndisp.plot()\nplt.show()\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\n\n\n\n\n\nFigure 18. Random forest confusion matrix\n\n\n\n\n              precision    recall  f1-score   support\n\n       Fixed       0.90      0.85      0.87      1968\n      Mobile       0.84      0.90      0.87      1838\n\n    accuracy                           0.87      3806\n   macro avg       0.87      0.87      0.87      3806\nweighted avg       0.87      0.87      0.87      3806\n\n\n\nSource: ML_Models.ipynb\n\n\n\n\n# Perform grid search to find more optimal hyperparameters for \n# The Random Forest Classifier Model\n\n# Define the parameter grid\nparam_grid = {\n    'model__max_depth': [None, 5, 10, 15],\n    'model__max_leaf_nodes': [None, 5, 10, 15],\n    'model__min_samples_leaf': [1, 2, 4],\n    'model__min_samples_split': [2, 5, 10]\n}\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(pipe_trans, param_grid, cv=2, scoring='accuracy', verbose=1, n_jobs=-1)\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\nFitting 2 folds for each of 144 candidates, totalling 288 fits\n{'model__max_depth': None, 'model__max_leaf_nodes': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 5}\n0.8707304256437205\n\n\nSource: ML_Models.ipynb\n\n\n\n\n\nCode\n# Try Random Forest classifier with better parameters\n# Create a pipeline for the transformed data\npipe_trans = Pipeline([\n    # Apply Random Forest\n    ('model', RandomForestClassifier(n_estimators=300, max_depth=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=5,  ))  \n])\n\n# Train the model\npipe_trans.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipe_trans.predict(X_test)\n\n# Plot confusion matrix\ncm = confusion_matrix(y_test, y_pred, labels=pipe_trans.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipe_trans.classes_)\ndisp.plot()\nplt.show()\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\n\n\n\n\n\nFigure 19. Random forest confusion matrix trained with more optimal parameters\n\n\n\n\n              precision    recall  f1-score   support\n\n       Fixed       0.90      0.84      0.87      1968\n      Mobile       0.84      0.90      0.87      1838\n\n    accuracy                           0.87      3806\n   macro avg       0.87      0.87      0.87      3806\nweighted avg       0.88      0.87      0.87      3806\n\n\n\nSource: ML_Models.ipynb"
  },
  {
    "objectID": "posts/data-analytics/data-analytics.html#model-comparison-and-analysis",
    "href": "posts/data-analytics/data-analytics.html#model-comparison-and-analysis",
    "title": "Exploring Distributions, Transformations, and Predictive Modeling",
    "section": "4.3 Model Comparison and Analysis",
    "text": "4.3 Model Comparison and Analysis\nThe choice of models depended on the nature of the prediction task. Gradient Boosting Regression demonstrated superior performance in predicting average download speed, while Random Forest Classification excelled in predicting network types. The decision to employ Yeo-Johnson transformation in regression was justified by the slight improvement in predictive accuracy (Pan, P., Li, R. and Zhang, Y., 2023). Both SVM and Random Forest Classifier provided competitive results for network classification, with the latter outperforming SVM."
  },
  {
    "objectID": "posts/star-map-nav/star-map-nav.html",
    "href": "posts/star-map-nav/star-map-nav.html",
    "title": "Star Map Navigation Simulation",
    "section": "",
    "text": "Play Demo\n\n\n\n\n\n\nCamera Keyboard Controls\n\n\n\n\n\n\n\n\nForward\nUp Arrow or W\nDown\nQ\n\n\nBackward\nDown Arrow or S\nLook Left\nZ\n\n\nStrafe Left\nLeft Arrow or A\nLook Right\nX\n\n\nStrafe Right\nRight Arrow or D\nLook Up\nR\n\n\nUp\nE\nLook Down\nF\n\n\nEscape Program\nEsc\nReset Galaxy\nSpace\n\n\n\nInstructions\nUse the camera controls to look around the generated galaxy and use the mouse to select stars by clicking on them. When two stars have been selected the shortest path between them is drawn and the path/distances are displayed to the user in the UI. To deselect stars click on anything that isn’t a star. Press Space to regenerate a new galaxy.\n\n🚀 Play Demo\n\n\n\n\n\nStar Map Navigation Simulation\nProject to implement a star map navigation style simulation using the Unity Games Engine all written in C#. Mathematical programming techniques were used to craft a custom pathfinding djikstra script to chart shortest path from user selected stars. I created custom math methods to perform various calculations involving matrices and incorporated custom lerp libraries and UI scripts from my previous Modes of Motion Unity project.\n\n\n\n\n\n\nSource Code\n\n\n\nView source code on Github repository.\n\n\n\n\nVideo Walkthrough\nVideo"
  },
  {
    "objectID": "posts/neural_net_optimizers/neural_net_optimizers.html#custom-models",
    "href": "posts/neural_net_optimizers/neural_net_optimizers.html#custom-models",
    "title": "Neural Network Optimization Algorithms",
    "section": "1.1 Custom Models",
    "text": "1.1 Custom Models\n\nNeural Network: A basic feedforward neural network implemented from scratch.\nQuantized Neural Network: A neural network with quantized weights for improved efficiency."
  },
  {
    "objectID": "posts/neural_net_optimizers/neural_net_optimizers.html#pytorch-models",
    "href": "posts/neural_net_optimizers/neural_net_optimizers.html#pytorch-models",
    "title": "Neural Network Optimization Algorithms",
    "section": "1.2 PyTorch Models",
    "text": "1.2 PyTorch Models\n\nXOR_Model: A model for the XOR problem using Adam optimizer.\nXOR_GA_Model: A model for the XOR problem using Genetic Algorithm.\nXOR_PSO_Model: A model for the XOR problem using Particle Swarm Optimization.\nMnist_Model: A model for the MNIST dataset using Adam optimizer.\nMnist_GA_Model: A model for the MNIST dataset using Genetic Algorithm.\nMnist_PSO_Model: A model for the MNIST dataset using Particle Swarm Optimization."
  },
  {
    "objectID": "posts/neural_net_optimizers/neural_net_optimizers.html#custom-optimizers",
    "href": "posts/neural_net_optimizers/neural_net_optimizers.html#custom-optimizers",
    "title": "Neural Network Optimization Algorithms",
    "section": "1.3 Custom Optimizers",
    "text": "1.3 Custom Optimizers\n\n1.3.1 Genetic Algorithm\n\nFunctions:\n\n__init__: Initializes the optimizer with a given model, population size, mutation rate, and weight range.\n_get_model_parameters: Retrieves the model parameters as a list of numpy arrays.\n_set_model_parameters: Sets the model parameters from a given list of numpy arrays.\ngenerate_population: Generates an initial population of solutions with random weights.\nfitness_function: Calculates the fitness of a model based on the inverse of the loss function.\nlayer_crossover: Performs uniform crossover between two parent solutions at the layer level.\nneuron_crossover: Performs uniform crossover between two parent solutions at the neuron level.\nmutate: Mutates a given solution by randomly altering its weights.\nstep: Executes one iteration of the genetic algorithm, involving evaluation of the population, selection of the best individuals, and creation of a new population through crossover and mutation.\n\nUsage in Models:\n\nMnist_GA_Model: Trains a PyTorch model for the MNIST dataset.\nXOR_GA_Model: Trains a PyTorch model for the XOR problem.\nQuantized_NN: Trains a custom quantized neural network.\n\n\n\n\n1.3.2 Particle Swarm Optimization\n\nFunctions:\n\n__init__: Initializes the optimizer with a given model, weight range, number of particles, and clipping option.\n_get_model_parameters: Retrieves the model parameters as a list of numpy arrays.\n_set_model_parameters: Sets the model parameters from a given list of numpy arrays.\ngenerate_particles: Generates a swarm of particles with random positions and velocities.\nobjective_function: Calculates the objective value (loss) of a model.\nclip_move: Moves a particle with clipping.\nmove: Moves a particle without clipping.\nstep: Executes one iteration of the particle swarm optimization algorithm, involving updating the velocity and position of each particle based on cognitive and social components.\n\nUsage in Models:\n\nMnist_PSO_Model: Trains a PyTorch model for the MNIST dataset.\nXOR_PSO_Model: Trains a PyTorch model for the XOR problem.\nQuantized_NN: Trains a custom quantized neural network."
  },
  {
    "objectID": "posts/neural_net_optimizers/neural_net_optimizers.html#xor-problem",
    "href": "posts/neural_net_optimizers/neural_net_optimizers.html#xor-problem",
    "title": "Neural Network Optimization Algorithms",
    "section": "2.1 XOR Problem",
    "text": "2.1 XOR Problem"
  },
  {
    "objectID": "posts/neural_net_optimizers/neural_net_optimizers.html#mnist-dataset",
    "href": "posts/neural_net_optimizers/neural_net_optimizers.html#mnist-dataset",
    "title": "Neural Network Optimization Algorithms",
    "section": "2.2 MNIST Dataset",
    "text": "2.2 MNIST Dataset\n\nAdam: \nGenetic Algorithm: \nParticle Swarm Optimization:"
  },
  {
    "objectID": "posts/ai-ethics-IP/ai-ethics-IP.html#legal-landscape",
    "href": "posts/ai-ethics-IP/ai-ethics-IP.html#legal-landscape",
    "title": "Artificial Intelligence and Intellectual Property",
    "section": "2.1 Legal landscape",
    "text": "2.1 Legal landscape\nIt can be argued that there is a growing emphasis on ethical AI development, with organizations and researchers actively working to establish ethical principles and guidelines (Carrillo, M. R., 2020, p.6). While there has been progress made in ethical AI development (Nourbakhsh, I. R., 2021, p.1), potential negative ethical impacts still exist. Focusing solely on ethical principles may not be sufficient to prevent unethical behavior. A distinction between law and ethics must be established to carefully consider the ethical impacts raised in this paper. Laws are created to enforce what society agrees on as ethically acceptable and to hold individuals accountable for those ethical standards. Widespread confusion between legal and ethical principles specifically in AI is used to develop ethical principles to the exclusion of laws, reasoning that they are interchangeable (Carrillo, M. R., 2020, p.6). On the surface, it may seem favorable to focus more on establishing ethical principles, but this may be counterproductive to ensuring ethical behavior.\nThe current legal landscape for AI development may exacerbate the psychological factors underpinning unethical behavior in this space. In recent years, soft law measures have been taken supporting ethical principles over hard regulation. Most notably, soft law, an international instrument containing statements of expected behavior (Yan, M., 2019, p.50), lacks enforceability and accountability in that there is often no development of institutional frameworks or clear assignment of responsibility (Delacroix, S. and Wagner, B., 2021, p.3). Concepts of cost, risk and social acceptability exert considerably influence over whether an individual will engage in unethical behavior. In the absence of regulation, perceptions of cost and risk are minimized and, in some cases, even contribute to increased social acceptance of unethical behavior (Mills, P. and Groening, C., 2021, pp.378-380). Given that it can be difficult to establish IP alone, with the addition of AI bringing with it its current regulatory state may obscure the costs and risks associated with IP theft even further. Lack of regulation in AI development may also fuel social acceptance to abuse AI in regard to IP, as is the case in password sharing with Netflix (Mills, P. and Groening, C., 2021, pp.378).\nCritics might argue that legal frameworks will eventually catch up with AI advancements, closing regulatory gaps and minimizing the risks of unethical behavior. While legal frameworks do have the potential to evolve, the challenge remains in catching up and keeping pace with rapidly advancing AI technologies. Waiting for legal frameworks to catch up might allow a window for unethical practices to proliferate.\nIn addition, psychological factors contribute to whether a person will commit unethical behaviors. Concepts such as anticipated guilt, social acceptability, indirect action, psychological distance and cost/rewards can be used to explain or even predict the likelihood of unethical behavior (Mills, P. and Groening, C., 2021, pp.378-380, Gratch, J. and Fast, N. J., 2022, pp.2-4, Reardon, J. et al., 2019, pp.511,514). These concepts will be elaborated on to further tease out the ethical impact that AI may have on IP."
  },
  {
    "objectID": "posts/ai-ethics-IP/ai-ethics-IP.html#psychological-factors-that-contribute-to-unethical-behavior",
    "href": "posts/ai-ethics-IP/ai-ethics-IP.html#psychological-factors-that-contribute-to-unethical-behavior",
    "title": "Artificial Intelligence and Intellectual Property",
    "section": "2.2 Psychological factors that contribute to unethical behavior",
    "text": "2.2 Psychological factors that contribute to unethical behavior\nReardon, J. et al. (2019, p.522) has demonstrated that a consumer’s ethical perceptions about downloading pirated music had the highest direct impact on whether they participated in such activities. Namely, if a consumer felt strongly that downloading illegal music was ethically wrong then they were far less likely to do it. This beautifully illustrates how having a strong ethical code can have a massive impact in preventing unethical behavior. Ellis, L. M. (2022, pp.9,10,12) explains that idea theft is looked upon more harshly than money theft because the behavior is most often attributed to negative personality characteristics such as self-interest and inauthenticity. As a result, idea thieves suffer severe interpersonal consequences thereby proving idea theft is regarded as a highly unethical behavior (Ellis, L. M., 2022, p.12). There are limitations in linking Ellis, L. M. (2022) studies on idea theft specifically with IP theft as there are nuanced differences between the two. However, as the two bear marked similarities in core concepts such as financial gain and authenticity it is reasonable to conclude both are viewed similarly as highly unethical behaviors by society in general.\nSeeing that IP theft is considered as highly unethical and ethical perceptions have the highest direct impact in preventing unethical behavior one could conclude that pursuing the development of ethical principles to the exclusion of laws may be the most effective approach. However, González-Esteban y Patrici Calvo, E. (2022, p.1) reveals that there exists an increase in professional malpractice including plagiarism, illicit appropriation of ideas, concepts, and results and improper or fraudulent use of information. While IP theft is not specifically listed as increasing by González-Esteban y Patrici Calvo, E. (2022, p.1) the actions that are listed comprise some if not all aspects of IP theft. How is it that actions which contribute to IP theft remain prevalent amidst strong ethical perceptions against them are maintained at large? In this author’s opinion, the answer to that question highlights the potential ethical impact that AI introduces to the subject of IP.\nThe apparent contradiction between what should theoretically prevent IP theft and the reality of the situation can, in part, be explained by the psychological factors we touched on earlier. Anticipated guilt, arising from a consideration of violating one’s own standards, produces powerful motivation to act within the bounds of one’s ethical perceptions (Mills, P. and Groening, C., 2021, pp.378-380, Reardon, J. et al., 2019, pp.511,514). However, when there exists a social consensus that a given behavior is ethically acceptable evidenced by the fact that many individuals engage in the behavior this constitutes social acceptability. Social acceptability exerts a potent moderating effect of anticipated guilt such that as social acceptability of an unethical behavior increases so does engagement in said behavior (Mills, P. and Groening, C., 2021, pp.378-380). There exist strong personal motivations, namely anticipated guilt, to avoid engaging in IP theft. However, the social acceptability of core behaviors involved in IP theft has likely caused many to defer their ethical reservations on the matter.\nAI may strengthen other psychological factors that could influence the social acceptability of IP theft. Gratch, J. and Fast, N. J. (2022, pp.2) explain that indirect action, when one party acts on another through an agent, increases likelihood of unethical behavior. An attractive feature of AI is its ability to process amounts of data that are impossible for a human to absorb and extract valuable insights from that data. Therefore, it is likely that humans will use AI to sift through data that may, without their knowledge, contain IP. The model may then present insights that encroach on someone else’s IP but by the process abstracts the ideas from the owner of the property. This presents a potential situation whereby an individual could inadvertently steal someone else’s IP. Or the psychological distance afforded through the indirect action of using an AI model may reduce the perceived intensity of the negative consequences (Gratch, J. and Fast, N. J. 2022, p.2). In either case, these lower perceptions of the potential cost involved in the action increase the likelihood of IP theft. The increased likelihood of using AI to steal IP could then result in an increase in cases resulting in an even greater social acceptance of this unethical behavior."
  },
  {
    "objectID": "posts/ai-ethics-IP/ai-ethics-IP.html#organizational-psychological-factors",
    "href": "posts/ai-ethics-IP/ai-ethics-IP.html#organizational-psychological-factors",
    "title": "Artificial Intelligence and Intellectual Property",
    "section": "2.3 Organizational psychological factors",
    "text": "2.3 Organizational psychological factors\nThe discussion up until this point has been formulated around the individual use of AI in IP. However, other psychological factors explaining unethical behavior emerge when considering the social and environmental dynamics that accompany life in an organization. The prevalence of corporate scandals indicates a pervasiveness of unethical behavior that routinely transpires within organizations (Griep, Y. et al., 2023, p.1). While AI technology is becoming increasingly more accessible to individuals, the development and usage of these is still more accessible to organizations that possess the resources to engage them. It is important to ponder AI usage within the context of an organizational environment with the objectives that accompany it and how this could further elucidate the ethical impacts it may have on IP.\nPsychological Contract Theory posits that an employee has a mental model of the exchange agreement between him/herself and the organization. This psychological contract is what they believe their organization is obligated to provide them with in return for their contributions. A perceived breach in this contract increases the likelihood that the employee will engage in unethical behavior that harms the company. However, if the contract is perceived as fulfilled then the employee is more likely to engage in unethical pro-organizational behavior as they feel obligate to reciprocate beneficial treatment. Unethical pro-organizational behaviors are unethical acts that employees engage in with a desire to benefit their organization. Even though these acts routinely damage organizations, the employees who perpetrate them have the intention of helping (Griep, Y. et al., 2023, pp.2,12,13).\nOrganizations with the resources to develop and employ AI technologies often deliver competitive wages and benefits to their employees. This increases the likelihood that their employees would be more inclined towards unethical pro-organizational behavior resulting in AI abuse at the expense of IP. Feelings of obligation to reciprocate beneficial treatment may move employees to use AI to steal IP even when the organization does not condone it. Employees that discover previously unknown IP theft in their models may be inclined to lie about and/or cover up the training dataset. These coupled with the speed with which an AI model can process data could facilitate IP theft on a mass scale.\nFuture self-continuity refers to how well a person identifies with themself in the future and if they can image how they might feel. If their future self feels like a stranger to them then they are more likely to act in an unethical way because they do not have a good sense of how they will feel in the future (Hershfield, H. E., Cohen, T. R. and Thompson, L., 2012, p.300). Hershfield, H. E., Cohen, T. R. and Thompson, L. (2012, pp.307-308) were able to shift a person to think more in terms of the future in general rather than future self and by doing so increased a propensity to think more about immediate short-term gains and thereby increase likelihood of engaging in unethical behavior. Many organizations channel employees focus on quarterly earnings and shareholder meetings where the goal is increased profit from last quarter. This consistent and pressurized training of focus on short-term outcomes may successfully shift employee thinking away from a healthier future self-continuity thereby raising likelihood of them engaging in unethical behavior. With a lower future self-continuity, a person may feel more inclined to seize a competitive edge by using AI to steal IP.\nAny one of these two organizational factors on their own increase likelihood of unethical behavior but in combination with each other creates powerful conditions to elicit unethical use of AI. Unethical use of AI, being able to bestow great financial rewards, may be all too tempting for someone who feels an obligation to reciprocate beneficial treatment. Especially when an individual’s focus is being constantly aligned with quarterly earnings, shifting thoughts from ethically fortifying thought patterns of future self-continuity. If we include the current legal landscape of AI use and development, with its lack of regulatory boundaries, along with psychological distance afforded by AI technologies, a deadly cocktail emerges. A potent cocktail that can easily intoxicate individuals towards unethical use and further adding to it a rise in social acceptability. As illustrated, these issues can stack up on each other to add more weight towards tipping the scales in favor of unethical behavior."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#purpose-of-document",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#purpose-of-document",
    "title": "Automation in Systems Administration Project",
    "section": "1.1 Purpose of Document",
    "text": "1.1 Purpose of Document\nThe purpose of this document is to provide an in-depth system analysis of the existing Tandoori Inc network and to propose designs for the upgrade and expansion of said network."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#scope",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#scope",
    "title": "Automation in Systems Administration Project",
    "section": "1.2 Scope",
    "text": "1.2 Scope\nThe scope of the project involves an examination of the current network to ascertain where it can be improved. In addition, design and implementation of the future expansion showing contrasts between current and future networks. The proposed solutions must include detailed configurations and address potential growth of the business in the future."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#existing-network-analysis",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#existing-network-analysis",
    "title": "Automation in Systems Administration Project",
    "section": "2.1 Existing Network Analysis",
    "text": "2.1 Existing Network Analysis\nThe existing network is located in Vancouver head office and utilizes 7 on-premises servers comprised of two (2) domain controllers, two (2) file servers, one (1) backup server, one (1) web server, and one (1) firewall/router. In addition, there is one (1) load balanced database server cluster and one (1) load balanced mail server cluster both deployed from AWS cloud infrastructure. There are seventy-five (75) work stations & laptops, seventy-two (72) BYOD phones all with antivirus software installed. All clients that do not require static IP addresses acquire there addresses from a DHCP server on the primary domain controller(Subnet and IP address table are shown in section 2.2 IP Address Table). The exception to this are the BYOD phones, they acquire their local IP addresses from the wireless access point. A local DNS is also installed on the primary domain server. The file servers are configured with a folder hierarchy consisting of a “CompanyData” folder containing four (4) departments, “Administration”, “Marketing”, “Sales”, and “IT”, all departments housing data in public and private folders. ( See Figure 1 )\n\n\n\n\n\n(a) IP Address Table\n\n\n\n\n\n(b) Network Diagram\n\n\nFigure 1. Existing Network"
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#future-proposed-network",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#future-proposed-network",
    "title": "Automation in Systems Administration Project",
    "section": "3.1 Future / Proposed Network",
    "text": "3.1 Future / Proposed Network\nA new location in Kelowna will be setup as a branch office for Interior B.C. Seventeen (17) employees from the Vancouver branch will move to the Kelowna branch and fifteen (15) new empoyees will be hired locally. The new branch will require the addition of another subnet and servers to service the new location. The Kelowna branch will be connected to the head office at all times with all resources being available in both locations.\nIn addition, a new RnD department with it’s own subnet and Read-Only Domain Controller will be setup in the Vancouver branch. All workstations in the RnD Department network segment will acquire IP addresses from the DHCP server hosted on the primary domain controller in the Vancouver head office network. To minimize the WAN traffic the Kelowna branch will house the same amount and kind of servers that the Vancouver branch does allowing for the clients in the Kelowna network to access services in their own location. The Kelowna network will be comprised of two (2) domain controllers, two (2) file servers, one (1) backup server, one (1) web server, and one (1) firewall/router. The domain controllers, file servers, and web server will replicate from the Vancouver branch assuring that all company data and services are in sync across the organization. Active Directory will be updated to include the addition of the new branch and subsequent servers and client machines. A DFS will be setup for the entire business and the folder hierarchy for the previous file servers will be transferred to DFS based file services on the two (2) file servers located at the Vancouver branch. DFS file service must be accessible to RnD network but RnD is restricted to only one folder."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#fault-tolerance-and-availability",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#fault-tolerance-and-availability",
    "title": "Automation in Systems Administration Project",
    "section": "3.2 Fault Tolerance and Availability",
    "text": "3.2 Fault Tolerance and Availability\nActive Directory will be configured to regularly backup all crucial server data to the backup servers hosted in the network that they are located. Failover clusters for both the Vancouver and Kelowna network will be deployed in the company’s already existing AWS infrastructure. These clusters will employ load balancing technology and elastic provisioning should the on premises servers go offline. These strategies will allow for the new system to have excellent fault tolerance and high availability ensuring that the expanded network maintain a high quality of performance."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#security",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#security",
    "title": "Automation in Systems Administration Project",
    "section": "3.3 Security",
    "text": "3.3 Security\nEdge routers on both Vancouver and Kelowna networks with act as a firewall and router in one. Local firewalls on all servers and clients will also be configured for maximum protection. In addition, an Inter-Site VPN will be employed so that the Vancouver and Kelowna networks will have a secured/tunneled connection leaving it impossible for man-in-the-middle attacks to occur. Each branch will access the AWS infrastructure through a secure VPN connection as well. The VPN will be configured to utilize the highest level of security available without the need for certificates."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#administration",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#administration",
    "title": "Automation in Systems Administration Project",
    "section": "3.4 Administration",
    "text": "3.4 Administration\nFor administrative purposes, IT will be given access to all servers and administrative capabilities within the company by means of group policies employed in Active Directory. In addition to being able to perform their duties internally they will also be given administrative access externally through secure VPN connections. This will allow for quick responses to outages or emergencies that occur outside of office hours. Upstream and downstream WSUS servers will be used to ensure smooth updates and upgrades to the various servers and clients in the system. The upstream WSUS server will be hosted on the Primary Domain Controller (DC1.Van.Tandoori.Local) in the Vancouver network and will be managed by IT staff there. The downstream WSUS server will be provisioned on the Primary Domain Controller (DC1.Kel.Tandoori.Local) in the Kelowna network which will inherit update approvals from the upstream server housed in Vancouver. This configuration will save bandwidth on internet connections and allow for updates to be administered entirely by IT staff at head offices in Vancouver. Active Directory Group Policies will be used to implement software deployment and access for the various servers, client machines, and users in the network. Careful considerations will be given to what softwares and accesses will be afforded to the machines and users in the network to ensure that the principle of least privilege is adheard to. Therefore, only necessary software and necessary access will be provisioned using Group Policies in the Active Directory environment."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#branchcache",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#branchcache",
    "title": "Automation in Systems Administration Project",
    "section": "3.5 BranchCache",
    "text": "3.5 BranchCache\nTo optimize WAN bandwidth BranchCache technology will be utilized for company data. The two (2) file servers located at the Kelowna branch will be configured as hosted cache servers and will cache data housed in the file servers located at the Vancouver branch. This will allow for quick retrieval of company data for clients located in the Kelowna network and will ensure continuity of data across the entirety of the organization’s infrastructure."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#future-network-ip-address-tables",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#future-network-ip-address-tables",
    "title": "Automation in Systems Administration Project",
    "section": "3.6 Future Network – IP Address Tables",
    "text": "3.6 Future Network – IP Address Tables\nSubnets were designed in a way to allow for large amounts of room for growth. Both Vancouver and Kelowna networks were allocated a 16 bit address space to give plenty of room to add new departments or even resubnet the existing subnets to allow for more room if departments outgrow their allocated space. Even the subnets for each department were created very large (given a 24 bit address space) to allow for growth within the department. The RnD department network was given a 24 bit address space that well exceeds its need at this current time. ( See Figure 2 )\n\n\n\n\n\n(a) Vancouver Network Address Table\n\n\n\n\n\n(b) RnD Network Address Table\n\n\n\n\n\n(c) Kelowna Network Address Table\n\n\nFigure 2. Future Network – IP Address Tables"
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#future-network-diagrams",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#future-network-diagrams",
    "title": "Automation in Systems Administration Project",
    "section": "3.7 Future Network Diagrams",
    "text": "3.7 Future Network Diagrams\n\n\n\n\n\n(a) Full Network Diagram\n\n\n\n\n\n(b) Vancouver Network Diagram\n\n\n\n\n\n(c) Kelowna Network Diagram\n\n\nFigure 3. Future Network Diagrams"
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#directaccess",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#directaccess",
    "title": "Automation in Systems Administration Project",
    "section": "4.1 DirectAccess",
    "text": "4.1 DirectAccess\nEven though the internet will eventually move towards IPv6 only, by and large IPv4 is still predominantly used today. This presents a problem because when your company makes the switch you will have communication issues with external clients using IPv4 that try to access your services. DirectAccess uses IPv6 transition protocols to facilitate communications with IPv4 connections. In my professional opinion I believe you should invest your time and energy into converting soon and using DirectAccess to address the communications issues that come with it. However, I think you should implement the pending network expansion first and allow the company time to adjust to the new infrastructure before making the switch. After 2 – 5 years operating with the new branch network and RnD department network your IT team should have all of the major issues worked out, giving you the right circumstances to move ahead with transitioning to IPv6. Please consult the references to DirectAccess technology posted in the Appendix of this document to become more familiar with the process and to help you make an informed decision."
  },
  {
    "objectID": "posts/automation-sys-admin/automation-sys-admin.html#references",
    "href": "posts/automation-sys-admin/automation-sys-admin.html#references",
    "title": "Automation in Systems Administration Project",
    "section": "References",
    "text": "References\n\nDirectAccess\nhttps://docs.microsoft.com/en-us/windows-server/remote/remote-access/directaccess/single-server-wizard/da-basic-plan-s1-infrastructure\nhttps://directaccess.richardhicks.com/2014/10/28/directaccess-ipv6-transition-protocols-explained/\n\n\nWSUS Servers\nhttps://docs.microsoft.com/de-de/security-updates/windowsupdateservices/18127375\nhttps://askme4tech.com/how-install-configure-wsus-downstream-server-windows-server-2016\n\n\nBranchCache\nhttps://docs.microsoft.com/en-us/windows-server/networking/branchcache/branchcache\n\n\nPrinciple of Least Priviledge\nhttps://www.cisa.gov/uscert/bsi/articles/knowledge/principles/least-privilege"
  },
  {
    "objectID": "posts/opt_algs1/opt-algs1.html#command-line-interface-cli-and-gradio-integration",
    "href": "posts/opt_algs1/opt-algs1.html#command-line-interface-cli-and-gradio-integration",
    "title": "Traveling Salesmen Problem Python 🐍 vs Mojo 🔥",
    "section": "1.1 Command-Line Interface (CLI) and Gradio Integration",
    "text": "1.1 Command-Line Interface (CLI) and Gradio Integration\nTakes input parameters from the command line or Gradio interface, offering flexibility in execution. Gradio integration allows running the algorithm with user-selected parameters through a subprocess.\n\n\n\n\n\n\nNote\n\n\n\nView Gradio interface source code."
  },
  {
    "objectID": "posts/opt_algs1/opt-algs1.html#genetic-algorithm-implementation",
    "href": "posts/opt_algs1/opt-algs1.html#genetic-algorithm-implementation",
    "title": "Traveling Salesmen Problem Python 🐍 vs Mojo 🔥",
    "section": "1.2 Genetic Algorithm Implementation",
    "text": "1.2 Genetic Algorithm Implementation\nUtilizes functions for initializing a population, selecting parents, creating children, and performing crossovers. Implements a tournament selection method for parent selection. Supports both time-based and generation-based execution, depending on user preference. Incorporates a mutation rate, allowing for controlled exploration. Mutation involves swapping two cities randomly in the city list. Implements Order Crossover (OX1) for creating children with randomly generated crossover points."
  },
  {
    "objectID": "posts/opt_algs1/opt-algs1.html#numpy",
    "href": "posts/opt_algs1/opt-algs1.html#numpy",
    "title": "Traveling Salesmen Problem Python 🐍 vs Mojo 🔥",
    "section": "2.1 NumPy",
    "text": "2.1 NumPy\nNumPy is used for parent crossover operations in the breed3 function(Listing 1), such as creating arrays, boolean indexing, filtering, and concatenation. These operations are often more concise and efficient when performed using NumPy functions resulting in faster execution.\n\nListing 1. Python Breed Function with NumPy\n\ngenalg.py\n\n# Fastest breed function using Numpy\ndef breed3(parent1, parent2, point1, point2):\n    \n1    mid_chunk = np.array(parent1[point1:point2])\n\n2    remaining_elements = np.array(parent2)[~np.isin(parent2, mid_chunk).all(axis=1)]\n    \n3    child = np.concatenate((remaining_elements[:point1], mid_chunk, remaining_elements[point1:]))\n    \n    return mutate(child)\n\n\n1\n\nCreating Mid Chunk: parent1[point1:point2] selects a chunk of elements from parent1 between indices point1 (inclusive) and point2 (exclusive). np.array(...) converts this chunk into a NumPy array and assigns it to mid_chunk.\n\n2\n\nBoolean Indexing and Filtering: np.isin(parent2, mid_chunk) creates a boolean array of the same shape as parent2, where True indicates the elements of parent2 that are also present in mid_chunk. all(axis=1) checks along each row if all elements are True. ~ negates the boolean array, so True becomes False and vice versa. This boolean array is used to filter out rows from parent2 where all elements match those in mid_chunk. The selected rows (remaining elements) are converted into a NumPy array and assigned to remaining_elements.\n\n3\n\nConcatenation: np.concatenate(...) concatenates the remaining elements before point1, the mid_chunk, and the remaining elements after point1. The result is a NumPy array representing the child."
  },
  {
    "objectID": "posts/opt_algs1/opt-algs1.html#parallel-processing",
    "href": "posts/opt_algs1/opt-algs1.html#parallel-processing",
    "title": "Traveling Salesmen Problem Python 🐍 vs Mojo 🔥",
    "section": "2.2 Parallel Processing",
    "text": "2.2 Parallel Processing\nAllows the user to choose between multithreading(Listing 2) or multiprocessing(Listing 3) for creating children concurrently. Uses ThreadPoolExecutor and ProcessPoolExecutor for efficient parallel execution.\n\nListing 2. Python Multithread Function\n\ngenalg.py\n\n#Fastest version of create children using multithreading\ndef multithread_create_children(parents):\n    children = []\n    # sets num of threads based on cpu count\n    num_threads = os.cpu_count()\n    #split parent population amound the threads\n    parent_chunks = np.array_split(parents, num_threads)\n    \n    #start all threads\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        futures = {executor.submit(normal_create_children, chunk): chunk for chunk in parent_chunks}\n        for future in as_completed(futures):\n            data = future.result()\n            children.append(data)\n\n    return np.concatenate(children)\n\n\nListing 3. Python Multiprocess function\n\ngenalg.py\n\n# Mulitprocessing version of create children\ndef multiprocess_create_children(parents):\n    children = []\n    # sets number of processes based on cpu count \n    num_processes = os.cpu_count()\n\n    # splits up the parent population into chunks according to the num of processes\n    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n        parent_chunks = np.array_split(parents, num_processes)\n        futures = {executor.submit(normal_create_children, chunk): chunk for chunk in parent_chunks}\n        for future in as_completed(futures):\n            data = future.result()\n            children.append(data)\n\n    return np.concatenate(children)"
  },
  {
    "objectID": "posts/opt_algs1/opt-algs1.html#mojo-language",
    "href": "posts/opt_algs1/opt-algs1.html#mojo-language",
    "title": "Traveling Salesmen Problem Python 🐍 vs Mojo 🔥",
    "section": "Mojo Language",
    "text": "Mojo Language\nView Mojo documentation."
  },
  {
    "objectID": "posts/opt_algs1/opt-algs1.html#deployment",
    "href": "posts/opt_algs1/opt-algs1.html#deployment",
    "title": "Traveling Salesmen Problem Python 🐍 vs Mojo 🔥",
    "section": "Deployment",
    "text": "Deployment\n\nSource Code\nView the source code for the app deployment on the HuggingFace repository.\n\n\nGradio App\nView Gradio app deployment hosted on HuggingFace.\n\n\nDeployment Resources\nHugging Face - AI community & place to host ML deployments\nhttps://huggingface.co/\nGradio - Open-source Python library used to build machine learning/data science demos & web applications\nhttps://www.gradio.app/"
  },
  {
    "objectID": "posts/modes-of-motion/modes-of-motion.html",
    "href": "posts/modes-of-motion/modes-of-motion.html",
    "title": "Modes of Motion Programming Mathematics",
    "section": "",
    "text": "Play Demo\n\n\n\n\n\n\nCamera Keyboard Controls\n\n\n\n\n\n\n\n\nForward\nUp Arrow or W\nDown\nQ\n\n\nBackward\nDown Arrow or S\nLook Left\nZ\n\n\nStrafe Left\nLeft Arrow or A\nLook Right\nX\n\n\nStrafe Right\nRight Arrow or D\nLook Up\nR\n\n\nUp\nE\nLook Down\nF\n\n\nEscape Program\nEsc\n\n\n\n\n\nCamera Canvas\nWhen user clicks anywhere in the scene camera canvas is enabled allowing the user to do various camera movements based on the options they choose. In this mode the user can create a limited amount of cubes and spheres using the “Create Cube” & “Create Sphere” buttons.\nObject Canvas\nWhen an object is clicked on it is selected then the object canvas is enabled allowing the user to move the object using the various options they choose.\n\n🚀 Play Demo\n\n\n\n\n\nModes of Motion\nProject to demonstrate various methods of movement within the Unity Games Engine, making use of my own linear interpolation (lerp) libraries and scripts using Unity physics all written in C#. This scene employs mathematical techniques like projectile formulas and equations that have been developed over the course of the module.\n\n\n\n\n\n\nSource Code\n\n\n\nView source code on Github repository.\n\n\n\n\nVideo Walkthrough\nVideo\n\n\nReferences\n\nUnity Documentation\nhttps://docs.unity3d.com/ScriptReference/index.html\n\n\nSpawning Objects\n\n\n\nCollision Detection\n\n\n\nSpecific Coding Problems Queried on:\nhttps://stackoverflow.com/questions/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brandon Toews: Artificial Intelligence Research AssistantMy projects portfolio.",
    "section": "",
    "text": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation\n\n\n\n\n\n\n\nLarge Language Models\n\n\nBenchmarking\n\n\nAI / Machine Learning\n\n\nCybersecurity\n\n\n\n\nA framework for evaluating Large Language Models in domain-specific contexts, addressing benchmark contamination, evaluation misalignment, and dynamic knowledge requirements.\n\n\n\n\n\n\nJun 16, 2025\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nBig Data Analytics: DDoS Real-Time Classification\n\n\n\n\n\n\n\nBig Data Analytics\n\n\nCybersecurity\n\n\nAI / Machine Learning\n\n\n\n\nA comprehensive analysis of real-time DDoS attack classification using clustering, classification, and streaming analytics with Apache Spark.\n\n\n\n\n\n\nJan 17, 2025\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nSensor Fusion Using EKF for Navigation and Path Planning\n\n\n\n\n\n\n\nSensor Fusion\n\n\nExtended Kalman Filter\n\n\nRobotics Navigation\n\n\nPath Planning\n\n\n\n\nImplementation of sensor fusion using Extended Kalman Filter for autonomous robot navigation and path planning in warehouse environments using Isaac Sim.\n\n\n\n\n\n\nJan 13, 2025\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nNeural Network Optimization Algorithms\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nOptimizing the weights and baises of neural networks using genetic algorithm and particle swarm optimization methods. Includes custom PyTorch optimizers.\n\n\n\n\n\n\nMay 23, 2024\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Forecasting xLSTM vs Markov Chain\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nCompare univariate and multivariate xLSTM models against Markov Chain model to predict future values based on historical temporal sequence weather data.\n\n\n\n\n\n\nMay 13, 2024\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nAutonomous Drone Delivery Optimization\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nAdvanced algorithmic solutions aimed at optimizing autonomous drone delivery systems. Utilizing a combination of A-star and a genetic algorithm, this study not only addresses complex logistical challenges but also exemplifies the practical application and effectiveness of artificial intelligence in solving real-world problems.\n\n\n\n\n\n\nMay 13, 2024\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nExploring Distributions, Transformations, and Predictive Modeling\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nAn exploratory data analysis on an Ookla speedtest dataset with applications of descriptive analytics, data transformations, hypothesis testing, and predictive modelling.\n\n\n\n\n\n\nJan 9, 2024\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nTraveling Salesmen Problem Python 🐍 vs Mojo 🔥\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nAn application that uses a genetic algorithm for solving the Traveling Salesman Problem written in Python and Mojo programmming languages for comparison.\n\n\n\n\n\n\nDec 25, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nArtificial Intelligence and Intellectual Property\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nThis paper critically examines the ethical implications of AI and intellectual property, focusing on the intersection of legal frameworks and psychological factors.\n\n\n\n\n\n\nDec 21, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning FastAI Model\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nProject to train a deep learning model to correctly classify an image of a wasp or a bee using transfer learning with the fastai library.\n\n\n\n\n\n\nJun 9, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nTkinter Machine Learning Prototype Software\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nProject to design and develop a prototype software with UI to import, transform and save data to and from a cloud database. Software built to apply machine learning algorithms to explore interesting findings from dataset and use to propose a business case for the design.\n\n\n\n\n\n\nJun 5, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nProgramming Design Patterns\n\n\n\n\n\n\n\nProgramming\n\n\n\n\nMechanics sandbox game to demonstrate programming design patterns using Unreal Engine. Implemented command, state machine, object pool & prototype design patterns using C++ and Unreal’s Blueprints Visual Scripting system.\n\n\n\n\n\n\nMay 19, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nStar Map Navigation Simulation\n\n\n\n\n\n\n\nProgramming\n\n\n\n\nUnity Games Engine project to implement a star map navigation style simulation with custom pathfinding script.\n\n\n\n\n\n\nMay 13, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nRetro C++ Game LaserMan\n\n\n\n\n\n\n\nProgramming\n\n\n\n\nRetro game written in C++ using a beginner’s game-development framework called Playbuffer\n\n\n\n\n\n\nJan 13, 2023\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression, Classification, Clustering\n\n\n\n\n\n\n\nAI / Machine Learning\n\n\n\n\nProject to compare different AI algorithms and an exploration of how to improve their accuracy.\n\n\n\n\n\n\nDec 22, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nHotel Booking System Mock Design\n\n\n\n\n\n\n\nSoftware / System Design\n\n\n\n\nProject to design a hotel booking system based on a mock system design case scenario.\n\n\n\n\n\n\nDec 9, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nModes of Motion Programming Mathematics\n\n\n\n\n\n\n\nProgramming\n\n\n\n\nUnity Games Engine project to demonstrate various methods of movement, making use of custom linear interpolation (lerp) libraries and scripts using Unity physics.\n\n\n\n\n\n\nNov 30, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nAutomation in Systems Administration Project\n\n\n\n\n\n\n\nNetworking\n\n\n\n\nProject to plan and design an expansion to a fictitious Microsoft Windows Server based network, develop a prototype network, and to produce documentation along with it.\n\n\n\n\n\n\nSep 19, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nLinux Administration Project\n\n\n\n\n\n\n\nNetworking\n\n\n\n\nProject to design and setup a mock prototype network using headless CentOS 7.0 servers and to produce documentation along with it.\n\n\n\n\n\n\nJun 17, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nOpenStack Virtualization\n\n\n\n\n\n\n\nNetworking\n\n\n\n\nCreated an OpenStack environment by manually installing and configuring services.\n\n\n\n\n\n\nMar 17, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nMicrosoft Exchange Server Network\n\n\n\n\n\n\n\nNetworking\n\n\n\n\nDeployed an Exchange server environment using Microsoft Exchange Server 2016.\n\n\n\n\n\n\nMar 3, 2022\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nWireless Technology Networking\n\n\n\n\n\n\n\nNetworking\n\n\n\n\nDesigned a mock solution for a P2P network with wifi coverage. Produced documentation to propose hardware, along with instructions for installation and configuration.\n\n\n\n\n\n\nDec 19, 2021\n\n\nBrandon Toews\n\n\n\n\n\n\n  \n\n\n\n\nNetworking with Cisco Technologies\n\n\n\n\n\n\n\nNetworking\n\n\n\n\nDesigned and constructed a fictional network using GNS3 simulation software and VMs running on VMware.\n\n\n\n\n\n\nDec 14, 2021\n\n\nBrandon Toews\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/big-data-analytics/big-data-analytics.html#clustering",
    "href": "posts/big-data-analytics/big-data-analytics.html#clustering",
    "title": "Big Data Analytics: DDoS Real-Time Classification",
    "section": "2.1 Clustering",
    "text": "2.1 Clustering\nFor the clustering implementation two models were trained using a standard scaler and principal component analysis reducing the dimensions using 3 components.\n\n2.1.1 DBSCAN Clustering Analysis Results and Discussion\nThe application of Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to the network traffic dataset yielded results that suggest complex, overlapping patterns in the underlying data structure. The algorithm’s performance can be evaluated through several established clustering quality metrics, each providing insight into different aspects of the clustering solution. The clustering solution achieved a Silhouette score of 0.0252, indicating poor cluster definition and separation. This notably low score suggests that the boundaries between clusters are significantly blurred, with many instances potentially lying in regions of overlapping density. The Mutual Information score of 0.2373 further supports this interpretation, indicating weak correspondence between the clustering solution and the true class labels.\nThe identical Homogeneity and Completeness scores (both 0.4436) present an interesting symmetry in the clustering solution (Figure 15). This equivalence suggests that the algorithm’s performance in maintaining class purity within clusters matches its ability to avoid splitting classes across multiple clusters. While these scores indicate moderate performance, they reveal that the DBSCAN clustering solution captures only partially the underlying class structure of the data.\n\n\n\nFigure 16. DBSCAN Evaluation Metrics\n\n\nThe metrics collectively suggest that the density-based approach may not optimally capture the natural groupings in this particular network traffic dataset. This could be attributed to several factors: 1) The low Silhouette score may indicate that the traffic patterns form continuous density gradients rather than distinct, separated clusters. 2) The results may reflect suboptimal choices for DBSCAN’s core parameters (ε and minPts). 3) The network traffic data may exhibit patterns that are not well-suited to density-based clustering approaches. These findings suggest several avenues for further investigation such as an exploration of alternative distance metrics that might better capture the similarity relationships in network traffic patterns. A systematic evaluation of DBSCAN parameter combinations to optimize clustering performance may provide significant benefit. Overall, the results contribute to our understanding of the challenges in applying density-based clustering to network traffic analysis while highlighting the importance of careful algorithm selection and parameter tuning in such applications. This analysis reveals that while DBSCAN provides interesting insights into the data structure, its relatively poor performance metrics suggest that alternative clustering approaches might be more appropriate for this dataset (Alam, A., Malhotra, A. and Schizas, I.D., 2025, p. 8, para. 1).\n\n\n2.1.2 KMeans Clustering Analysis Results and Discussion\nThe implementation of K-means clustering on the network traffic dataset yielded seven distinct clusters with varying distributions and quality metrics. The algorithm’s performance can be evaluated through multiple complementary measures of cluster quality and distribution characteristics. The clustering solution achieved a Silhouette score of 0.752, indicating strong cluster cohesion and separation (Figure 16). This relatively high score suggests that objects within each cluster are well-matched to their assigned clusters and poorly matched to neighboring clusters. The Mutual Information score of 0.574 demonstrates moderate agreement between the clustering solution and the underlying class structure. However, the disparity between Homogeneity (0.295) and Completeness (0.680) scores warrants further investigation. The lower homogeneity score indicates that clusters contain a mixture of classes, while the higher completeness score suggests that instances of the same class tend to be assigned to the same cluster (Tabejamaat, M. et al., 2025, p. 4, para. 1).\n\n\n\nFigure 17. KMeans Evaluation Metrics\n\n\nThe clustering solution produced a notably heterogeneous distribution of instances across the seven clusters. The dataset sample used for training has an equal 1000 instance distribution for all classes in an attempt to prevent bias in the model. However, the majority cluster (Cluster 1) contains 5,129 instances, representing the dominant pattern in the dataset (Figure 17). Two secondary clusters of substantial size emerged: Cluster 6 with 1,089 instances and Cluster 5 with 605 instances. The remaining clusters exhibited markedly smaller populations: Cluster 0 (120 instances), Cluster 2 (23 instances), Cluster 3 (33 instances), and notably, Cluster 4 with a single instance. This distribution pattern suggests the presence of both major behavioral patterns and potential anomalous cases in network traffic.\n\n\n\nFigure 18. KMeans Class distributions\n\n\nThe stark variation in cluster sizes, combined with the high Silhouette score, suggests that while the clustering solution effectively identifies distinct patterns in the data, it may be capturing hierarchical structure that warrants further investigation. The presence of a singleton cluster (Cluster 4) merits particular attention as it may represent a significant anomaly. Further research directions might include hierarchical analysis of the dominant cluster to identify potential sub-patterns. An ensemble method may be able to produce more stable results given the big data nature of this dataset (“RSPCA: Random Sample Partition and Clustering Approximation for ensemble learning of big data”, 2025, p. 5, para. 4). A detailed examination of the singleton cluster to validate its significance in the context of network traffic patterns would no doubt yield better results. These findings contribute to our understanding of the underlying structure in network traffic patterns while highlighting areas for continued investigation and refinement of the clustering approach.\n\n\n\nFigure 19. Forward IAT Total vs Backward IAT Total - Ground Truth\n\n\n\n\n\nFigure 20. Forward IAT Total vs Backward IAT Total - Clustering Prediction\n\n\n\n\n\nFigure 21. Flow Bytes/s vs Flow Packets/s - Ground Truth\n\n\n\n\n\nFigure 22. Flow Bytes/s vs Flow Packets/s - Clustering Prediction"
  },
  {
    "objectID": "posts/big-data-analytics/big-data-analytics.html#classification",
    "href": "posts/big-data-analytics/big-data-analytics.html#classification",
    "title": "Big Data Analytics: DDoS Real-Time Classification",
    "section": "2.2 Classification",
    "text": "2.2 Classification\nThe implemented Random Forest classifier demonstrates robust performance in distinguishing between various DDoS attack vectors and normal network traffic. Operating on a comprehensive dataset of 398,755 records with seven balanced classes (56,965 entries per class), the model achieves an accuracy of 88% following standardization of features (Figure 19). The model’s classification efficacy can be evaluated through multiple metrics. The baseline accuracy for a seven-class balanced classification problem would be approximately 14.3%, making the achieved 88% accuracy a substantial improvement over random classification. This performance is particularly noteworthy given the complexity of network traffic classification and the subtle differences that do exist between different attack signatures as demonstrated in Section 2.1. The diagonal elements of the matrix indicate strong classification performance for most attack types, with several classes showing particularly high true positive rates (Figure 19). However, there are notable interactions between certain attack classes, suggesting some attack signatures share similar characteristics that may complicate classification. Of particular interest are the off-diagonal elements, which represent misclassifications between different attack types. Some classes demonstrate near-perfect separation, as evidenced by rows containing predominantly zero values for non-diagonal elements. This suggests that certain attack types have highly distinctive signatures that the model can readily identify.\n\n\n\nFigure 23. Random Forest Classifier Evaluation Metrics\n\n\n\n\n\nFigure 24. Random Forest Classifier Spark Jobs"
  },
  {
    "objectID": "posts/big-data-analytics/big-data-analytics.html#streaming-analysis",
    "href": "posts/big-data-analytics/big-data-analytics.html#streaming-analysis",
    "title": "Big Data Analytics: DDoS Real-Time Classification",
    "section": "2.3 Streaming Analysis",
    "text": "2.3 Streaming Analysis\nThis study presents a distributed stream processing system for real-time Distributed Denial of Service (DDoS) attack detection utilizing Apache Spark’s streaming capabilities and a pre-trained Random Forest classifier. The system demonstrates effective real-time classification of network traffic into multiple categories, including benign traffic and various attack vectors. The implemented architecture comprises several key components operating in a distributed environment simulated with Docker Hadoop and Spark containers (Hoozemans, J. et al., 2021, p. 38, para. 10). The core system utilizes HDFS for model persistence and data storage, while the processing pipeline incorporates comprehensive feature engineering and standardization. The system processes 65 distinct network traffic features, providing a comprehensive view of network behavior patterns. The processing pipeline operates on a 5-second interval, enabling near-real-time detection capabilities (Figure 21).\n\n\n\nFigure 25. Streaming Random Forest Classifier\n\n\n\n\n\n\n\n\nStreaming Analysis Video\n\n\n\n\n\n\n\nVideo\nStreaming Analysis Video\n\n\n\n\n\nReal-time classification results demonstrate the system’s ability to identify multiple attack vectors simultaneously. The system successfully maintains continuous processing capabilities while handling diverse traffic patterns, showing particular effectiveness in distinguishing between normal traffic and attack vectors. The sequential nature of similar traffic types in the results suggests the system’s ability to detect sustained attack patterns. The implementation demonstrates several notable performance characteristics such as low-latency classification with 5-second processing intervals. It successfully handles multiple attack vectors simultaneously with a robust preprocessing pipeline maintaining data quality in real-time."
  },
  {
    "objectID": "posts/big-data-analytics/big-data-analytics.html#dimensionality-reduction",
    "href": "posts/big-data-analytics/big-data-analytics.html#dimensionality-reduction",
    "title": "Big Data Analytics: DDoS Real-Time Classification",
    "section": "3.1 Dimensionality Reduction",
    "text": "3.1 Dimensionality Reduction\nThis analysis examines the effect of dimensionality reduction through Principal Component Analysis (PCA) on a Random Forest classifier’s performance in detecting DDoS attacks. The study compares the baseline model against a PCA-transformed variant, revealing significant insights into the trade-offs between dimensionality reduction and classification accuracy. The experiment utilized two configurations, (1) a baseline model with standardized features and (2) PCA-transformed model with k=2 components (experimentation realized that 2 components achieved the best performance possible with PCA) (Figure 22). Both configurations maintained consistent preprocessing steps and Random Forest parameters, isolating PCA’s impact on model performance.\n\n\n\nFigure 26. Random Forest Classifier with PCA vs without\n\n\nBaseline Model Performance: - Classification accuracy: 88% - Computational efficiency: 0.142 seconds CPU time - Clear class separation evident in confusion matrix\nPCA-Transformed Model Performance: - Classification accuracy: 79% - Computational efficiency: 0.140 seconds CPU time - Explained variance ratios: - Component 1: 23.50% - Component 2: 13.41% - Cumulative explained variance: 36.91% - Increased misclassification rates observed in confusion matrix - Algorithm-determined optimal components: 1\nThe implementation of PCA resulted in a substantial degradation of classification performance, with accuracy declining by 9 percentage points. The minimal improvement in computational efficiency (approximately 0.002 seconds) does not justify the significant loss in classification accuracy. The low cumulative explained variance (36.91%) suggests that the two principal components fail to capture the complex patterns necessary for effective DDoS attack classification."
  },
  {
    "objectID": "posts/big-data-analytics/big-data-analytics.html#critical-analysis",
    "href": "posts/big-data-analytics/big-data-analytics.html#critical-analysis",
    "title": "Big Data Analytics: DDoS Real-Time Classification",
    "section": "3.2 Critical Analysis",
    "text": "3.2 Critical Analysis\n\n3.2.1 Clustering\nWhile KMeans performed better than DBSCAN for clustering the results are in no way useful for real-world application. This is despite utilizing a standard scaler and PCA, which did improve performance significantly, but still not enough for any real use case. However, the local development environment afforded the opportunity to incorporate the RAPIDS library collection. “The RAPIDS data science framework is a collection of libraries for running end-to-end data science pipelines completely on the GPU” (RAPIDS.ai, 2025) (Hoozemans, J. et al., 2021, p. 45, para. 2). The RAPIDS framework provided instrumental support for the cleaning, exploration, and combining of the dataset required for this implementation. This allowed for expedited Cluster model training and exploratory development of Classification models which were ported to Spark later.\n\n\n3.2.2 Classification\nIn the context of network security applications, this level of classification accuracy presents a promising foundation for practical deployment. However, further analysis of precision and recall metrics for each attack type would be beneficial, particularly given the critical nature of false positives and false negatives in security contexts. While the current performance is promising, several avenues for potential improvement exist, including a deeper investigation of feature importance to identify key traffic characteristics. Analysis of misclassification patterns to understand potential signature overlaps would prove beneficial. A more robust evaluation of model performance under various network conditions and attack intensities is a necessity before a deployment in a security detection environment.\nExperimentations with PCA-based dimensionality reduction indicate it may not be suitable for this particular DDoS detection task. The complex nature of network traffic patterns appears to require the fuller feature set for accurate classification. Future work might explore alternative dimensionality reduction techniques (Reddy, G.T. et al., 2020, p. 54780, para. 2) (Amouzgar, M. et al., 2022). Feature selection based on Random Forest importance scores may yield more promising results. A more comprehensive investigation of optimal component numbers could produce a better balance between accuracy and efficiency. This study contributes to the understanding of dimensionality reduction impacts on DDoS detection systems and highlights the importance of maintaining feature richness in network security applications.\n\n\n3.2.3 Streaming Analysis\nThe usefulness of real-time network traffic classification would be further aided by an integration of temporal analysis through timestamp-based pattern detection. A neural network architecture such as LSTM (Mohammadi, M. et al., 2018, p. 2930, para. 6) or xLSTM could elucidate more complex relationships borne out of timing (Ren-Hung Hwang et al, 2019, p. 6, para. 2) (Alharthi, M. and Mahmood, A., 2024, p. 1488, para. 3). This would provide an entirely fresh but equally powerful perspective in terms of real-time threat detection. Further, an implementation of sliding window analytics for trend analysis providing real-time performance metric monitoring could prove useful in anticipating full blown attacks by capturing initial probing type behavior. Development of an automated alert system for critical attack patterns along with an automated response, carefully measured, would doubly add value. This implementation demonstrates a proof of concept showcasing the feasibility of a real-time DDoS attack detection in a distributed environment."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#robot-kinematics",
    "href": "posts/sensor-fusion/sensor-fusion.html#robot-kinematics",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "1.1 Robot Kinematics",
    "text": "1.1 Robot Kinematics\nChosen Robot: Nova Carter\nKinematic Model: Differential Drive\nJustification:\n\nSimplicity and ease of implementation\nSuitable for indoor environments and small-scale robotic platforms\nPrecise control over direction and speed with minimal mechanical complexity\nWidely applicable for wheeled robots performing navigation tasks in structured environments\n\nThe kinematic model for differential drive robots, like the Nova Carter robot in this implementation, is expressed in Equation 1 and is responsible for explaining the movement of the robot. It describes how the robot’s position (x, y) and orientation θ evolve with respect to linear and angular velocities.\n\\[\n\\begin{aligned}\n\\dot{x} &= v \\cdot \\cos(\\theta) \\\\\n\\dot{y} &= v \\cdot \\sin(\\theta) \\\\\n\\dot{\\theta} &= \\omega\n\\end{aligned}\n\\tag{1}\\]\nWhere:\n\nv is the linear velocity (from the Twist message)\nω is the angular velocity\nθ is the robot’s orientation\n\nThe wheel velocities \\(v_r\\) and \\(v_l\\) are derived using:\n\\[\n\\begin{aligned}\nv_r &= \\frac{2v + \\omega L}{2R} \\\\\nv_l &= \\frac{2v - \\omega L}{2R}\n\\end{aligned}\n\\tag{2}\\]\nWhere:\n\nL = 0.413m (wheel distance)\nR = 0.14m (wheel radius)\n\nThe simulated environment, discussed in greater detail in the next section (Section 1.2), is run in Isaac Sim. Isaac Sim’s differential drive model uses OmniGraph nodes, specifically the Differential Controller Node, to compute wheel velocities from linear and angular velocity inputs using the wheel velocity equations (Equation 2) and kinematic parameters specified in the node (Figure 1). The differential drive path controller MATLAB script calculates linear and angular velocities from target positions in the planned path and sends these to the controller in Isaac Sim through a ROS2 bridge.\n\n\n\nFigure 1. Nova Carter Differential Drive Parameters in Isaac Sim\n\n\nKinematic Parameters:\n\nwheelDistance: 0.413 meters (distance between the wheels)\nwheelRadius: 0.14 meters (radius of the wheels)\nmaxLinearSpeed: 2.0 m/s\nmaxAngularSpeed: 3.0 rad/s\nmaxAcceleration / maxDeceleration: 2.0 m/s² (limits for smooth motion)\ndt: 0.01667 (time step, approximately 60 Hz simulation rate)\n\n\n\n\nFigure 2. Robot Dimensions"
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#sec-environment-setup",
    "href": "posts/sensor-fusion/sensor-fusion.html#sec-environment-setup",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "1.2 Environment Setup",
    "text": "1.2 Environment Setup\nThe environment is simulated within Isaac Sim as it provides a realistic scene which is highly useful for practically applying theoretical robotics concepts. Isaac Sim is a high-fidelity physics engine that is capable of simulating commercial and industrial robots, robotic movement, and sensor data making it a prime candidate for this simulation (Nvidia Omniverse IsaacSim, 2025). The scene is a warehouse with forklifts, pallets, and shelving for the robot to navigate through. These static obstacles have been placed to create a maze-like environment that the robot can plan paths and navigate through. On running the main MATLAB script, a robot object is created and given a goal position to navigate to. The user can observe the robot moving towards the specified goal through the viewport in Isaac Sim (Figure 3), front camera viewport in RViz2 (Figure 4), and the robot’s occupancy map displaying the planned path and the robot’s pose along the way (Figure 5).\n\n\n\nFigure 3. 3rd Person View in Isaac Sim Viewport\n\n\n\n\n\nFigure 4. 1st Person View in RViz2 Viewport\n\n\n\n\n\nFigure 5. Ground Truth Occupancy Map (Left), Robot’s LiDAR updated Occupancy Map (Right)"
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#sensor-selection",
    "href": "posts/sensor-fusion/sensor-fusion.html#sensor-selection",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "2.1 Sensor Selection",
    "text": "2.1 Sensor Selection\nChassis Odometry Sensor: Collects position (x, y) and quaternion orientation (x, y, z, w) based on wheel encoder data, used in combination with IMU for more accurate localization. GPS signals are significantly weakened or blocked by warehouse roofs, walls, and metal racking (Ghasemieh, A. and Kashef, R., 2024, p. 2, para. 3). On the other hand, odometry works independently of external signals, making it ideal for indoor environments. GPS typically has 5-10 meter accuracy, while odometry typically achieves centimeter-level precision for short distances (Ghasemieh, A. and Kashef, R., 2024, p. 3, para. 1). This precision is crucial for warehouse operations like navigating narrow aisles. Odometry provides immediate feedback while GPS can have significant delay in position updates. This allows for quick response time, which is essential for obstacle avoidance and precise positioning. The system compensates for odometry’s main weakness, drift, by fusing it with IMU data through the Extended Kalman Filter and using LiDAR for correction against discovered landmarks.\nChassis IMU Sensor: Is a combined sensor package that usually includes accelerometers (measuring linear acceleration) and gyroscopes (measuring angular velocity/rotation). These measurements are particularly valuable for detecting rapid changes in the robot’s motion and helping to correct for wheel slippage that might occur on the warehouse floor. The acceleration data is integrated over time to estimate velocity, while the angular velocity provides direct measurement of the robot’s rotation rate. The advantage of using an IMU in a warehouse setting is that it combines these sensors into a single calibrated package. Because of this, accelerometer and gyroscope readings are perfectly synchronized with a single timestamp for all measurements making it easier for sensor fusion. Sensors are pre-calibrated relative to each other ensuring alignment between acceleration and rotation axes. Finally, there is considerable space and cost efficiency in using a single component for mounting and wiring.\nFront 2D LiDAR Sensor: Measures distances in a single horizontal plane with accompanying angle of measurement. 2D LiDAR generates significantly less data to process than a 3D LiDAR sensor which allows for more responsive occupancy grid updates. Real-time performance is crucial for warehouse navigations that include dynamic obstacles. This data undergoes several processing steps, including range validation and conversion to Cartesian coordinates, before being used to update the robot’s internal map and assist in obstacle avoidance."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#data-acquisition",
    "href": "posts/sensor-fusion/sensor-fusion.html#data-acquisition",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "2.2 Data Acquisition",
    "text": "2.2 Data Acquisition\nThe heart of our data collection system lies in its ROS2-based architecture. ROS2 provides a robust framework for handling real-time sensor data through its publisher-subscriber model (Carreira, R. et al., 2024, pp. 11-12). Each sensor communicates through dedicated ROS2 topics controlled with a precise timing mechanism that ensures consistent data sampling across all sensors. Operating at 60Hz (with a time step of 0.01667 seconds), this timer-based approach synchronizes data collection and processing, crucial for maintaining accurate state estimation. This high update rate allows the robot to respond quickly to changes in its environment while maintaining smooth motion control. To facilitate system analysis and improvement, we’ve implemented a comprehensive data logging system. This system records not only the raw sensor readings but also the processed state estimates and ground truth data for comparison. The logged data proves invaluable for post-mission analysis, allowing us to evaluate system performance and identify areas for improvement."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#fusion-techniques",
    "href": "posts/sensor-fusion/sensor-fusion.html#fusion-techniques",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "3.1 Fusion Techniques",
    "text": "3.1 Fusion Techniques\nThe Standard Kalman Filter assumes the state transition and observation models are linear. The kinematic model employed for the differential drive is inherently non-linear due to the trigonometric functions cos(θ) and sin(θ) (Equation 1). The state transition model (Equation 4) describes transitions from states by combining the state definition (Equation 3) with the non-linear kinematic model (Equation 1). EKF solves this problem by linearizing the non-linear system at each time step by computing Jacobian matrices (Jiang, L. and Wu, L., 2024, p. 2, para. 1). The state transition Jacobian matrix (Equation 5) is calculated by considering the state transition model (Equation 4) as f(x) and finding the partial derivatives for each function of f(x) with respect to each term in the state definition model (Equation 3).\n\\[\n\\mathbf{x} = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\\\ v \\\\ \\omega \\end{bmatrix}\n\\tag{3}\\]\n\\[\n\\begin{aligned}\nx_{k+1} &= x_k + v \\cdot \\Delta t \\cdot \\cos(\\theta) \\\\\ny_{k+1} &= y_k + v \\cdot \\Delta t \\cdot \\sin(\\theta) \\\\\n\\theta_{k+1} &= \\theta_k + \\omega \\cdot \\Delta t \\\\\nv_{k+1} &= v_k \\\\\n\\omega_{k+1} &= \\omega_k\n\\end{aligned}\n\\tag{4}\\]\n\\[\n\\mathbf{F} = \\begin{bmatrix}\n1 & 0 & -v \\cdot \\Delta t \\cdot \\sin(\\theta) & \\Delta t \\cdot \\cos(\\theta) & 0 \\\\\n0 & 1 & v \\cdot \\Delta t \\cdot \\cos(\\theta) & \\Delta t \\cdot \\sin(\\theta) & 0 \\\\\n0 & 0 & 1 & 0 & \\Delta t \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\tag{5}\\]\nThe observation models are linear, and we add measurement noise to explain the uncertainty in the readings from the sensors for things like drift or wheel slippage (Equation 6 & Equation 7). Since we need to perform matrix calculations because of having to linearize the state transition model with a Jacobian matrix, we also need to use Jacobians of the observation models to apply to the state transition Jacobian (Equation 5). The observation model Jacobians (Equation 8 & Equation 9) are calculated by considering the observation models (Equation 6 & Equation 7) as h(x) and finding the partial derivatives for each function of h(x) with respect to each term in the state definition model (Equation 3).\n\\[\n\\mathbf{z}_{odom} = \\mathbf{h}_{odom}(\\mathbf{x}) = \\begin{bmatrix} x \\\\ y \\\\ \\theta \\\\ v \\\\ \\omega \\end{bmatrix}\n\\tag{6}\\]\n\\[\n\\mathbf{z}_{imu} = \\mathbf{h}_{imu}(\\mathbf{x}) = \\begin{bmatrix} v \\\\ \\omega \\end{bmatrix}\n\\tag{7}\\]\n\\[\n\\mathbf{H}_{odom} = \\frac{\\partial \\mathbf{h}_{odom}}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\tag{8}\\]\n\\[\n\\mathbf{H}_{imu} = \\frac{\\partial \\mathbf{h}_{imu}}{\\partial \\mathbf{x}} = \\begin{bmatrix}\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\tag{9}\\]\nFinally, we perform the prediction step of the state transition where we apply the process noise to the state transition Jacobian to model the uncertainty of the prediction and update the predicted state covariance (Equation 10). Then we incorporate the measurement. Then we update the observed measurements using the observation Jacobians, measurement noise, and predicted state covariance to create the Kalman gain filter. This filter is then used to weigh the contribution of the predicted state transition against the measurement observations in the final outcome of the state prediction. The filter bases these contributions of the predicted state transition and measurement observations on the respective covariances, or the uncertainty, of each (Equation 11).*\n\\[\n\\begin{aligned}\n\\mathbf{x}_{k+1|k} &= \\mathbf{F} \\cdot \\mathbf{x}_k \\\\\n\\mathbf{P}_{k+1|k} &= \\mathbf{F} \\cdot \\mathbf{P}_k \\cdot \\mathbf{F}^T + \\mathbf{Q}\n\\end{aligned}\n\\tag{10}\\]\n\\[\n\\begin{aligned}\n\\mathbf{y} &= \\mathbf{z} - \\mathbf{H} \\cdot \\mathbf{x}_{k+1|k} \\\\\n\\mathbf{S} &= \\mathbf{H} \\cdot \\mathbf{P}_{k+1|k} \\cdot \\mathbf{H}^T + \\mathbf{R} \\\\\n\\mathbf{K} &= \\mathbf{P}_{k+1|k} \\cdot \\mathbf{H}^T \\cdot \\mathbf{S}^{-1} \\\\\n\\mathbf{x}_{k+1} &= \\mathbf{x}_{k+1|k} + \\mathbf{K} \\cdot \\mathbf{y} \\\\\n\\mathbf{P}_{k+1} &= (\\mathbf{I} - \\mathbf{K} \\cdot \\mathbf{H}) \\cdot \\mathbf{P}_{k+1|k}\n\\end{aligned}\n\\tag{11}\\]\n*This measurement update step is done twice in the code implementation, once for the odometry measurements and again for IMU measurements."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#application-in-navigation",
    "href": "posts/sensor-fusion/sensor-fusion.html#application-in-navigation",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "3.2 Application in Navigation",
    "text": "3.2 Application in Navigation\nUltimately, EKF is an appropriate method for sensor fusion in this case because it can effectively combine IMU and Odometry readings to improve state estimation and subsequent navigation accuracy. Not only this, EKF can simultaneously handle the non-linearity introduced by the state transition model to produce a reasonable level of accuracy for the task of navigating in a warehouse environment. Artificial Neural Networks (ANNs) are another viable option for sensor fusion as they too can combine sensor readings and manage non-linear associations quite well. However, ANNs are typically more computationally expensive than EKF and require an undetermined amount of training and fine tuning to produce sufficient accuracy resulting in longer development durations. As a result, EKF has been used in this simulation to produce a lightweight and accurate solution suitable for the hardware resource and real-time computational constraints of the project.\nAs seen depicted in Figure 6, EKF sensor fusion provides the robot with accurate enough pose estimations, in relation to ground truth, to allow for the robot to make properly informed decisions about velocities and headings. Whereas, when the robot does not have a consistent and accurate idea of where it is and how it is oriented in space the velocity adjusts become more erratic and susceptible to bias over time. The redundancy provided by data collected from multiple sensors measuring similar types of orientation information provides a navigation framework that is resilient against sensor noise and/or failure. Individual sensor noise fluctuations are not correlated with one another, permitting a more well-rounded estimation of state at any given time. We will delve into how well this EKF implementation performs by examining in comparison with alternate methods of pose estimation.\n\n\n\nFigure 6. Sensor Fusion Accuracy Compared to A* Planned Path & Ground Truth"
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#performance-metrics",
    "href": "posts/sensor-fusion/sensor-fusion.html#performance-metrics",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "4.1 Performance Metrics",
    "text": "4.1 Performance Metrics\nWe provide three (3) different modes of state estimation to compare the effects of measurement noise on navigation and to illustrate just how effective EKF is at addressing this. First, a dead reckoning implementation that uses only control inputs and the state transition function to track orientation and navigate. Second, odometry-only state estimation that doesn’t compensate for the simulated sensor noise of the odometer. Lastly, the EKF solution that fuses odometry and IMU readings, models sensor and process uncertainty, and that elegantly combines state transition predictions with the observed measurements. We will look at:\n\nHow accurate the modes are at estimating their state in comparison with the ground truth state.\nHow accurate the modes are at following the planned path, calculated from an A* algorithm taken from the MATLAB robotics toolbox. This will be a comparison of the ground truth and the planned path as the ground truth is a representation of what the robot actually did.\nNavigation and path error probability distributions, bar charts depicting mean and maximum path deviation, path length, completion time."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#analysis-of-results",
    "href": "posts/sensor-fusion/sensor-fusion.html#analysis-of-results",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "4.2 Analysis of Results",
    "text": "4.2 Analysis of Results\nAs can be seen in Figure 7 and Figure 8, dead reckoning is highly inaccurate in pose estimation and is subsequently unable to follow the planned path in any practical capacity. It seems that without sensor data to interoceptively inform the robot of its orientation it is overcome by sensor noise, bias, and process noise.\n\n\n\nFigure 7. Dead Reckoning - Accuracy Compared to A* Planned Path, Ground Truth, & State Estimation\n\n\n\n\n\nFigure 8. Dead Reckoning - Pose Estimate Error Probability Distributions Compared to Ground Truth\n\n\nOdometry-only is a marked improvement over dead reckoning on all accounts and performs reasonably well considering we are only using one (1) sensor (Figure 9 & Figure 10). The robot’s state estimation is erratic but not so much so that it cannot follow the planned path to reach the goal pose. This mode of estimation may be sufficient in certain environments and with sensors that do not produce high levels of noise. However, as sensor noise increases the performance of this mode of estimation decreases, causing many overcompensations to velocities and heading (Figure 11). These frequent and powerful corrections would likely cause more wear and tear on the vehicle.\n\n\n\nFigure 9. Odometry Only - Accuracy Compared to A* Planned Path, Ground Truth, & State Estimation\n\n\n\n\n\nFigure 10. Odometry Only - Pose Estimate Error Probability Distributions Compared to Ground Truth\n\n\nThe EKF solution performs the best of all three (3), showcasing a robust ability to handle measurement noise in real-time. This mode of state estimation boasts the least amount of error in all metrics across the board with the exception of completion time. EKF produces tighter error probability distributions with equal or lower magnitudes than the other modes of state estimation. The EKF ground truth has a maximum deviation from the planned path almost half that of, and even a few points away from being equal to the mean deviation of the odometry-only solution (Table 2). As we manually increase measurement noise, we can see how the EKF still maintains stability and accuracy while odometry-only suffers from even greater fluctuations in state estimation (Figure 16 & Figure 17). Even though EKFs tend to be computationally expensive they do produce remarkable results in contexts like these (Vitali, R.V., McGinnis, R.S. and Perkins, N.C., 2021, pp. 1-2).\n\n\n\nFigure 11. EKF - Accuracy Compared to A* Planned Path, Ground Truth, & State Estimation\n\n\n\n\n\nFigure 12. EKF - Pose Estimate Error Probability Distributions Compared to Ground Truth\n\n\n\n\n\nFigure 13. All Ground Truths Compared to A* Planned Path\n\n\n\n\n\nFigure 14. All Estimation Methods’ Estimations, Ground Truths, A* Planned Path\n\n\n\n\n\nFigure 15. All Estimation Methods’ Ground Truths Against A* Planned Path\n\n\n\n\nTable 1. All Estimation Methods’ Ground Truths Against A* Planned Path Error\n\n\n\n\n\n\n\n\n\nMethod\nMean Position Error (m)\nRMS Position Error (m)\nMean Heading Error (rad)\nRMS Heading Error (rad)\n\n\n\n\nFusion\n0.043518\n0.049445\n0.014593\n0.017925\n\n\nOdometry Only\n0.066875\n0.076514\n0.015236\n0.019748\n\n\nDead Reckoning\n0.72064\n0.79609\n0.29554\n0.35184\n\n\n\n\n\n\nTable 2. All Estimation Methods’ Ground Truths Against A* Planned Path Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nMean Deviation (m)\nMax Deviation (m)\nPath Length (m)\nCompletion Time (s)\nPath Smoothness (avg heading change)\nTime within 0.1m\nTime within 0.2m\nTime within 0.5m\n\n\n\n\nFusion\n0.12613\n0.18047\n2.1128\n0.96\n0.030647 rad\n23.5294%\n100%\n100%\n\n\nOdometry Only\n0.1577\n0.35182\n2.172\n0.84\n0.021918 rad\n42.8571%\n66.6667%\n100%\n\n\nDead Reckoning\n0.25327\n0.39551\n1.0653\n0.68\n0.042377 rad\n6.0606%\n45.4545%\n100%\n\n\n\n\n\n\n\nFigure 16. Odometry Only - Higher Noise Accuracy Compared to A* Planned Path & Ground Truth\n\n\n\n\n\nFigure 17. EKF - Higher Noise Accuracy Compared to A* Planned Path & Ground Truth"
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#critical-evaluation",
    "href": "posts/sensor-fusion/sensor-fusion.html#critical-evaluation",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "5.1 Critical Evaluation",
    "text": "5.1 Critical Evaluation\nWhile EKF clearly outperforms the other methods of state estimation explored in this paper, it is not without its challenges. Tuning the process noise manually is a time-consuming trial-and-error endeavor even with data analytics to inform the adjustments. Meanwhile, deploying and developing Artificial Neural Networks (ANN) are becoming easier with support from libraries like PyTorch and are more than capable of handling the non-linear nature of state estimations and transitions (Ghorbani, S. and Janabi-Sharifi, F., 2022, p.1, para. 5). The current EKF implementation could take more full advantage of the IMU and odometry orientation data by processing quaternion information directly using quaternion specific equations rather than converting the quaternion to θ at the time of collection (Vitali, R.V., McGinnis, R.S. and Perkins, N.C., 2021, pp. 2-6).\nAs stated previously, the robot uses a 2D LiDAR sensor to update its internal occupancy map which is crucial for navigating a dynamic environment like a warehouse. The horizontal orientation of the obstacle detection provided by this sensor presents issues in this context. Even though the sensor is effectively mapping most of the obstacles in the environment it is missing a key feature that results in collisions. If forklift forks are raised off the ground, then the LiDAR doesn’t detect these long protrusions which routinely causes collisions with the robot. Compounding the situation further, the robot’s occupancy map cannot be inflated as each time the LiDAR updates the map it would have to call the inflate method causing already inflated objects to continue to grow until enveloping the entire map. The current A* algorithm being used doesn’t have a minimum turning radius property, therefore, the planner charts paths that bring the robot too close to objects causing collisions, even with obstacles it registers in its occupancy map. Using a high-fidelity simulation engine like Isaac Sim pushes these kinds of real-world challenges to the fore providing the opportunity to address these in a simulated environment before deployment. In conjunction with this, usage of the ROS2 bridge for the environment setup facilitates easy portability to real-world applications (Carreira, R. et al., 2024, pp. 5-8).\n\n\n\nFigure 18. 2D LiDAR Doesn’t Update Forklift Forks onto Robot’s Occupancy Map\n\n\n\n\n\nFigure 19. Robot Collides with Forklift Forks"
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#improvements",
    "href": "posts/sensor-fusion/sensor-fusion.html#improvements",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "5.2 Improvements",
    "text": "5.2 Improvements\nA more comprehensive obstacle detection system is required before this implementation could be used for real world applications. The Nova Carter robot does have a 3D LiDAR sensor which would be more than capable of addressing the forklift forks issue. Or a YOLOv8n vision model, custom trained to detect forklifts, could be used in combination with the front camera to identify forklifts and mark the area around the forks on the robot’s occupancy map (Jiang, L. and Wu, L., 2024, pp. 10-13). Also, the plannerHybridAStar tool in the MATLAB robotics toolbox would effectively address the obstacle avoidance issues that are currently experienced."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html#segway-rmplite-220-specifications",
    "href": "posts/sensor-fusion/sensor-fusion.html#segway-rmplite-220-specifications",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "5.3 Segway RMPLite 220 Specifications",
    "text": "5.3 Segway RMPLite 220 Specifications\nPhysical Parameters:\n\nDimensions: L×W×H (mm): 730×499×280\nStructure Parameters: Axil base×Wheelbase×Ground clearance (mm): 513.5×413×69\nTire size: 11 inches (280mm) Hub motor\nWeight: 33KG\nStandard Load: 50kg\nObstacle Avoidance: 5cm/8°/Speed bump\nSuspension Travel: 4mm (Rear)\nDrive: FWD, Differential Steering\nIP Rating: IP65\n\nFunctions:\n\nMax Speed: 3m/s\nMax Steering Speed: 3rad/s\nMinimum Turning Radius: 0m\nBraking: With No Load: 3m/s 0.9m, Braking Acc: 0.5g\nControl: Remote control, host computer control\nBraking Method: Electronic Brake\n\nConnectivity:\n\nInterface: UART, CAN\nSupporting system, API: C/C++, ROS\nFeedback Data: Encoder, Hall, IMU\n3D Model: Gazebo, Rviz model\n\nBattery:\n\nRange: 1152wh- Max Load:3m/s, Range:80Km\nCapacity: 48V 20Ah/24Ah\nCharging: Manual charging/Swappable battery/Provided with automatic charging interface\nHost computer power: 48V 400W\n\nUI:\n\nButtons: Emergency stop button, Push to move button, Power button\nStatus Indication: Power on/off status indicator, Platform base status indicator, controls indication, Battery level indicator, Charging status indicator\n\nExtension Kit Components:\n\nLight Strip\nUltrasonic Sensor\nBumper\nInfrared Sensor\nSensor Mounting Rod\n\nThe Segway RMP (Robotics Mobility Platform) comes with a general and integrated robot chassis solution for enterprises and/or third-party developers. The hardware modular design and software SDK interface has the ability to support secondary development and/or customized services. RMP Lite provides a highly adaptable platform covering scenarios such as indoor/outdoor delivery, patrolling, service, cleaning(disinfecting), AGV (warehouse), and special application robots. Segway RMP lite comes with a large-capacity battery to sustain 10 hours of non-stop service. The software is compatible with ROS and Isaac operating systems providing Gazebo and Rviz simulation models with tutorial cases."
  },
  {
    "objectID": "posts/sensor-fusion/sensor-fusion.html",
    "href": "posts/sensor-fusion/sensor-fusion.html",
    "title": "Sensor Fusion Using EKF for Navigation and Path Planning",
    "section": "",
    "text": "::: {.callout-tip collapse=“true”} ## Video Walkthrough\nVideo Walkthrough :::"
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#motivations",
    "href": "posts/domain-bench/domain-bench.html#motivations",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.1 Motivations",
    "text": "1.1 Motivations\nLarge Language Models (LLMs) have revolutionized natural language processing (NLP), demonstrating remarkable capabilities across diverse domains (McIntosh et al. 2024). Their increasing integration into critical sectors such as healthcare, law, and cybersecurity necessitates rigorous evaluation of their performance (McIntosh et al. 2024). While general benchmarks have contributed to progress, these benchmarks often fail to capture specific requirements crucial to specialized applications, indicating the need for domain-specific evaluation frameworks."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#problem-statement",
    "href": "posts/domain-bench/domain-bench.html#problem-statement",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.2 Problem Statement",
    "text": "1.2 Problem Statement\nCurrent benchmarking practices face several critical challenges. Benchmark data contamination (BDC), resulting from overlaps between training datasets and test benchmarks, significantly inflates performance metrics and misleads stakeholders about the true capabilities of a model (Golchin and Surdeanu 2024). Many evaluation methods also use probability-based scoring in multiple-choice tests, which inadequately represents the true reasoning and generation capabilities of a model (Lyu, Wu, and Aji 2024). Traditional static benchmarks do not adequately evaluate LLM performance in evolving knowledge domains, as demonstrated by substantial performance declines observed when models are tested against dynamically evolving benchmarks (Xia, Deng, and Zhang 2024)."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#research-questions",
    "href": "posts/domain-bench/domain-bench.html#research-questions",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.3 Research Questions",
    "text": "1.3 Research Questions\nThis research seeks to answer the following questions:\n\nHow can a domain-adaptable LLM benchmarking framework mitigate common evaluation challenges such as data contamination and evaluation method misalignment?\nTo what extent does retrieval-based augmentation improve LLM performance in specialized tasks?\nWhat multidimensional metrics best capture nuanced performance aspects of LLMs?\nHow can benchmarks adapt to evolving domain knowledge while maintaining consistent performance evaluation?"
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#research-objectives",
    "href": "posts/domain-bench/domain-bench.html#research-objectives",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.4 Research Objectives",
    "text": "1.4 Research Objectives\nAligned with these research questions, the objectives of this study are:\n\nDevelop a modular and dynamic benchmarking framework that is adaptable to specialized domains.\nImplement and evaluate multiple LLM architectures and augmentation strategies, including retrieval augmentation and knowledge graph integration.\nEstablish a comprehensive multimetric evaluation approach.\nCreate mechanisms for continual benchmark evolution to reflect updates in domain knowledge."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#scope",
    "href": "posts/domain-bench/domain-bench.html#scope",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.5 Scope",
    "text": "1.5 Scope\nThis research focuses specifically on domain-specific benchmarking frameworks for LLMs demonstrated through cybersecurity compliance advisory tasks, addressing a rapidly evolving and complex knowledge domain. The applicability of this framework to similar specialized domains is also examined."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#ethical-considerations",
    "href": "posts/domain-bench/domain-bench.html#ethical-considerations",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.6 Ethical Considerations",
    "text": "1.6 Ethical Considerations\nKey ethical considerations include the following:\n\nPotential misinformation risks due to incorrect or misleading model output.\nTransparency regarding known model limitations.\nFairness of evaluation across diverse domain requirements.\nPrivacy concerns addressed through the use of publicly available and synthetic data.\nMitigating automation bias by emphasizing justification and evidence-supported responses."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#layout",
    "href": "posts/domain-bench/domain-bench.html#layout",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "1.7 Layout",
    "text": "1.7 Layout\nThe dissertation proceeds with a Literature Review examining current practices, emerging frameworks, and some distilled conclusions based on the review; followed by Output Design detailing the methodology, architecture, and evaluation parameters of the proposed framework; and concludes with comprehensive references."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#background",
    "href": "posts/domain-bench/domain-bench.html#background",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "2.1 Background",
    "text": "2.1 Background\nBenchmarking LLMs serves several critical functions. Benchmarks provide a solid basis for performance evaluations of how well LLMs perform across different capabilities such as reasoning, knowledge retrieval, language understanding, etc. They in turn allow researchers to quantify improvements over time and compare different model architectures and training techniques. Organizations can then use benchmark results to inform the appropriate selection of models for specific applications based on their strengths and weaknesses. Benchmarks highlight areas where models struggle, guiding future research efforts. Many benchmarks now include evaluations of harmful outputs, biases, and other safety concerns. Despite their importance and utility, LLM benchmarks face several significant challenges.\nBenchmarking LLMs has historically been crucial for measuring progress and comparing models. Standard benchmarks such as General Language Understanding Evaluation (GLUE), SuperGLUE and Massive Multitask Language Understanding (MMLU) provided fixed datasets and tasks to evaluate core NLP capabilities, enabling consistent comparisons between models (McIntosh et al. 2024). These benchmarks focus on end-to-end metrics for tasks such as answering questions, translating, or common-sense reasoning, treating the model as a black box that produces an answer per query. This approach helped quantify improvements and highlight strengths and weaknesses of the model over time. Organizations could use benchmark results to guide model selection for applications and researchers could identify where models struggle to spur future advances. Over time, benchmarks also began to assess ethical and safety dimensions, such as bias and harmful output, to ensure responsible artificial intelligence (AI) development. In short, LLM benchmarks serve as standardized yardsticks for performance, driving the evolution of the field.\nHowever, foundational benchmark practices come with assumptions and limitations. One such assumption is that improvements in model performance on high-profile benchmarks like MMLU, HumanEval, etc., are indicative of the model developing a deeper understanding or acquiring certain abilities. In addition, a common practice in LLM evaluation is using the predicted probabilities of a model to choose answers in tasks, especially multiple-choice questions, instead of letting the model generate an answer in natural language. Many benchmark evaluations, for efficiency, will have the model score each possible answer (option A, B, C, etc.) and pick the highest-probability option as its answer. This method is label-based or probability-based evaluation, as opposed to generation-based evaluation, where the model actually produces an answer, sometimes with an explanation, and that output is checked. The main reason why many current evaluation frameworks default to probability-based scoring is due to computational constraints, namely, it is faster and cheaper to get the probability of a model on a fixed set of answers than to generate text. The assumption is that this method of evaluation serves as a suitable proxy for the behavior of a model in generation-based tasks. Finally, long-standing statistical metrics such as accuracy for classification, F1, precision/recall, BLEU/ROUGE for generation, etc., have provided a way to objectively quantify performance. However, these statistical measures are limited in their ability to properly stratify and score nuanced outputs that often result from real-world application.\nA growing trend in LLM research and application is the incorporation of tools, such as Retrieval-Augmented Generation (RAG), to enhance LLM capabilities and accuracy. A RAG system has (1) a retrieval component that fetches documents relevant to the query, and (2) a generation component that produces a final answer using both its internal knowledge and the retrieved context (see Figure 1), with the aim to reducing hallucinations and keeping knowledge up-to-date (Gao et al. 2024, 1). Traditional LLM benchmarks assume that the model’s knowledge and reasoning are self-contained, that the model knows the answer from training or must infer it from a given prompt context. They do not explicitly test the model’s ability to retrieve and use external information, nor do they decompose performance into subtasks. For example, MMLU assesses knowledge across domains with multiple-choice questions, but a model’s score conflates knowledge recall and reasoning, without isolating whether an error was due to lack of information or incorrect reasoning. Similarly, holistic evaluation efforts like Holistic Evaluation of Language Models (HELM) provide diverse metrics (accuracy, calibration, bias, etc.), but still treat the model as a single black-box system producing an answer per query. In short, conventional benchmarks excel at measuring what answer the model gives, but not how it got that answer.\n\n\n\nFigure 1. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer. Source: (Gao et al. 2024, 3)"
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#related-work",
    "href": "posts/domain-bench/domain-bench.html#related-work",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "2.2 Related Work",
    "text": "2.2 Related Work\n\n2.2.1 Benchmark Contamination Issues\nOne critical threat to benchmark integrity is BDC, which is the leakage of test examples into the training data of a model. Xu et al. (Xu et al. 2024) underscore that BDC is widespread and often hard to detect, yet undermines the credibility of benchmark results. Essentially, many high-profile benchmarks have had some of their questions or answers seen by large models during training, for instance, models memorized solutions to portions of MMLU or HumanEval. The reasoning is that it leads to inflated evaluation scores as a model may appear to excel at a task not because it truly mastered the underlying skill, but because it recalls the answers from memory. The survey also finds that while some researchers originally argued that memorizing answers is not necessarily bad or unavoidable, the prevalent view is that it “poses significant challenges to the reliability and validity of LLM evaluations” (Xu et al. 2024, 1). Although the survey is limited to only compiling known techniques, one gap they identify is the lack of a unified, systematic approach to defining and tackling BDC. They highlight that no single mitigation will solve the issue completely given the scale of LLM training data, which often sweeps up entire Internet archives.\nAn analysis by Golchin & Surdeanu (Golchin and Surdeanu 2024) revealed specific contaminated benchmarks, for example, finding that datasets like AG News and XSum had leaked content when tested with GPT-4. These findings reinforce Xu et al. (Xu et al. 2024)’s conclusions that training data contamination is common and widespread across many tasks.\n\n\n2.2.2 Misalignment in LLM Evaluation Practices\nAnother issue is the misalignment between how we evaluate LLMs in benchmarks and how LLMs are actually used in the real world. McIntosh et al. (McIntosh et al. 2024) argue that many benchmarks fail to measure what we really care about. In their audit of 23 state-of-the-art benchmarks, they found that evaluations often rely on simplistic proxies or narrow setups that do not reflect genuine performance in deployment. For example, several benchmarks credit a model for getting the correct answer but do not check whether the model’s reasoning process made sense or if it just guessed patterns. Many benchmarks lack diversity in inputs or values, meaning a model can score well by overfitting to stereotyped prompts. McIntosh et al. (McIntosh et al. 2024) observed issues such as cultural bias in test content and high sensitivity to prompt formatting, which indicates that benchmark scores might not translate to real-world reliability. This focus on final answers rather than the process means that models can sometimes ‘game’ benchmarks, thus achieving good scores through shortcuts or pattern matching rather than true understanding. In short, models could appear state-of-the-art on a leaderboard yet disappoint in practical usage because the evaluation was misaligned with real usage conditions. A limitation of the study by McIntosh et al. (McIntosh et al. 2024) itself is that their critique is qualitative and does not provide a quantitative fix. However, the study unveils a diversity of issues present within benchmarking practices that highlight the need for intentional improvement.\nA concrete example of evaluation misalignment is given by Lyu, Wu, & Aji (Lyu, Wu, and Aji 2024), which brings to light some compelling insights into the current limitations of predicted probability-based evaluations. The primary metrics considered in the study were (1) accuracy under each evaluation mode, probabilities-based evaluations versus generation-based, and (2) consistency between the two. They show that the probability-based method “inadequately aligns with generation-based prediction” (Lyu, Wu, and Aji 2024, 1) creating a misrepresentation of the performance and behavior of a model (see Figure 2). For example, the option that the model assigns highest probability is not always the one it would output when asked to explain or answer directly. The misalignment could manifest as the model having a hidden preference it does not act on when forced to choose via probabilities. Essentially, the authors treat the generation-based outcome as the ‘ground truth’ of what the model really believes or would do in practice, and they check how often the probability proxy matches that. This underscores that the convenience and cost efficiency of probability-based evaluations come at the cost of not fully understanding the real-world behavior of a model. A limitation of Lyu, Wu, & Aji (Lyu, Wu, and Aji 2024)‘s study is that they assume generation-based evaluation is the ’ground truth’ measure of performance, but generation brings its own uncertainties. For example, a model might generate a correct answer phrased differently from the expected answer, and automatic evaluation could count that wrong unless carefully handled. They do note that evaluating generation often requires careful parsing or human judgment, which is why probabilistic methods gained popularity in the first place, but they do not propose solutions to ameliorate the extra costs of doing so.\n\n\n\nFigure 2. An illustration of label-based, sequence-based and generation-based predictions for evaluating LLMs on NLP benchmarks. Source: (Lyu, Wu, and Aji 2024, 2)\n\n\n\n\n2.2.3 Newer Benchmarking Frameworks\nXia, Deng, & L. Zhang (Xia, Deng, and Zhang 2024) tackle the problem of benchmarks becoming overfitted by introducing EVOEVAL, an approach to dynamically evolve coding challenges (see Figure 3). They start from popular coding benchmarks like HumanEval and MBPP and use LLMs themselves to generate new variations of these programming problems. The idea is to create challenges that are similar in spirit but sufficiently different in surface details or domain, so that an LLM which memorized the original solutions will be caught off guard. The study uses standard coding task metrics, such as the pass rate of generated code against unit tests (e.g. pass@k metrics), to evaluate model proficiency. However, they compared metrics on the original benchmarks versus the evolved ones. A key finding in their analysis is the drop in performance (%) when switching to evolved tasks. Models that previously topped the coding leaderboards saw absolute drops in accuracy of 20%–47% in the evolved problems, and many fell dramatically in ranking. This reveals that existing coding benchmarks probably overestimated true model competency, as models had effectively overfit on the narrow distribution or even leaked solutions. Xia, Deng, & L. Zhang (Xia, Deng, and Zhang 2024) highlight phenomena like brittleness to slight rewording, for instance, a prompt asking for a solution “in two sentences” might confuse a model that learned to expect a certain format. Some EVOEVAL tasks require combining two simpler tasks, which many models struggled with, showing weakness in multistep reasoning or code synthesis. Some notable limitations are: (1) using an LLM to generate benchmarks might introduce its own biases or errors. (2) The authors had to ensure that the new problems were neither trivial variations nor unsolvable; this probably required manual curation, which could be time consuming. (3) EVOEVAL is specifically focused on the domain of coding. Although their findings pertain to code benchmarks, it is entirely possible that these trends are prevalent in other domains but would require their own “evolution” to prove definitively. (4) Finally, as models improve or new training data appear, possibly including EVOEVAL itself in the future, this approach would have to continually generate further evolved tasks.\n\n\n\nFigure 3. Overview of EVOEVAL evolving problem generation pipeline. Source: (Xia, Deng, and Zhang 2024, 4)\n\n\nDalvi et al. (Dalvi et al. 2024) take a very practical angle by introducing LLMeBench, a benchmarking framework aimed at making LLM evaluation flexible, extensible, and efficient. LLMeBench comes with generic dataset loaders, supports multiple model providers, and has many pre-implemented standard evaluation metrics (see Figure 4). Importantly, it supports in-context learning setups like zero-shot and few-shot, meaning it can automate prompts for models with given examples if needed. The framework does not invent new metrics, but streamlines the use of existing ones such as accuracy for classification, F1, precision/recall, BLEU/ROUGE, etc. Another aspect is that by supporting multiple tasks, LLMeBench encourages the use of holistic evaluation. LLMeBench can easily run a model through a battery of tasks such as translation, question-answering, and reasoning puzzles using a single framework. Therefore, this framework fosters comprehensive evaluation rather than a single-metric focus. One limitation is that LLMeBench, while flexible, is only as good as the benchmarks one feeds into it, but it does not solve what to evaluate, it helps with how. If one were to use poor-quality or biased benchmark data, the framework would faithfully report metrics, but the insights would still depend on the input. In addition, LLMeBench will not automatically flag contamination issues or suggest new tasks and is therefore susceptible to the issues already raised. In terms of real-world applicability, LLMeBench addresses the barrier to entry, which could allow domain experts to focus more on writing good questions and let the tool handle the rest. McIntosh et al. (McIntosh et al. 2024) noted that many benchmarks suffer from “implementation inconsistencies” and slow iteration, which LLMeBench directly tackles by providing a consistent implementation and enabling quick reruns of evaluations.\n\n\n\nFigure 4. The architecture of the LLMeBench framework. The dotted boxes represent the core implemented modules of the architecture. Customization for new tasks, datasets, and models can be done on Dataset, Model Provider, Evaluation, and Asset modules. Source: (Dalvi et al. 2024, 1)\n\n\nTo summarize key challenges in standard LLM benchmarking, Table 1 presents a comparative analysis of major studies on benchmark contamination, overfitting, and misalignment.\n\n\nTable 1. General LLM Benchmarking Issues\n\n\n\n\n\n\n\n\n\nPaper & Authors\nBenchmarking Issue\nProposed Approach\nKey Findings\nFuture Directions\n\n\n\n\nMcIntosh et al. 2024\nOverfitting in static benchmarks; misalignment with real-world use.\nAnalyzed 23 benchmarks, advocating for evolving test sets.\nBenchmarks overestimate model reliability due to gaming.\nImplementing truly dynamic benchmarks remains unsolved.\n\n\nXu et al. 2024\nTest contamination leads to inflated scores.\nAdvocates systematic dataset filtering and leak detection.\nTraining data contamination is widespread.\nLarge-scale detection remains a challenge.\n\n\nLyu, Wu, & Aji 2024\nMisalignment between probability-based and free-form evaluation.\nCompares probability-based vs. generated response accuracy.\nProbability selection misrepresents true model ability.\nEfficient generation-based scoring remains an open problem.\n\n\nXia, Deng, & L. Zhang 2024\nOverfitting in static coding benchmarks.\nEVOEVAL: Dynamically mutates coding tasks.\nModels show 20-47% drop on evolved problems.\nNeeds broader application beyond coding.\n\n\nDalvi et al. 2024\nFragmented evaluation tools.\nLLMeBench: Unified multi-metric benchmarking.\nStandardizes model comparisons.\nStill depends on pre-existing datasets.\n\n\n\n\n\n\n2.2.4 RAG & Multi-Task Evaluation\nThe survey by Gao et al. (Gao et al. 2024) provides a comprehensive overview of RAG systems and specifically introduces metrics and benchmarks to assess RAG models alongside an up-to-date evaluation framework. RAG evaluation frameworks go one step further by explicitly measuring the different stages of a retrieve-and-generate pipeline (see Figure 5) (Es et al. 2023). Evaluating such systems involves multidimensional metrics:\n\nRetrieval quality – Is the retrieved context relevant and sufficient for the query?\nGeneration quality – Is the final answer correct and is it faithful to the retrieved evidence?\nIntegration performance – How well does the LLM incorporate the retrieved information? Does it avoid hallucination or ignore evidence?\n\n\n\n\nFigure 5. Overview of RAG Evaluations. Adapted from: (Gao et al. 2024, 16)\n\n\nRecent RAG-specific evaluators such as Retrieval Augmented Generation Assessment (RAGAS) (Es et al. 2023) and the Automated RAG Evaluation System (ARES) (Saad-Falcon et al. 2024) exemplify this approach. RAGAS introduces a reference-free multimetric framework to automatically assess RAG pipelines. It proposes a suite of zero-shot LLM-based evaluation metrics that target each aspect of the pipeline, (1) the relevance of the retrieved passages, (2) the faithfulness of the LLM’s response to those passages, and (3) the overall quality of the response. In practice, RAGAS uses prompt-based evaluation with an LLM, such as GPT-4, to score output on several dimensions, eliminating the need for ground truth answers for every query. Concretely, the RAGAS score combines two metrics for the generation stage, faithfulness and answer relevancy, and two metrics for retrieval, context precision, and context recall. Faithfulness measures whether the answer accurately reflects the information found in the retrieved documents (i.e., no unsupported claims), and answer relevancy checks if the answer addresses the query directly without unnecessary content. Context precision/recall evaluate whether retrieved passages are relevant to the query and cover the needed information.\nIn contrast, existing LLM benchmarks rarely assess these facets separately. Traditional QA evaluations might reward an answer that is correct, but they will not detect whether the model had to hallucinate missing facts or if it ignored provided context. RAG evaluation frameworks fill this gap by explicitly rewarding answers that are correct and grounded in evidence. For example, Gao et al. (Gao et al. 2024) note that RAG models are particularly aimed at mitigating hallucinations and providing traceable sources, so the evaluation criteria emphasize factuality and source attribution. This is aligned with emerging benchmarks such as Knowledge Intensive Language Tasks (KILT), a set of knowledge-intensive tasks that require models to retrieve supporting Wikipedia passages, where evaluations similarly combine a correctness measure with evidence retrieval accuracy. RAGAS and ARES add automation and finer granularity to this more comprehensive style of evaluation. ARES, for example, evaluates RAG systems along three dimensions, context relevance, answer faithfulness, and answer relevance. It creates synthetic QA pairs to fine-tune lightweight ‘judge’ models that can score each aspect, and calibrates them with a small number of human-annotated examples. This allowed ARES to reliably evaluate RAG performance on eight knowledge-intensive tasks, from benchmarks like KILT and SuperGLUE, with only a few hundred human labels, by having the learned judges predict ratings for thousands of cases.\nTable 2 summarizes recent advancements in RAG benchmarking, highlighting how different studies address retrieval accuracy, generation quality, and multistage evaluation.\n\n\nTable 2. RAG / Multi-Task Benchmarking Issues\n\n\n\n\n\n\n\n\n\nPaper & Authors\nBenchmarking Issue\nProposed Approach\nKey Findings\nFuture Directions\n\n\n\n\nGao et al. 2024\nConventional benchmarks ignore retrieval models.\nMulti-stage RAG evaluation: retrieval, generation, integration.\nDistinguishes retrieval quality from generated response.\nWidespread adoption of multi-stage metrics is needed.\n\n\nEs et al. 2023\nLack of automated RAG evaluation.\nUses LLM-based scoring for retrieval and accuracy.\nEnables scalable evaluation without ground truth labels.\nLLM-generated scores may introduce biases.\n\n\nSaad-Falcon et al. 2024\nScaling RAG evaluation with minimal human labeling.\nARES: Uses few human labels to train evaluators.\nReduces manual scoring dependency.\nUpdating evaluators for evolving data remains an issue.\n\n\nRasiah et al. 2024\nBenchmarks too simplistic for real-world applications.\nSCALE: Legal domain benchmark with long-text, multilingual tasks.\nModels struggle with long, domain-specific inputs.\nSimilar domain-specific benchmarks are needed.\n\n\nFriel, Belyi, & Sanyal 2025\nNo standardized RAG benchmark or explainability.\nRAGBench: 100k examples + TRACe, an explainability framework.\nEnables large-scale, explainable RAG evaluation.\nEnsuring alignment between automated and human scoring remains an issue."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#conclusions",
    "href": "posts/domain-bench/domain-bench.html#conclusions",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "2.3 Conclusions",
    "text": "2.3 Conclusions\nExamining these works collectively, several common themes emerge with regard to general principles and best practices for domain-specific LLM benchmarks.\n\n2.3.1 Maintaining Benchmark Integrity\nA consistent concern is to ensure that the evaluation truly measures generalization, not memory. Xu et al. (Xu et al. 2024)‘s survey highlighted how pervasive this issue is, warning that models often get inflated scores by ’knowing’ test answers in advance. It is important to distinguish that the memorization of the answers is not the problem in question, but can be an indicator of the real issue, which is brittleness and overfitting. Although the mere presence of model memorization does not definitively prove overfitting has occurred benchmarking solutions should be designed to elucidate this issue. Works such as Xia, Deng, & L. Zhang (Xia, Deng, and Zhang 2024)’s EVOEVAL, which regenerates and mutates test questions, show promise in this regard. The general principle made clear in this review is to keep the benchmarks novel and unpredictable. Echoing this theme, McIntosh et al. (McIntosh et al. 2024) advocates for dynamic benchmarks that evolve so that models cannot simply overfit. For domain-specific benchmarks, this might mean using proprietary or freshly collected data that was not in common pre-training corpora, or continuously adding new test cases over time. In doing so, we maintain the integrity of the benchmark, ensuring that the scores remain a trustworthy signal of the capability of a model.\n\n\n2.3.2 Evolving Evaluation to Reflect Real Usage\nThere is a clear trend towards making evaluations more holistic, realistic and aligned with how LLMs are actually used. Lyu, Wu, & Aji (Lyu, Wu, and Aji 2024) explicitly show that evaluation methods can be misaligned, if we optimize for convenience like in the case of multiple-choice probability evaluation, we might miss the true behavior of the model. Similarly, McIntosh et al. (McIntosh et al. 2024) and Rasiah et al. (Rasiah et al. 2024) push for benchmarks that test models in more complex scenarios (multi-turn interactions, long documents, diverse languages) because real-world tasks are complex. A general principle is that benchmarks should simulate the conditions under which we expect the model to perform. For instance, if an LLM will be used by non-English speakers, the benchmark should have multilingual components, as SCALE does. If the model will function as a dialogue agent, the benchmark should include interactive prompts or multistep reasoning tasks, not just single-turn queries. We see this in RAGBench and Li, Yuan, & Z. Zhang (Li, Yuan, and Zhang 2024)’s work by incorporating retrieval into the evaluation system. Since many real deployments use tools to assist LLMs, the benchmarks must evaluate that combined system. Another aspect of evolving evaluation is the use of multimetric assessment. Instead of a one-number accuracy or BLEU, there is a move to break down the performance into submetrics such as RAGBench’s TRACe to get a more complete picture. This is especially important in domain-specific contexts like legal or medical, where an answer might need to be not only correct but also justified and safe. By having granular metrics such as correctness, justification adequacy, harmful content check, etc., benchmark results become more actionable, developers can see why a model fails and improve it. In summary, best practices involve designing benchmarks that are high-fidelity proxies for deployment scenarios: dynamic, diverse, and evaluated on multiple axes of quality.\n\n\n2.3.3 Leveraging Tools and Hybrid Approaches\nAnother emerging principle is that benchmarks can and should test a model’s ability to use tools or external knowledge, rather than confining the evaluation to end-to-end prompting. Li, Yuan, & Z. Zhang (Li, Yuan, and Zhang 2024) and Friel, Belyi, & Sanyal (Friel, Belyi, and Sanyal 2025) both illustrate this by focusing on retrieval-augmented settings. This intersection of tool-use with benchmarking is increasingly relevant as advanced models often come with an ecosystem of plugins or support systems. A domain-specific example: A cybersecurity LLM might have access to a database of known vulnerabilities; a good benchmark would measure how well the LLM queries that database and integrates the results into its advice, not just what it remembers. By designing benchmarks that allow tool use, for example, providing an API or knowledge base as part of the test environment, we measure a more practical skill, the ability of an AI to know what it does not know and find out. This also helps combat hallucinations and data staleness, as seen in RAG approaches. In intersections, this addresses some contamination issues by relying on an external source rather than training memory. This in turn aligns with the goal of realistic evaluation, since human AI users often expect AI to cite sources or use web search. It is a shift from the old paradigm of closed-book QA towards an open-book evaluation model.\nIn particular, RAG evaluation highlights the importance of ground truth reference signals for factual tasks. In base LLM evaluation, this insight suggests incorporating open-book testing: instead of only closed-book QA, have benchmarks where the model can consult a knowledge source, as a form of RAG, and see if that boosts performance. If an LLM under closed-book conditions fails a question but succeeds when allowed to retrieve relevant text, that indicates the base model’s limitation was missing knowledge, not reasoning ability. Conversely, if it fails even with the reference provided, the issue lies in understanding or reasoning. This differentiated evaluation, closed-book versus open-book, was historically done in QA research and can be informed by RAG frameworks. Gao et al. (Gao et al. 2024) mention that RAG enables continuous knowledge updates and domain-specific info integration. Evaluating a base model in scenarios with and without such updates can quantify how much retrieval augments it. In summary, by borrowing RAG’s metrics such as faithfulness, relevance, etc., and techniques such as LLM-based judging, multicomponent analysis, we can design more nuanced and robust evaluations for base models. This ensures that enhanced capabilities such as factual grounding are explicitly tested and that a model’s score reflects not just whether it is right, but why and how it arrives at the answers."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#methodologies",
    "href": "posts/domain-bench/domain-bench.html#methodologies",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "3.1 Methodologies",
    "text": "3.1 Methodologies\nTo develop an effective cybersecurity compliance advisory benchmarking framework, we employ a structured methodology that ensures real-world relevance and automation. The benchmark is designed around publicly available cybersecurity standards (e.g., NIST 800-53, NIST Cybersecurity Framework (CSF) 2.0, CIS Controls, CSA Cloud Controls Matrix (CCM), GDPR, and MITRE ATT&CK) and focuses on evaluating an LLM’s ability to advise on compliance-related queries. This includes:\n\nIdentifying Real-World Advisory Tasks – Defining key use cases such as answering compliance-related questions, identifying gaps, providing policy recommendations, and cross-mapping standards (NIST 2020).\nAutomating Benchmark Dataset Creation – Extracting and structuring compliance questions from regulatory texts, case law, certification exams, and expert Q&A forums (McIntosh et al. 2024).\nEvaluating Multiple Architectures – Comparing base models, fine-tuned models, RAG-enhanced models, and GraphRAG architectures (Xu et al. 2024).\nContinuous Refinement and Benchmark Evolution – Preventing benchmark overfitting by generating test variants and monitoring for artificial performance inflation (Xia, Deng, and Zhang 2024). The framework addresses benchmark evolution through three practical mechanisms:\n\nScheduled Resource Updates: Periodic manual replacement of resource documents with the latest regulatory versions, recognizing that automated detection of domain knowledge evolution represents a complex research problem requiring continuous monitoring of regulatory bodies.\nSchema-Preserved Regeneration: Maintaining consistent output schemas while regenerating questions with updated resource documents, producing differently worded questions with the same evaluative content to test genuine understanding versus memorization.\nMulti-Model Generation: Using different LLMs (e.g., Gemma2:9b, Llama3.1, GPT-4) with identical schemas and resource documents to create diverse question formulations while ensuring benchmarks are not biased toward specific model families."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#software",
    "href": "posts/domain-bench/domain-bench.html#software",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "3.2 Software",
    "text": "3.2 Software\nThe benchmarking framework is implemented with Docker and Ollama for seamless local model deployment and evaluation (Ollama 2024). Ollama enables running 7B-13B parameter open-source models on consumer GPUs, ensuring cost-effective testing. The TrustGraph framework is used for GraphRAG, leveraging knowledge graphs to enhance retrieval accuracy (TrustGraph 2024). Hugging Face’s “evaluate” library provides standardized performance metrics, while Python-based scripts automate dataset curation, evaluation pipeline execution, and metric tracking. Additional tools include:\n\nLangGraph – For agent workflow orchestration and state management (LangGraph 2025).\nLangChain – For LLM-based retrieval, response generation, and logging interactions (LangChain 2024).\nFAISS – For vector-based document retrieval in standard RAG configurations (Douze et al. 2025).\nQdrant – For knowledge graph storage and querying in GraphRAG experiments (Qdrant 2025).\nPydantic – For dynamic schema generation and structured output validations (Pydantic 2025)."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#system",
    "href": "posts/domain-bench/domain-bench.html#system",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "3.3 System",
    "text": "3.3 System\nThe benchmarking system follows a modular three-phase pipeline (see Figure 7), comparing four LLM configurations (see Figure 8):\n\nBase LLM - Direct model inference using Ollama (7B-13B open models).\nStandard RAG - Vector-based retrieval augmentation using FAISS indexing.\nGraphRAG - Knowledge graph-based retrieval using TrustGraph framework.\nAgent - Multi-tool ReAct agent combining vector RAG, graph RAG, and web search capabilities.\n\n\n\n\nFigure 7. Domain-Bench framework architecture showing the complete three-phase evaluation process: (1) EvalAgent-driven benchmark generation using LangGraph workflows and multi-modal RAG, (2) systematic evaluation across four distinct model architectures, and (3) multidimensional assessment using five evaluation criteria.\n\n\n\n\n\nFigure 8. Four model architectures evaluated systematically: BaseLLM (direct inference), StandardRAG (vector-based retrieval), GraphRAG (knowledge graph retrieval using TrustGraph), and Agent (multi-tool ReAct agent combining multiple retrieval strategies).\n\n\nEach architecture is deployed through a unified interface and evaluated on curated benchmarks of domain-specific queries. The pipeline executes queries across all architectures, retrieves relevant context (for RAG/GraphRAG/Agent models), logs outputs, and calculates performance metrics using the EvalAgent framework."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#resources",
    "href": "posts/domain-bench/domain-bench.html#resources",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "3.4 Resources",
    "text": "3.4 Resources\nThe implementation relies on publicly available compliance datasets and documentation:\n\nNIST Special Publications (SP 800 series) (NIST 2020)\nNIST Cybersecurity Framework (CSF) 2.0 (NIST 2024)\nCIS Critical Security Controls v8 (CIS 2023)\nFedRAMP Security Controls & Compliance Guidelines (FedRAMP 2023)\nGDPR regulatory text and enforcement case studies (EuropeanCommission 2023)\n\nThe system employs an automated EvalAgent framework that processes academic papers to extract evaluation principles, generates domain-specific system prompts, and creates benchmarks through multi-document RAG retrieval from compliance resources (see Figure 9) (Krishna 2024).\n\n\n\nFigure 9. EvalAgent benchmark generation workflow showing the automated extraction of evaluation principles from academic papers through a five-step process: principle extraction, schema generation, system prompt creation, and benchmark generation using multi-modal RAG."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#evaluation-parameters",
    "href": "posts/domain-bench/domain-bench.html#evaluation-parameters",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "3.5 Evaluation Parameters",
    "text": "3.5 Evaluation Parameters\nTo measure the success of each implementation, the benchmarking framework (see Figure 10) evaluates models across multiple dimensions:\n\nAgreement – Is there consistency between a model’s predicted probabilities and its ability to generate coherent, relevant, and accurate text? High agreement implies that the model’s probabilistic outputs accurately reflect its generative capabilities across diverse tasks, including those with both definitive solutions and open-ended inquiries (Lyu, Wu, and Aji 2024).\nFunction Correctness – Does the answer reliably and accurately fulfill the intended function specified in a task or prompt? Does it properly address its real-world intended usage? (Xia, Deng, and Zhang 2024).\nReasoning – Can the model logically justify its recommendations? (Xu et al. 2024).\nRelevance – Does the response directly address the compliance question? (Es et al. 2023).\nRetrieval Effectiveness (for RAG/GraphRAG) – Are retrieved documents relevant and properly used? (Gao et al. 2024).\n\n\n\n\nFigure 10. Evaluation pipeline demonstrating the systematic assessment process from benchmark questions through multi-architecture execution to comprehensive scoring across five evaluation dimensions.\n\n\nEach model is tested on a standardized benchmark and its performance is logged across these metrics. The results are analyzed to determine:\n\nWhich model architecture performs best?\nDoes retrieval improve reasoning and accuracy?\nDoes GraphRAG reduce hallucinations and improve compliance adherence?\nAre improvements genuine or due to artificial benchmark gaming? (Xia, Deng, and Zhang 2024)\n\nIf the best-performing model exhibits benchmark overfitting (e.g., memorization of test questions), the architecture is adjusted, re-tested, and iteratively refined (Rasiah et al. 2024)."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#implementation-workflow",
    "href": "posts/domain-bench/domain-bench.html#implementation-workflow",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "3.6 Implementation Workflow",
    "text": "3.6 Implementation Workflow\nThe Domain-Bench framework operates through an integrated workflow that combines automated benchmark generation with systematic architecture evaluation (as shown in the complete framework, Figure 7):\n\nPrinciple Extraction: Academic papers are processed to extract evaluation criteria using the EvalAgent’s Chain-of-Thought reasoning capabilities.\nBenchmark Generation: Domain-specific questions are created through multi-modal RAG, combining vector search and knowledge graph retrieval from compliance resources.\nArchitecture Evaluation: Each benchmark is executed across all four model configurations, with responses logged and contextualized.\nMultidimensional Assessment: The same EvalAgent framework that generated benchmarks evaluates responses across five dimensions, ensuring consistency and fairness.\n\nThis end-to-end automation enables scalable, reproducible evaluation while maintaining the flexibility to adapt to evolving domain requirements."
  },
  {
    "objectID": "posts/domain-bench/domain-bench.html#implementation-details",
    "href": "posts/domain-bench/domain-bench.html#implementation-details",
    "title": "Beyond Static Benchmarks: A Modular and Dynamic Framework for Domain-Specific LLM Evaluation",
    "section": "Implementation Details",
    "text": "Implementation Details\nThe Domain-Bench framework represents a comprehensive approach to addressing the critical challenges identified in current LLM benchmarking practices. Through its modular design and automated evaluation pipeline, it provides a scalable solution for domain-specific assessment while maintaining the rigor necessary for reliable performance measurement.\n\nTechnical Specifications\nHardware Requirements: - Minimum 16GB RAM for 7B parameter models - NVIDIA GPU with 8GB+ VRAM recommended - 100GB+ storage for document repositories and model weights\nSoftware Dependencies: - Python 3.9+ - Docker 20.10+ - Ollama 0.1.26+ - CUDA 11.8+ (for GPU acceleration)\nModel Support: - Open-source models: Llama3.1, Gemma2, Mistral, CodeLlama - Commercial APIs: OpenAI GPT-4, Anthropic Claude, Google PaLM - Custom fine-tuned models via Hugging Face integration\n\n\nDeployment Architecture\nThe framework follows a microservices architecture enabling horizontal scaling and fault tolerance:\n\nBenchmark Generation Service: Handles EvalAgent workflow and question creation\nModel Evaluation Service: Manages model inference across different architectures\n\nRAG Service: Provides document retrieval and knowledge graph querying\nEvaluation Service: Computes multidimensional metrics and generates reports\nData Management Service: Handles document ingestion, versioning, and storage\n\n\n\nFuture Enhancements\nPlanned developments include: - Real-time adaptation: Dynamic benchmark adjustment based on model performance patterns - Multi-domain extension: Framework generalization to healthcare, finance, and legal domains\n- Advanced contamination detection: Integration of sophisticated leak detection algorithms - Human-in-the-loop evaluation: Incorporation of expert validation for complex advisory scenarios - Federated benchmarking: Support for distributed evaluation across organizations while preserving data privacy"
  },
  {
    "objectID": "posts/big-data-analytics/big-data-analytics.html",
    "href": "posts/big-data-analytics/big-data-analytics.html",
    "title": "Big Data Analytics: DDoS Real-Time Classification",
    "section": "",
    "text": "Source Code\n\n\n\nView source code on Github repository."
  }
]